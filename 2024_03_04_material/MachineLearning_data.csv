id,author,created,title,content,subreddit,url,num_comments,score,upvote_ratio,comments
196j1w0,AutoModerator,2024-01-14 16:00:18+00:00,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/,31,4,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32BC4810>
19cqde6,automatonv1,2024-01-22 07:41:30+00:00,[D] After chatGPT are people still creating their own new custom NLP models these days?,"Been a little out of touch with training ML and DL models using scikit-learn and Tensorflow off-late. Just wondering if ML Engineers still train their own NLP models (or even CV, Prediction, Clustering models etc.) still.

If so, What kind of models are you training? And what use cases are you solving? If you replaced your custom models with ChatGPT, How is that going?

I would like to reacquaint myself with the ML ecosystem. Curious to hear your thoughts.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cqde6/d_after_chatgpt_are_people_still_creating_their/,44,51,0.85,<praw.models.comment_forest.CommentForest object at 0x0000026E32BDF850>
19cwf65,_learning_stuff_,2024-01-22 14:07:44+00:00,[D] Why we're not seeing a lot of content about Mamba architecture?,Like maybe some interviews with the authors? Yet to see e.g. TWIML AI podcast talk about Mamba architecture.,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cwf65/d_why_were_not_seeing_a_lot_of_content_about/,3,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32BB6490>
19ctbuq,SuperbMonk4403,2024-01-22 11:14:36+00:00,[D] ML dev in containers,"So like many others out there I’m in a predicament where I have a Linux development environment that I access through SSH (pretty awesome machines) but there are relatively bare metal and come with docker, Nvidia drivers, and some python. 
The catch: it’s all offline.

Instead of trying to guess and bring up eight versions etc, I’m working to pursue using containers (which only need to confirm compatibility with the nvidia driver).

I have two main questions:

1. Is there any benefit to either the NGC Container for PyTorch vs this PyTorch on on docker hub? I do like the devel base build due to having extra drivers and build tools.
2. For “remote” dev work I’m seeing two options: “dev containers” and Jupyter lab.

The dev containers I’m worried about confirming offline support, but a lot of people like the full IDE. Jupyter Lab I haven’t had much experience outside of the notebook. 
Does the Jupyter python IDE offer things like code completion and syntax highlighting?

Any insights are welcomed.


https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch

https://hub.docker.com/layers/pytorch/pytorch/2.0.1-cuda11.7-cudnn8-devel/images/sha256-4f66166dd757752a6a6a9284686b4078e92337cd9d12d2e14d2d46274dfa9048?context=explore",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ctbuq/d_ml_dev_in_containers/,4,4,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32BCA0D0>
19cvxz4,Street-Judgment7640,2024-01-22 13:44:41+00:00,[D] ARR 2023 December (NAACL 2024) Discussion,Reviews are supposed to be realeased today.,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cvxz4/d_arr_2023_december_naacl_2024_discussion/,1,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32BBE210>
19ctr9p,Conclusion_Silent,2024-01-22 11:42:27+00:00,[R] How Does the GPT-4V API deal with large Images?,"I want to pass varied-size infographics to the GPT4V model. I'm not sure what size to set and how to make my costs as low as possible. These Images can get quite large to the 5000 pixel range and can be of different pixel ratios too.  
\- What settings do I consider?  
\- I input the same images to ChatGPT Plus and it performs well but somehow I cant seem to figure out appropriate settings for the OpenAI API.

PS: If you can help me with this resolution thing for Multimodal models like Llava, Bakllava, Blip2, InstructBLIP, etc I'd be thankful",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ctr9p/r_how_does_the_gpt4v_api_deal_with_large_images/,4,4,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E32B88D10>
19csc4x,Ok_Constant_9886,2024-01-22 10:05:18+00:00,[D] I wrote an article on everything I know about LLM evaluation metrics,"Hey everyone, I've been working non-stop in the LLM evaluation space for the past 6 months, from training custom LLMs for evaluation to building evaluation metrics on top of OpenAI's GPT models. I wrote a long article on everything I know about LLM evaluation metrics, and I hope someone finds this useful, may it be for interest or at work. Let me know if you found it useful or any questions/suggestions you may have!

Here is the link to the article: [https://medium.com/@jeffreyip54/llm-evaluation-metrics-everything-you-need-for-llm-evaluation-6b129157e33c](https://medium.com/@jeffreyip54/llm-evaluation-metrics-everything-you-need-for-llm-evaluation-6b129157e33c)

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19csc4x/d_i_wrote_an_article_on_everything_i_know_about/,0,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E32BCEE90>
19cxibs,ZhalexDev,2024-01-22 14:59:36+00:00,[P] I read through all NeurIPS 2023 Abstracts and wrote about it,"I made this resource that I think might be quite useful here, especially for those looking to find some new, relevant works to read or use for their own projects. It discusses the content from roughly 300 papers, but the topics broadly pertain to all of NeurIPS 2023. Happy reading!

Link: https://alexzhang13.github.io/blog/2024/neurips2023",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cxibs/p_i_read_through_all_neurips_2023_abstracts_and/,0,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32BB72D0>
19cx4ol,Bhargav_28,2024-01-22 14:41:49+00:00,[D] Early stopping but when ?,"Hello, 

I have been recently trying to find out better ways to use early stopping than patience and delta values, and I stumbled on this paper [https://page.mi.fu-berlin.de/prechelt/Biblio/stop\_tricks1997.pdf](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) . Given the criteria mentioned in this paper I found it to be very logical to go ahead with this approach. I also happen to  notice that this is a very old paper and seems like none of the major platforms consider the implemenations here. Is there something I am completely missing on why this is not a valid approach ? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cx4ol/d_early_stopping_but_when/,0,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E2F1641D0>
19cv8q6,cnichkawde,2024-01-22 13:07:31+00:00,[D] Beyond Transformers: Structured State Space Sequence Models, I wrote an article explaining the fundamentals of State Space Sequence Models. The purpose of this article is to present the foundational level concepts in a simplified manner. This field is rapidly evolving in the realm of artificial intelligence owing to the leap it gives over Transformer architecture in terms of speed and memory consumption. Here is the link to the article: [https://cnichkawde.github.io/statespacesequencemodels.html](https://cnichkawde.github.io/statespacesequencemodels.html) ,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cv8q6/d_beyond_transformers_structured_state_space/,7,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32BE58D0>
19cxv2b,javirk,2024-01-22 15:15:11+00:00,[D] Why use RLHF instead of simple loss backpropagation in LLMs?,"Hi, this might be a very simple question, but I can't wrap my head around it: why are LLMs trained with RLHF instead of treating the reward as a loss function and backpropagating as usual? The closest I have found about it is in the [DPO paper](https://arxiv.org/pdf/2305.18290.pdf) (end of Section 3): 

>Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning

But I don't understand what part of the language generation is not differentiable when the reward model is a linear layer on top of the final transformer layer.

Thank you!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cxv2b/d_why_use_rlhf_instead_of_simple_loss/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32828550>
19cdcwz,APaperADay,2024-01-21 20:48:36+00:00,[R] VMamba: Visual State Space Model,"**Paper**: [https://arxiv.org/abs/2401.10166](https://arxiv.org/abs/2401.10166)

**Code and Models:** [https://github.com/MzeroMiko/VMamba](https://github.com/MzeroMiko/VMamba)

**Abstract:**

>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)   stand as the two most popular foundation models for visual   representation learning. While CNNs exhibit remarkable scalability with   linear complexity w.r.t. image resolution, ViTs surpass them in fitting   capabilities despite contending with quadratic complexity. A closer   inspection reveals that ViTs achieve superior visual modeling   performance through the incorporation of global receptive fields and   dynamic weights. This observation motivates us to propose a novel   architecture that inherits these components while enhancing   computational efficiency. To this end, we draw inspiration from the   recently introduced state space model and propose the **Visual State Space  Model** (**VMamba**),  which achieves linear complexity without sacrificing  global receptive  fields. To address the encountered direction-sensitive  issue, we  introduce the Cross-Scan Module (CSM) to traverse the spatial  domain  and convert any non-causal visual image into order patch  sequences.  Extensive experimental results substantiate that VMamba not  only  demonstrates promising capabilities across various visual  perception  tasks, but also exhibits more pronounced advantages over  established  benchmarks as the image resolution increases. Source code  has been  available at [this https URL](https://github.com/MzeroMiko/VMamba).

**The other Vision Mamba:** [https://redd.it/19bgoug](https://redd.it/19bgoug)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cdcwz/r_vmamba_visual_state_space_model/,0,29,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32BE75D0>
19cx8y6,RippSir,2024-01-22 14:47:21+00:00,[R] Where to place the most important instructions in prompts?,"Hi all,
do we know where is the optimal placement of most important instructions in (for example in gpt4 case) prompts? Is there some experimentally observed pattern of forgetting? Or is there some theoretical basis for such a pattern?

e.g. in RAG systems, the retrieved documents can form a large chunk of token limit, do we place instructions for answering before the documents or after them? How do we sort the documents in the prompt based on their relevancy?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cx8y6/r_where_to_place_the_most_important_instructions/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E3282B810>
19csu68,risilm,2024-01-22 10:41:28+00:00,[P] Site for ML coding project?,"Hello, can someone suggest me where to find some projects to do using ML (preferably Python)? I feel like I am at a point in which I am kind of ok with the theory, but I experienced so far coding only in courses/filling notebooks, and I would like to build something on my own from scratch now. Suggestions?
Thanksss",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19csu68/p_site_for_ml_coding_project/,2,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E327C9890>
19cnsgm,AaronSpalding,2024-01-22 05:00:15+00:00,[R] How should I fuse prediction vectors from two quite different modalities?,"I am new to this field, and I need to implement a classifier which fuses the predictions from two other classifiers.

For example, I have classifier A for modality A (i.e. image frame) , and classifier B for modality B (audio).

Classifier A alone can achieve an accuracy of 90%, but classifier B can only achieve an accuracy of 66% for the same test data.

The output of classifier A is pred\_A, whose values look like -11.56, 8.73, -12.15, 10.07 ...

The output of classifier B is pred\_B, whose values look like  -0,068， 0.091, -0.125, 0.052 ...

What is the best way to fuse pred\_A and pred\_B to achieve a better accuracy?

I tried to directly add these two, but with no luck. Then, I am thinking if I should first concatenate these two vectors and then stack an additional torch.linear() on top of the concatenated vector and train everything, but still with no luck.

Could someone provide some comments here about what could easily go wrong?  Should I ""normalize"" or ""sigmoid"" these two vectors before passing them to torch.linear(), or something else?

Thanks in advance.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cnsgm/r_how_should_i_fuse_prediction_vectors_from_two/,4,5,0.73,<praw.models.comment_forest.CommentForest object at 0x0000026E32BF29D0>
19ce73y,gullydowny,2024-01-21 21:23:11+00:00,[D] What's the secret to getting set up with an Apple Silicon chip,"Trying to get a Docker container set up to train a Magenta model and I'm having massive problems with the M chip and Python.

Me and ChatGPT will figure it out eventually but is EVERYBODY working on this type of thing going through this? I've been at this for 12 hours, am I going to end up doing everything on an EC2 instance?

I'm not intending to train it on an M chip, just write the damned Python and deploy it",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ce73y/d_whats_the_secret_to_getting_set_up_with_an/,10,19,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E32BF2C50>
19cou9l,paarulakan,2024-01-22 06:00:28+00:00,[D] How does augmentation by back translation works?,"Training LLMs on low resource languages requires augmentation. One of the popular method is to translate dataset from (mostly) English to desired language. 

Assuming that there is two models one already trained for translation(translation model), and a model(new-model) to be newly trained from data produced by translation-model. new-model is trained for language modelling objective. This approach works, but how? How is this helping the newly trained model? Why is it working? What information is gained from Translation model? Does this mean the Translation model is richer in information?

If the data (parallel corpus) used to train the translation-model is available for training new-model will the need for translated corpus vanish? (Since all the knowledge of the low resource language is gained from the original parallel corpus itself and we now directly have access to that)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cou9l/d_how_does_augmentation_by_back_translation_works/,2,3,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E3282AD50>
19cmwet,Standard_Letter_3196,2024-01-22 04:11:39+00:00,[D] LREC-COLING 2024 Discussion,"LREC-COLING reviews are coming out soon, so making a discussion post about it!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cmwet/d_lreccoling_2024_discussion/,10,4,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32BF9D90>
19ct5ur,vooood,2024-01-22 11:03:21+00:00,[D] Question about training and/or creating a custom model," Hello,

I am I need of (very) specific reinforced training of an AI model or might be even a custom model, but am unsure where to turn.

What I need is an initial model where all vectors would be 0 at start and the model would be given input and all possible output options for the current input. This output would be sent to an API for evaluation and the API would return a score for reinforced learning depending on how appropiate the choice was.

The with sending the input and processing the output of the model with the API is straightforward, don't need help with that. What I need is help with what AI model should or could I do this? Is there a model where I don't need to define all tokens in advance but they could be added ""on the fly"" as new output options are given, depending on input? Would I have to create a completely new AI model for this rather than using existing ones?

Thank you in advance for your help. Please don't remove this post if this is a wrong subreddit, rather point me towards a proper one if I made a mistake.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ct5ur/d_question_about_training_andor_creating_a_custom/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32BF9990>
19ck4yy,vodku_buhayu,2024-01-22 01:49:08+00:00,[R] Survey About AI Trust and Trustworthiness,"Hello everyone, 

I am a Ph.D. student and a part of my thesis research is public perceptions of AI, specifically regarding trust and trustworthiness. So, I am looking for participants for my survey (it is a short Qualtrics survey that should take less than 10 minutes). I would really appreciate your input and insights! 

Link to the [survey](https://uoguelph.eu.qualtrics.com/jfe/form/SV_3skifTlwwwX70s6)

\*  The research is conducted by Cyber Science Lab at the University of Guelph, Guelph, Ontario, Canada. This project has received approval from the Research Ethics Board, ensuring compliance with Canadian federal guidelines for research involving human participants (REB#23-08-018).  ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ck4yy/r_survey_about_ai_trust_and_trustworthiness/,0,5,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E32BFA6D0>
19cshxy,vibrantform,2024-01-22 10:16:55+00:00,[Discussion] offline TTS with ssml tags,"Is there an off-line project similar to coqui-tts that uses ssml tags, at least ability to add some breaks / pauses? I know about piper. Unfortunately piper is not designed to work with coqui models and coqui doesn't support ssml tags. Other models available for piper are not that good and I am looking for an alternative. I am looking for a project that has good quality English pronunciation without added artificial emotions (preferably British but also US). I want to use it for articles, ebooks and language learning purposes.

&#x200B;

For example coqui model 21: tts\_models/en/vctk/vits seem pretty good. A lot of models don't pronounce some English words correctly (for example negotiate, negotiation).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cshxy/discussion_offline_tts_with_ssml_tags/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32C03A50>
19crtxp,yuvalgrader,2024-01-22 09:28:14+00:00,[D] Quick Poll: TensorFlow vs. PyTorch in 2024," As we progress in 2024, I'm curious to know the community's preference: TensorFlow or PyTorch? Which do you prefer for your projects and why 

[View Poll](https://www.reddit.com/poll/19crtxp)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19crtxp/d_quick_poll_tensorflow_vs_pytorch_in_2024/,3,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E32BF8510>
19chbs4,Snoo_72181,2024-01-21 23:35:18+00:00,[D] What is state-of-the-art in object detection?,Also what are some good resources to stay updated on state-of-the-art models for various subsets of AI? And what other baseline models to compare them to?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19chbs4/d_what_is_stateoftheart_in_object_detection/,1,6,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E32C0B290>
19bz6l5,Agitated-Ad809,2024-01-21 08:52:50+00:00,[D] Are there any hands on/practical ML YouTube channels?,I have been looking for practical DL or ML paper implementation or hands on YouTube channels. Are there any channels you'd recommend?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/,19,42,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E32C1CDD0>
19ccuw5,APaperADay,2024-01-21 20:27:35+00:00,[R] Leveraging Large Language Models for NLG Evaluation: A Survey,"**Paper**: [https://arxiv.org/abs/2401.07103](https://arxiv.org/abs/2401.07103)

**Abstract**:

>In the rapidly evolving domain of Natural Language Generation (NLG)  evaluation, introducing Large Language Models (LLMs) has opened new  avenues for assessing generated content quality, e.g., coherence,  creativity, and context relevance. This survey aims to provide a  thorough overview of leveraging LLMs for NLG evaluation, a burgeoning  area that lacks a systematic analysis. We propose a coherent taxonomy  for organizing existing LLM-based evaluation metrics, offering a  structured framework to understand and compare these methods. Our  detailed exploration includes critically assessing various LLM-based  methodologies, as well as comparing their strengths and limitations in  evaluating NLG outputs. By discussing unresolved challenges, including  bias, robustness, domain-specificity, and unified evaluation, this  survey seeks to offer insights to researchers and advocate for fairer  and more advanced NLG evaluation techniques.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ccuw5/r_leveraging_large_language_models_for_nlg/,0,3,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E32C1B910>
19bzlxd,ShoeStatus2431,2024-01-21 09:22:12+00:00,[Discussion] Re-using state from LLM's / next-token predictors as an optimization,"I've been pondering how GPT-3/4 must work internally and possible optimizations. I'm wondering if someone could point me to research already done in this area -- or if I completely misunderstand how these models work.

So basically I'm wondering about the 'next-token' predictor aspect. Despite their function of predicting the next token, it seems evident to me that these models must have an internal process (developed in a 'black box' fashion during training) that anticipates the rest of the response. This anticipation appears necessary to prevent the model from emitting a next token that causes a dead-end, making it impossible to construct a coherent sentence.

Moreover, this foresight seems to extend beyond single sentences. GPT-4 responses often exhibit a highly structured format, including an introduction, in-depth analysis, and conclusion, indicating a higher level of planning or disposition in the answer. This leads me to believe that even though the model generates only one next token at a time, it might internally form a more complete response to ensure a well-chosen next token. Likely not in a way that the full response is hashed out in the normal token representation, but at least some internal representation close to this. Especially for shorter ranges (sentences) I imagine it could be quite precise, but over longer ranges (paragraphs etc.) perhaps it is more and more abstract.

This understanding raises a question: Is there a way to extract more of the full response from the network directly? Currently, it seems that the entire calculation is repeated with each token, taking into account the previously emitted token. I suspect that a significant portion of these calculations could be similar, or at least there might be a more efficient pathway to generate the complete answer from the internal state after the first token is emitted.

In practice, this might involve altering the model to produce longer or complete responses in each iteration, rather than just a single token. Alternatively, a secondary, smaller model could be developed to 'peek' into the internal state of the primary model after one token generation and generate complete answers from that. Or perhaps, a model trained in a way that allows for reusing internal states, thereby accelerating the generation of subsequent tokens. Perhaps something like simply restarting from the same state or shifting it somehow.

I'm curious about the feasibility of these ideas and whether they align with the current understanding of LLMs. I look forward to hearing your thoughts, especially if there are fundamental misunderstandings in my assumptions about how LLMs work.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bzlxd/discussion_reusing_state_from_llms_nexttoken/,56,28,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E32C47DD0>
19c90vo,Separate-Still3770,2024-01-21 17:48:07+00:00,[D] Preferred fine-tuning framework for instruction tuning?,"Hi everyone,

I am having a look at the different frameworks to fine-tune an LLM on a small private dataset. I don't want to go for something fancy, as I am not trying to develop custom training procedure, but just use state of the art models, fine-tuned on my data.

Therefore my criteria are mostly ease of use, availability of SOTA models like Mistral, community support, and performance (aka latest methods to train fast are implemented).

I am looking at the different options, and so far found that the most established ways (seemingly) to quickly fine-tune LLMs with SOTA models are to use:

\- [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main)

\- [Hugging Face TRL](https://github.com/huggingface/trl)

Axolotl seems to be a framework picking up speed and makes the training quite compact. Hugging Face frameworks seem to be slightly less user-friendly, but seem to provide more customisation.

What is your opinion on each, and do you have other frameworks you would recommend?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c90vo/d_preferred_finetuning_framework_for_instruction/,3,5,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32C1BE50>
19c5fra,Snoo_72181,2024-01-21 15:12:02+00:00,[P] I want to create a Large Vision Model (LVM) for Robotics,Any open source that I can contribute to? I am open to creating one from scratch too,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c5fra/p_i_want_to_create_a_large_vision_model_lvm_for/,4,7,0.74,<praw.models.comment_forest.CommentForest object at 0x0000026E32C288D0>
19c9f2f,kekkimo,2024-01-21 18:04:24+00:00,[D] Which multi-turn conversation datasets are chat LLMs fine-tuned on?,Which multi-turn conversation datasets are chat LLMs fine-tuned on and how the reward model is trained to have preference over conversations?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c9f2f/d_which_multiturn_conversation_datasets_are_chat/,0,2,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32C272D0>
19cg4en,BigDreamx,2024-01-21 22:43:12+00:00,[D] Confused,"Some people told me in my previous post that I cannot submit to multiple workshops (whether at the same conference or different), but I thought this is allowed as long as all the workshops don't have proceedings. Can someone explain?

Also, I can submit to a conference and a workshop at the same time right? For instance, to ICML 2024 and ICLR 2024 Workshop.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cg4en/d_confused/,2,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32C333D0>
19bkcqz,Wiskkey,2024-01-20 19:58:31+00:00,[R] Are Emergent Abilities in Large Language Models just In-Context Learning?,"[Paper](https://arxiv.org/abs/2309.01809). I am not affiliated with the authors.

Abstract:

>Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.

The authors discuss the work [here](https://h-tayyarmadabushi.github.io/Emergent_Abilities_and_in-Context_Learning/).

>However, our research offers a different perspective, addressing these concerns by revealing that the emergent abilities of LLMs, other than those which are linguistic abilities, are not inherently uncontrollable or unpredictable, as previously believed. Rather, our novel theory attributes them to the manifestation of LLMs’ability to complete a task based on a few examples, an ability referred to as “in-context learning” (ICL). We demonstrate that a combination of ICL, memory, and the emergence of linguistic abilities (linguistic proficiency) can account for both the capabilities and limitations exhibited by LLMs, *thus showing the absence of emergent reasoning abilities in LLMs*.

One of the work's authors discusses the work in [this video](https://www.youtube.com/watch?v=I_38YKWzHR8&t=3110).

The work is discussed in [this Reddit post](https://www.reddit.com/r/singularity/comments/16f87yd/no_evidence_of_emergent_reasoning_abilities_in/) (280+ comments). One of the work's authors posted comments there, including [this summary of the work](https://www.reddit.com/r/singularity/comments/16f87yd/comment/k328zm4/). [Here](https://www.reddit.com/user/H_TayyarMadabushi/comments/) are u/H_TayyarMadabushi 's Reddit comments, which as of this writing are entirely about the work.

The work is discussed in [this blog post](https://blog.cprompt.ai/demystifying-large-language-models-what-abilities-are-truly-emergent) (not by any of the work's authors).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bkcqz/r_are_emergent_abilities_in_large_language_models/,58,98,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32C76650>
19bqy3b,rlresearcher,2024-01-21 00:54:18+00:00,[R] Self-Rewarding Language Models,"Abstract: 

We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.

[https://arxiv.org/abs/2401.10020](https://arxiv.org/abs/2401.10020)  
",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bqy3b/r_selfrewarding_language_models/,7,32,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E32C0CAD0>
19c45lq,tepes_creature_8888,2024-01-21 14:07:52+00:00,[D] Post train generalization methods,"Are there any post train generalization methods? Suppose, you have trained a model and you see it is overfitted, and you want to slightly alter the weights so that model would show less overfitting? 

I can imagine some basic approaches as fine-tuning with introducing generalization things ( e.g. L2 + dropout training for 5 more epochs ) if model has not used it, but are there any papers with evaluation of what works best in such cases?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c45lq/d_post_train_generalization_methods/,0,3,0.71,<praw.models.comment_forest.CommentForest object at 0x0000026E32C6D1D0>
19c8yde,lostinspaz,2024-01-21 17:45:20+00:00,[D] existing python implementation for n-dimentional triangulation?,"I have a project I want to achive, where I figured out, it should be relatively straightforward... All I needed to do was use n-dimentional triangulation.

Then I read that is a not a straightforward calculation :-/  


Trawling through some google results, I read

>""Is it possible to construct a triangulation by choosing the points in the space as we go along?"": The answer is Yes. This is known as the incremental algorithm

  
So, ideally, a pointer to a pre-existing python implementation of that would be appreciated.

That being said, in the interests of efficiency and whatnot, I should probably describe the actual problem, so here goes:  


I want to start with a set of N+1 points, in an N dimentional space ( N <=1024, if it matters)  
I will also have a set of N+1 distances, related to each of those points.

I want to be able to generate a new point that best matches the distances to the original points, with the understanding that it is quite likely that the distances are approximate, and may not cleanly designate a single point. So some ""best fit"" approximation will most likely be required.  


&#x200B;

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c8yde/d_existing_python_implementation_for_ndimentional/,0,1,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E32C77ED0>
19bzv9a,Spiritual_Guide6862,2024-01-21 09:40:24+00:00,[R] Large Action models,"Should i start studying LAMs or the hype would gone after few months , i’m intersted in the field but i don’t think rabbit’s R1 will be successful for many reasons including the reallyhigh  latency.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bzv9a/r_large_action_models/,17,4,0.58,<praw.models.comment_forest.CommentForest object at 0x0000026E32C924D0>
19bzg00,GodCREATOR333,2024-01-21 09:10:45+00:00,[Discussion] Is it possible to use a Rtx 4070 12gb and a Rtx 3060 12gb together in a single pc for LLM's and other applications that might be benefited by this config?, I cannot afford a 24gb graphics card. Rtx 4070 serves for main gaming and Rtx 3060 will be used along with 4070 for LLM's that require high vram and other applications like blender etc. ,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bzg00/discussion_is_it_possible_to_use_a_rtx_4070_12gb/,11,5,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32C84950>
19bd8ua,ragavsachdeva,2024-01-20 14:44:04+00:00,[R] The Manga Whisperer: Automatically Generating Transcriptions for Comics,"Paper: [http://arxiv.org/abs/2401.10224](http://arxiv.org/abs/2401.10224)

Github: [https://github.com/ragavsachdeva/magi](https://github.com/ragavsachdeva/magi)

Try it yourself: [https://huggingface.co/spaces/ragavsachdeva/the-manga-whisperer/](https://huggingface.co/spaces/ragavsachdeva/the-manga-whisperer/)

TLDR: Given a high resolution manga page as input, Magi (our model) can (i) detect panels, characters, text blocks, (ii) cluster characters (without making any assumptions about the number of ground truth clusters), (iii) match text blocks to their speakers, (iv) perform OCR, (v) generate a transcript of who said what and when (by sorting the panels and text boxes in the reading order). See the figure below for an example.

Wanted to share something I've been working on the last few months and I hope that other people find it useful:)

I'm particularly pleased with how well the model can detect and cluster characters (despite extreme changes in viewpoint and partial visibility due to occlusion). The text to speaker matching has room for improvement as the model doesn't ""read"" the dialogues (it only tries to match them visually). I'm working towards making it better.

Here is a teaser:

[The predicted panels are in green, text blocks in red and characters in blue. The predicted character identity associations are shown by lines joining the character box centres. Text to speaker associations is not shown but the generated transcript is provided.](https://preview.redd.it/y9awfty5wldc1.png?width=3658&format=png&auto=webp&s=e924c369f360d8527e53e6248d15017c2c7fd254)

I'd be very interested to know if anyone uses this model for cool projects, personal or research. An interesting use case, which I do not have the bandwidth to explore, would be to scrape and automatically annotate large scale manga datasets using Magi to train Manga diffusion models.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bd8ua/r_the_manga_whisperer_automatically_generating/,16,82,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E32CA5A10>
19c48nb,pythoncoursesonline,2024-01-21 14:12:09+00:00,Machine Learning Specialization by Andrew NG [Discussion],,MachineLearning,https://pythoncoursesonline.com/machine-learning-specialization/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32C93090>
19bxusa,nmfisher,2024-01-21 07:22:03+00:00,[P] Generate & preview 3D Skeletal Animations (Momask),,MachineLearning,https://ixlabs.app,1,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32C84FD0>
19c31fn,lanytho,2024-01-21 13:07:10+00:00,[R] Hosting for CPU intensive simulation app," I'm looking for a service where I can host my python simulation app which is very resource intensive. For each session a dedicated CPU is needed.

Are there any services where each session of my app can have a dedicated CPU and I can share the app with my colleagues?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c31fn/r_hosting_for_cpu_intensive_simulation_app/,1,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E32C9BDD0>
19bgoug,APaperADay,2024-01-20 17:19:46+00:00,[R] Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model,"**Paper**: [https://arxiv.org/abs/2401.09417](https://arxiv.org/abs/2401.09417)

**Code and Models**: [https://github.com/hustvl/Vim](https://github.com/hustvl/Vim)

**Abstract**:

>Recently the state space models (SSMs) with efficient hardware-aware  designs, i.e., Mamba, have shown great potential for long sequence  modeling. Building efficient and generic vision backbones purely upon  SSMs is an appealing direction. However, representing visual data is  challenging for SSMs due to the position-sensitivity of visual data and  the requirement of global context for visual understanding. In this  paper, we show that the reliance of visual representation learning on  self-attention is not necessary and propose a new generic vision  backbone with bidirectional Mamba blocks (**Vim**), which marks the image  sequences with position embeddings and compresses the visual  representation with bidirectional state space models. On ImageNet  classification, COCO object detection, and ADE20k semantic segmentation  tasks, Vim achieves higher performance compared to well-established  vision transformers like DeiT, while also demonstrating significantly  improved computation & memory efficiency. For example, Vim is 2.8×  faster than DeiT and saves 86.8% GPU memory when performing batch  inference to extract features on images with a resolution of 1248×1248.  The results demonstrate that Vim is capable of overcoming the  computation & memory constraints on performing Transformer-style  understanding for high-resolution images and it has great potential to  become the next-generation backbone for vision foundation models. Code  is available at [this https URL](https://github.com/hustvl/Vim).

https://preview.redd.it/gf2b6teuomdc1.png?width=2880&format=png&auto=webp&s=3aece9b012541f8aa20dcee50eedb68bd9bed7c6",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bgoug/r_vision_mamba_efficient_visual_representation/,5,31,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E32CA7ED0>
19c8qv1,NoArmy6203,2024-01-21 17:36:29+00:00,[D] The steps for a good fraud detection," Hi I am a ML enthusiastic and I would like to know what are the right steps for an efficient fraud detection. For example what KPI, error, validation steps, and iusses are useful for a good project. If you can also write a list of actions, like : first step - check the data ...-second step .... 

Thank you so much ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c8qv1/d_the_steps_for_a_good_fraud_detection/,0,0,0.17,<praw.models.comment_forest.CommentForest object at 0x0000026E327F1C50>
19cd4ad,cardogio,2024-01-21 20:38:17+00:00,[D] Generative AI in vehicle search process & ownership cycle?,"I've been pondering the potential of generative AI in reshaping our experiences in car search and ownership. The core idea is about using something like a Retrieval Augmented Generation (RAG) pipeline, perhaps with an open-source model, feeding on a vast and diverse automotive content corpus.

I'm curious about a few aspects and would love to get your insights:

**AI-Driven Car Searching**: How do you think generative AI could change the way we search for cars? Imagine an AI that can provide not just car recommendations but contextual, in-depth information. Could this be a game-changer or just another toy whose novelty wears off in a week? 

**AI in Car Ownership**: There's a plethora of issues car owners face - maintenance questions, troubleshooting, and more. Where do you see generative AI stepping in to assist?

**Content for AI**: Considering a large corpus of automotive content for training such a system, what type of content would be most beneficial? Should we focus on technical specs, user reviews, or maintenance guides?

**Optimization and Challenges**: What challenges might we face in implementing generative AI in this domain? I'm thinking about accuracy, ethical considerations, and maintaining up-to-date information.

**Your Experiences**: Have there been moments where AI could've enhanced your car search or ownership experience? What did you wish for in those moments?

If you think about it, finding a car has been the same for over 100 years, even with the advent of the internet, the process still requires many actions most consumers (especially tech conscious people) hate. Negotiating with a sleezy sales rep, dealing with the dealerships hidden fees, and then owning the vehicle is like a game of financial roulette. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cd4ad/d_generative_ai_in_vehicle_search_process/,6,0,0.27,<praw.models.comment_forest.CommentForest object at 0x0000026E32CA6810>
19bkmiz,grokland,2024-01-20 20:10:08+00:00,[D] Where to find new or inspiring ML projects or approaches to learn from? Not necessarily cutting-edge ML,"Hello there,

I'm an ML engineer, and as all of us I do my best to keep up with ML/AI, not only SOTA but also different approaches or techniques that others practitioners use. 

There are plenty of discussions in here about how to do stay up to date with research (Youtube channels, Podcasts, Newsletters, Scientific Journals, .. you name it), but I feel those tend to be about complex problems solved by huge models that require huge GPU to train. And that's great and there's plenty to learn from it, but in my experience those are not the problems that we face either on our jobs or on our side projects. Or at least it's not the content I'd like to learn more about.   


I'm trying to find resources where to learn about how others have solved medium-size projects or how they've solved the obstacles they found along the way. I mean those tiny tricks you come up with that make all the difference - like having to preprocess the data differently (like adding the day of the week to the features, or use a different embedding or normalize in a different way), change the metric, doing dropout in a particular way, switch from RNN to LSTM ... This is the kind of thing you learn from Senior colleagues at work (1-2 people max if you're lucky), so there must be a better way.

My best resource for this so far is Kaggle, and I really enjoy seeing other people's approaches to data processing and modelling. Is there anything else you guys use?

All comments are appreciated. Thank you! ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bkmiz/d_where_to_find_new_or_inspiring_ml_projects_or/,9,14,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32CACC50>
19cc25y,Puromalandreo,2024-01-21 19:54:27+00:00,[D] I need help creating a simple tool,"I want to create a tool that learns the difference between a “kitchen” a “bathroom” and a “bedroom”. The tool would then be able to classify and categorised them into different folders by itself. 

Sounds simple but it’s been very complicated to code this and train the machine to do it. 

I’m new to coding and I’m using python. I actually don’t know much about coding and I have been coding more of the stuff with ChatGPT, if someone has any suggestions would be appreciate it",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19cc25y/d_i_need_help_creating_a_simple_tool/,9,0,0.13,<praw.models.comment_forest.CommentForest object at 0x0000026E32CB9690>
19bfkz2,Successful-Western27,2024-01-20 16:30:46+00:00,[R] Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering (Proposed method raises accuracy from 19% to 44% on benchmarks),"Research (and at least for me, painful personal experience) suggests that prompt engineering alone has inherent limitations when tackling complex coding challenges.

In a paper published on arXiv, the authors of a new study propose a novel iterative approach called AlphaCodium that focuses on repeatedly generating, executing, and debugging code against test cases. This concrete feedback loop allows LLMs to ""learn"" critical programming skills through iteration.

When evaluated on the competitive programming benchmark CodeContests, AlphaCodium increased code generation accuracy for GPT-4 from 19% to 44%. It also exceeded prior published methods such as AlphaCode while utilizing 10,000 times fewer model queries by avoiding brute force generation.

The principles employed in AlphaCodium are:

* Test-driven development provides an objective fitness function
* Modular coding
* Expanding test coverage reveals generalizability gaps
* Anchoring against known tests to prevent regressions

The researchers argue these software engineering practices are better suited for code generation compared to treating models as generic text generators. While more experimentation is needed, the test-debug loop demonstrated by AlphaCodium might point towards more capable AI programming techniques.

[Full summary is here](https://aimodels.substack.com/p/flow-engineering-doubles-code-generation). Paper is [here](https://arxiv.org/pdf/2401.08500.pdf). Repo is [here](https://github.com/Codium-ai/AlphaCodium).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bfkz2/r_code_generation_with_alphacodium_from_prompt/,4,25,0.87,<praw.models.comment_forest.CommentForest object at 0x0000026E32CAE850>
19b7i8s,mango-clay,2024-01-20 08:50:11+00:00,[D] Lesser known Research Areas ML,"[D]
What are some lesser-known or less explored areas in machine learning that u find interesting ? 
(Broder, not highly specialized ideas or topics) 
I'm seeking some areas so that I can study and find about them.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b7i8s/d_lesser_known_research_areas_ml/,65,86,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E32CF65D0>
19bi4bc,APaperADay,2024-01-20 18:22:03+00:00,[R] A generative model of memory construction and consolidation,"**Paper**: [https://www.nature.com/articles/s41562-023-01799-z](https://www.nature.com/articles/s41562-023-01799-z)

**Preprint version(s)**: [https://www.biorxiv.org/content/10.1101/2023.01.19.524711](https://www.biorxiv.org/content/10.1101/2023.01.19.524711)

**Code**: [https://github.com/ellie-as/generative-memory](https://github.com/ellie-as/generative-memory)

**Abstract**:

>Episodic memories are (re)constructed, share neural substrates with   imagination, combine unique features with schema-based predictions and   show schema-based distortions that increase with consolidation. Here we   present a computational model in which hippocampal replay (from an   autoassociative network) trains generative models (variational   autoencoders) to (re)create sensory experiences from latent variable   representations in entorhinal, medial prefrontal and anterolateral   temporal cortices via the hippocampal formation. Simulations show   effects of memory age and hippocampal lesions in agreement with previous   models, but also provide mechanisms for semantic memory, imagination,   episodic future thinking, relational inference and schema-based   distortions including boundary extension. The model explains how unique   sensory and predictable conceptual elements of memories are stored and   reconstructed by efficiently combining both hippocampal and neocortical   systems, optimizing the use of limited hippocampal storage for new and   unusual information. Overall, we believe hippocampal replay training   generative models provides a comprehensive account of memory   construction, imagination and consolidation.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bi4bc/r_a_generative_model_of_memory_construction_and/,3,13,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32CBE390>
19bydva,BigDreamx,2024-01-21 07:58:38+00:00,[D] Multiple Workshop,"If I get accepted to multiple workshops, do I have to withdraw one?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bydva/d_multiple_workshop/,16,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32CD2190>
19c3h4p,According-Pirate-725,2024-01-21 13:31:53+00:00,[P] AI based content generation,"Hey everyone,  
I  am working on a project for one of my university courses, I am under a  lot of pressure this semester and would be grateful if anyone can help  me out on this.

I have to build a  ML model which can generate classes of 3D objects like Cars, Chairs,  Trees. My primary idea is to use a GAN to build voxel or point cloud  object but all good implementations seem too complicated and its a  matter of time for me.  
If you have any ideas or can provide me with relevant research papers or github repos that would save me a lot of trouble.  
Thanks in advance.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c3h4p/p_ai_based_content_generation/,1,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E32CF8850>
19bkzax,tg1482,2024-01-20 20:25:37+00:00,[P] PriomptiPy - A python library to budget tokens and dynamically render prompts for LLMs,,MachineLearning,https://i.redd.it/0o0womxmmndc1.gif,3,8,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32CC7550>
19bhztz,ml_dnn,2024-01-20 18:16:45+00:00,[R] Reinforcement Learning," A Survey Analyzing Generalization in Deep Reinforcement Learning

[https://twitter.com/EzgiKorkmazAI/status/1744434469107335628](https://twitter.com/EzgiKorkmazAI/status/1744434469107335628)

Abstract:

Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with improved generalization abilities.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bhztz/r_reinforcement_learning/,0,8,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E32CC7510>
19c26n2,NoteDance,2024-01-21 12:15:36+00:00,[P] Machine learning library.,"Hello everyone, I wrote a machine learning library, does anyone want to try it?",MachineLearning,https://github.com/NoteDance/Note,1,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E32CCE590>
19bejd5,vakker00,2024-01-20 15:44:06+00:00,[D] The truth about noise schedulers for latent diffusion models?,"This is meant to be an open discussion about noise schedulers used for LDMs, such as Stable Diffusion in particular. One thing that I can't get my head around, is that either I don't understand something fundamental, or there's a general misunderstanding within the SD community about what the sampler is supposed to be for.

For example [this post](https://stable-diffusion-art.com/samplers/) claims:

>This **denoising process** is called **sampling** because Stable Diffusion generates a new sample image in each step. The method used in sampling is called the **sampler** or **sampling method**.

Also, in general there's a lot of discussion around which sampler to use and their characteristics (see [this](https://www.reddit.com/r/StableDiffusion/comments/zl494r/sampler_info_for_a_noob/)), etc., and how they are used to simulate the ODEs of diffusion of the noise.

The [original LDM paper](https://arxiv.org/abs/2112.10752) doesn't go into much detail about the sampler, it just mentions that they use [DDIM](https://arxiv.org/abs/2010.02502), and based on their description (and numerous implementation examples) the noise is sampled during the FORWARD diffusion process (i.e. when the random noise is added to the image and dispersed at each step), and then the model needs to predict this noise and remove it during the REVERSE diffusion process. I.e. the noise sampler needs to know how the noise is added (simulate the diffusion process) during training, but during inference the model should just undo the noise and there's no sampling involved.

E.g. using the Euler sampler to illustrate this with a sigma that depends on the step (because that's one of the simplest mathematically):

```
# forward diffusion
noise = normal(0, 1)
noisy_sample = original_sample + noise * sigma

# reverse diffusion
pred_noise = model(noisy_sample)
denoised_sample = noisy_sample - sigma_hat * pred_noise
```

I.e. it's just the opposite of adding the noise, there's no ""sampling"" during the inference step. For reference see [this implementation](https://github.com/huggingface/diffusers/blob/v0.11.0/src/diffusers/schedulers/scheduling_euler_discrete.py#L48). 

Now, my main question is: if the model was trained to predict the noise that was added by a particular algorithm (DDIM in the case of SD), then shouldn't we use that particular algorithm during inference? 

My theory is that SD is meant to be used with DDIM. However, I guess the community figured that using other samplers the denoising process becomes less accurate with additional imperfections, which helps to generate more diverse (or simply just different) results. But then I would claim that using DDIM with additionally injected noise into the denoising would produce the same effect (haven't tested this yet) without the additional confusion and ever increasing number of noise schedulers. 

Of course, there's a place for using other schedulers that are faster by approximating DDIM in less (or faster) steps, e.g. see the [LCM](https://arxiv.org/abs/2310.04378) approach, but then they should be labelled as simply an imperfect replacement, and not as an alternative that's ""better at creating X"".

Let me know what you think, I might be wrong so I'm just trying to get my head around the true intuition behind the vast range of schedulers available.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bejd5/d_the_truth_about_noise_schedulers_for_latent/,9,10,0.82,<praw.models.comment_forest.CommentForest object at 0x0000026E32809410>
19b4hqt,Mucky5739,2024-01-20 05:35:45+00:00,[D] Question about gradient descent in Machine Learning vs Local Maxima and Minima,"Hi, I’m a high schooler learning machine/deep learning, and I recently learned in math that we can find the local minimum value of functions by taking the first and second derivative to find its critical points, and then find the lowest value the function has. 

Why can’t we just find the minimum value of the loss function instead of using gradient descent? It seems much more efficient bc then we don’t need to make a bunch of small adjustments to find the minimum value - we can just calculate it instead

Would that work? It sounds kinda dumb cause like people would have obviously stsrted doing it ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b4hqt/d_question_about_gradient_descent_in_machine/,44,62,0.84,<praw.models.comment_forest.CommentForest object at 0x0000026E32D2A110>
19c6wll,dev-spot,2024-01-21 16:17:45+00:00,[N] Open Models - Revolutionizing AI Interaction with a Unique Twist [News],"\*\* Edited cause people seemed to really hate the way it was previously presented I guess? :/

I'm thrilled to introduce my latest project - Open Models. This isn't just another AI framework, it's a game-changer for how we interact with AI applications.

The idea is simple - an abstraction layer between the AI models (like TTS, TTI, LLM) and the underlying code that powers them. One centralized place from which you can manage all the AI models you'll need in the project, assuming you're working with an ""AI vendor"" of some sort. The real power of this project lies in its simplicity and openness IMO.

Whether you're an experienced developer or just starting out, this project can significantly shortcut debugging time and help you manage all AI requirements for your project following a very structured and guided approach.

Check out the video for detailed explanation and functionality showcase:

[https://youtu.be/AwlCiSkzIPc](https://youtu.be/AwlCiSkzIPc)

Github Repo w/ the full source code & architecture:

[https://github.com/devspotyt/open-models](https://github.com/devspotyt/open-models)

Feel free to subscribe to my newsletter to stay up to date with latest tech & projects I'm running:

[https://devspot.beehiiv.com/subscribe](https://devspot.beehiiv.com/subscribe)

Let me know what you think about it, or if you have any questions / requests for other videos / projects as well,

cheers",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c6wll/n_open_models_revolutionizing_ai_interaction_with/,11,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E32D15610>
19bobpj,Serious-Potential224,2024-01-20 22:53:23+00:00,Machine learning intern [D]," Hello. I am from Ukraine, and i'm writting because i have a situation where i cannot find anything related to ml in my country and cannot relocate because of a war. Can you recommend some companies or something that'll help me with my situation? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bobpj/machine_learning_intern_d/,1,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E3280AFD0>
19bieui,mlfcquestion,2024-01-20 18:34:37+00:00,[P] Image Analysis Framework Recommendations for Flow Cytometry,"I'm trying to determine a good tool or framework to use to assist me in classifying / grouping images of flow cytometry data.

If anyone could point me in the right direction, I would greatly appreciate it.

As an example of the data I am looking to categorize:

* This an example image to be classified:

https://preview.redd.it/upeu4xih2ndc1.png?width=829&format=png&auto=webp&s=052e1d73150c26e00b0584363ff128cd063f5c8c

*  This is my answer:

https://preview.redd.it/bl9fliai2ndc1.png?width=875&format=png&auto=webp&s=ce56418677e57ca3e9b466eb0ba9db7c6ec49375

* This is the 'correct' answer. (correct is in quotes because correct submissions are generated by consensus of submissions)

https://preview.redd.it/j7vxt9ui2ndc1.png?width=858&format=png&auto=webp&s=e09f5a8dbcf78ddb11f5c42184a3323baef9e36a",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bieui/p_image_analysis_framework_recommendations_for/,2,4,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32D07590>
19bw0ux,thatsadsid,2024-01-21 05:27:55+00:00,Do risers affect the speed of data transfer [D],I have two 3080ti attached to the motherboard through risers (x1). Would i see a huge performance degrade as opposed to pcie16?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bw0ux/do_risers_affect_the_speed_of_data_transfer_d/,9,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E32D11E10>
19be59k,xjustwaitx,2024-01-20 15:26:10+00:00,[P] EvolGPT: Expert-Level Performance on Tasks with Environmental Feedback,,MachineLearning,https://github.com/amitlevy/EvolGPT,0,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E327DE790>
19atnu0,Singularian2501,2024-01-19 21:01:45+00:00,[R] Self-Rewarding Language Models - Meta 2024,"Paper: [https://arxiv.org/abs/2401.10020](https://arxiv.org/abs/2401.10020)

Github: [https://github.com/lucidrains/self-rewarding-lm-pytorch](https://github.com/lucidrains/self-rewarding-lm-pytorch)

Abstract:

>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes. 

https://preview.redd.it/l7vav40qngdc1.jpg?width=1344&format=pjpg&auto=webp&s=9dce97a69f2ede66d6dabf6abbcfc75bf0e94f19

https://preview.redd.it/fuooe70qngdc1.jpg?width=1180&format=pjpg&auto=webp&s=a88fcf1c765ff42c18091889f5b14cd371248760",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/,24,141,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E32D41510>
19bbn0j,Master_of_Galaxy,2024-01-20 13:21:17+00:00,[D] Enquiry regarding financial assistance to attend ICLR 2024,"I am an undergraduate student with an accepted spotlight paper in the main conference, but our institution does not have any funding for undergraduate students. I checked last year's ICLR website, and it seems there was a google form to apply for financial aid, and it was rolled out very close to the end of Early registration fees deadline.   


I wanted to know if the financial aid guaranteed once I apply via this year's form when it rolls out? Also what do they cover generally, and what is the mode of reimbursement? As in, do I need to book flight tickets/hotels in advance with my own money, because they might be required for visa application and waiting for the financial aid seems risky.   


Apparently ICLR also has some student volunteering, which would be great if it is paid otherwise spending money from my own pocket as an undergrad to attend the conference seems like a huge financial burden, and I don't want to miss the opportunity either. It would be great if previous benefactors/people who have knowledge about this could weigh in on the topic. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bbn0j/d_enquiry_regarding_financial_assistance_to/,1,8,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32D37E10>
19bpmu6,CyberPotate,2024-01-20 23:52:14+00:00,[D] How do you handle predictions for data that lies outside the scope of the training dataset?,"Hello,

I'm not an expert in the field, so please excuse me if my terminology isn't precise. I'm currently working on a personal project and using some machine learning tools. Right now, I'm trying to predict energy consumption based on temperature and the previous day's consumption, so I've tried several machine-learning models. It seems to be working quite well, and I'm currently focusing on a GAM using the Python pyGAM library. However, I've noticed an issue where my input might be outside the range used in my training set. I'm wondering if there are any solutions to this, without resulting in nonsensical extrapolations.

I had understood that normalizing/standardizing the data might solve this? In my case, the model is very simple, so I hadn't used this approach as the results were already satisfactory. I've done some research, including looking into some books, but I didn't have the energy to delve into numerous chapters since they didn't seem to address my issue at first glance.

Thank you for your help.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bpmu6/d_how_do_you_handle_predictions_for_data_that/,4,1,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E32C033D0>
19c0cfs,FellowArchUser,2024-01-21 10:13:19+00:00,"I want to help solve problems, but I don't know where to look [D]","I want to help people implement ai, or make ai applications public (like linux) so there isint a monopoly on this technology. But i dont even know where to look for problems that need fixing or solutions to be implemented. 

Would anyone have any adivewhere to start. Thank you",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19c0cfs/i_want_to_help_solve_problems_but_i_dont_know/,4,0,0.27,<praw.models.comment_forest.CommentForest object at 0x0000026E32D38850>
19bkkyh,egusa,2024-01-20 20:08:09+00:00,[D] Microsoft CEO Contradicts His Chief Economist About Waiting to Address Unintended Consequences of New Technologies: WEF in Davos,,MachineLearning,https://sociable.co/big-tech/microsoft-ceo-contradicts-economist-technologies-wef-davos/,0,0,0.45,<praw.models.comment_forest.CommentForest object at 0x0000026E32C0A850>
19bkdue,Neurosymbolic,2024-01-20 19:59:56+00:00,"[R] Interview with Zack Serlin, MIT Lincoln Laboratories: Formal methods for...",,MachineLearning,https://youtube.com/watch?v=LqQdcIiox7o&si=W0X1zZAdmhoq79wP,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32D387D0>
19bex1f,AstronautVarious3791,2024-01-20 16:01:22+00:00,[D] How to assign weights to multi-task models?,"It's quite common in recommender systems to train multi-task models which simultaneously try to optimize for multiple objectives. However, one key set of hyperparameters to set here is the weight of each task's loss. The weights are usually chosen in a way that maximizes some business objective (like revenue, retention). So they are usually not learned as part of the training process itself.

Are there any popular or state-of-the-art ways of doing finding these task weights?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bex1f/d_how_to_assign_weights_to_multitask_models/,1,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32D3A910>
19bb439,ZennikOfficial,2024-01-20 12:52:12+00:00,[D] Sound Generation AI Tool,Can you recommend me an AI Tool that can generate sounds? Like if I write that I want the sound of a forest or a synth bass it will generate it.  Thank you.,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bb439/d_sound_generation_ai_tool/,3,1,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E32D39150>
19axat7,WisePalpitation4831,2024-01-19 23:34:27+00:00,"[D] Residual everything, convince me wrong?","changing features directly is a bad idea. This destroys information and leads to terrible issues when recuperating related to ""regression the mean"".

You can imagine each residual layer as a ""worker"" within a organization. Teams are organized into blocks and each team member works diligently cooperatively to prepare the deliverable for the next team (next block). Each team then works in conjunction to fill in for their role (add high freqs, low, etc) and the work is aggregated until the very last layer. a single layer (boss) signs off on all the work and passes it to the client (output)

**If you think about it more, each layer only has to output values ""to enhance x"", as opposed ""to synthesize a new x"".**

The reason I use a ""company"" reference here is to show that the original inputs are never actually destroyed (requirements, deliverable from another team). We wouldn't want to destroy pages of a SOW or TDD and make up our own information for example. Layers in a net need to operate the same,  only contributing new information to features but not destroying previous information. Its exciting to think about the many ways one could aggregate all this information as opposed to a simple residual addition. Multiplicative residuals have been interesting in my experience with converging faster but also taking up much more memory. The whole bottleneck of this approach seems to be the memory, as autoencoders routinely downsample their features and re up sample as opposed to keeping the dimension the same or extending it. Would love to hear thoughts.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19axat7/d_residual_everything_convince_me_wrong/,47,20,0.66,<praw.models.comment_forest.CommentForest object at 0x0000026E32D6F090>
19b66wx,Glittering_Revenue19,2024-01-20 07:21:15+00:00,[D] [P] Help Needed! Implementing Semi-Supervised Learning on Brain Tumor Classification,"Hello, I am new to machine learning and I am doing this project where I try to classify different types of brain tumors using Semi-Supervised Learning. I have tried to run my code and the results definitely seems odd (ex. ""perfect confusion matrix"").

I was wondering if I can get any help from any experts. Please PM me and I can send you the code and the reference code that I used.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b66wx/d_p_help_needed_implementing_semisupervised/,1,5,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32D4BE90>
19bm14v,worstthingsonline,2024-01-20 21:11:32+00:00,[D] Data used for ML models in scientific/technical use cases,"I'm interested in applications of ML to problems in science (think AlphaFold, GNoME etc.). With a lot of other tasks (CV, NLP etc.) the data is quite obvious (images, text etc.) but I don't really understand what kind of data is actually used to train a model like e.g. AlphaFold or GNoME. I imagine they use output from numerical simulations, 3D structures of molecules, etc., but I can't find any good resources on how they actually transform this into data that is usable for a model. Some general questions I have include

* What kind of data is used?
* What is the format of the data? 
* How is the data stored/managed at scale?
* How is the data cleaned/transformed?
* What are some general characteristics of this data?
* How do practitioners think about designing model architecture when working with this kind of scientific data?

Any examples, references or resources would be greatly appreciated!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bm14v/d_data_used_for_ml_models_in_scientifictechnical/,1,0,0.3,<praw.models.comment_forest.CommentForest object at 0x0000026E32D66990>
19b01uc,seventh_day123,2024-01-20 01:39:52+00:00,[P] [D] Starting the Training Journey: An Open-Source RLHF Full-Scale Training Framework for Building 70B+ Models Based on Ray and vLLM,"# Background

ChatGPT has been around for over a year now, and RLHF training is an indispensable part of training ChatGPT. Currently, there are already quite a few open-source RLHF training frameworks such as TRL, DeepSpeedChat or the recently popular LLaMA Factory. These frameworks are often based on parallelization methods like ZeRO, slicing the four models in the RLHF algorithm and placing them on the same GPU. In today's era of ever-larger model sizes, such scheduling cannot meet the needs of full-scale RLHF training for 70B+ or even just 13B+ models. It requires compromising on memory usage through merging the Actor Critic models or using methods like LoRA. However, these PEFT methods often mean compromising model performance.

Thus the open-source project

**OpenRLHF** 

[https://github.com/OpenLLMAI/OpenRLHF](https://github.com/OpenLLMAI/OpenRLHF)

was born. We redesigned the model scheduling based on Ray and vLLM:

1. For small 7B models, we place all models on the same GPU.
2. For medium 13B\~34B models, we use Ray to place the four models in PPO on different GPUs to enable full-scale fine-tuning.
3. For large 34B+ models, we use vLLM's TP parallelization to load the Actor model, with the other models still scattered across different GPUs using Ray.

# ZeRO2 + Adam Offload + Pinned Memory

For models smaller than 34B, we use an optimization scheme with ZeRO2 + Adam Offload + Pinned Memory. Our basic thinking is:

1. We found that 80% of the time in the RLHF training process is used for sample generation and inference with the GPT model. This is because GPT model's autoregressive decoding has O(n\^2) complexity and is usually memory bound.
2. The simplest way to improve inference efficiency is to avoid being memory bound and enhance GPU compute efficiency by increasing matrix multiplication size. But large matrix multiplications mean large batch\_sizes, leading to huge KV cache memory demands.
3. So we thought of freeing memory by offloading Adam optimizer weights to CPU memory, and using Pinned Memory to avoid GPU-CPU communication efficiency issues during gradient aggregation. Now we can not only increase batch\_size with the saved memory, but also use ZeRO2 to avoid the huge communication overhead caused by model slicing.
4. For 13B+ models, we find ZeRO2 cannot fit the four models on A100's 80G memory, so we place the models on separate GPUs using Ray. However, we assign more GPUs to Actor to reduce GPU idleness.

With this optimization strategy, we tested on a 13B model and achieved 4 times the training efficiency of DeepSpeedChat.

https://preview.redd.it/c14z9vl90idc1.png?width=1179&format=png&auto=webp&s=4a5226f201219fa57a5d9b7abf215e8ce1200db9

#  Ray + vLLM Architecture

However, for 34B+ models, we found that even using Ray to place models on separate cards, we still could not fit them.

So for the Actor inference module, we optimized distributed inference based on vLLM's TP parallelization and dynamic batching capabilities. For the other modules (i.e. the training modules for Actor/Critic and the inference modules for Reward/RefActor), since they only do one forward or backward pass, we use ZeRO3 for parallel training. The architecture is shown below:

&#x200B;

&#x200B;

https://preview.redd.it/hre3hjlk0idc1.png?width=1442&format=png&auto=webp&s=65123a45e1b4a85d83779b2d8b39962fb5a087ef

 

Every PPO training iteration, the updated weights from the DeepSpeed ZeRO3 training framework are sent to the vLLM inference engine. We implement this process using NVIDIA NCCL's high-performance communication. Given vLLM's high-performance inference capabilities, we achieve good performance gains. Further, we can fuse the Actor training nodes and inference nodes to reuse nodes and avoid GPU idleness, since these two modules do not work simultaneously.

With this, we have implemented a 70B+ model RLHF training scheme using Ray and vLLM, and our scheme is seamlessly compatible with the Huggingface Transformers library without needing to manually modify the model architecture like with Megatron-LM.

# PPO Implementation Tricks

In addition to system architecture optimizations, we further integrated RLHF algorithm optimizations. According to two classic PPO papers:

[**https://arxiv.org/abs/2005.12729**](https://arxiv.org/abs/2005.12729)

[**https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/​iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/**](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/​iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

There are many subtleties and hyperparameter tuning techniques in PPO algorithm implementation details.  In OpenRLHF we integrate all these Implementation Tricks, achieving stable training and convergence for the PPO training algorithm.

# Support for Multiple Alignment Algorithms

We not only implemented PPO, but also provide support for DPO/Rejection Sampling/Conditional SFT and other alignment algorithms.

See the OpenRLHF project [Readme.md](https://github.com/OpenLLMAI/OpenRLHF) for details.

# Quick Start

After installing the environment dependencies, we just need to submit the training job with Ray. OpenRLHF's models and datasets are fully compatible with the HuggingFace format, including popular MoE models like Mixtral 8\*7b, simply specify the model name or local directory path.

    # Luanch Ray
    nohup ray start --head --node-ip-address 0.0.0.0 --num-gpus 8 --block &> ray.log &
    
    # Submit Ray task
    ray job submit --address=""http://127.0.0.1:8265"" \
        --runtime-env-json='{""working_dir"": ""/openrlhf"", ""pip"": ""/openrlhf/requirements.txt""}' \
        --no-wait \
        -- python3 examples/train_ppo_ray.py \
        --ref_num_nodes 1 \               # ref policy node count
        --ref_num_gpus_per_node 2 \       # ref policy gpu count
        --reward_num_nodes 1 \            # reward model  node count
        --reward_num_gpus_per_node 2 \    # reward model gpu count
        --critic_num_nodes 1 \            # critic  node count
        --critic_num_gpus_per_node 4 \    # critic gpu count
        --actor_num_nodes 1 \             # actor   node count
        --actor_num_gpus_per_node 4 \     # actor  gpu count
        --vllm_num_engines 2 \            # actor  vllm node count
        --vllm_tensor_parallel_size 2 \   # actor vllm gpu count
        --pretrain meta-llama/Llama-2-70b-chat-hf \            # Actor pretrain model
        --reward_pretrain meta-llama/Llama-2-70b-chat-hf \     # Reward pretrain model
        --save_path /mnt/bn/wuxibin/cache/ckpt/llama_70b \     
        --micro_train_batch_size 1 \
        --train_batch_size 128 \
        --micro_rollout_batch_size 2 \
        --rollout_batch_size 1024 \
        --max_epochs 1 \
        --prompt_max_len 1024 \
        --generate_max_len 1024 \
        --zero_stage 3 \
        --bf16 \
        --actor_learning_rate 5e-7 \
        --critic_learning_rate 9e-6 \
        --init_kl_coef 0.01 \
        --prompt_data Open-Orca/OpenOrca,Dahoas/full-hh-rlhf,tasksource/oasst1_pairwise_rlhf_reward \  # dataset
        --prompt_data_probs 0.4,0.5,0.1 \                                                              # dataset mix probs
        --max_samples 80000 \                                                                          # max number of samples
        --normalize_reward \                                                                           # Reward Normalization
        --actor_init_on_gpu \
        --adam_offload \                                             
        --flash_attn \
        --gradient_checkpointing

 For SFT/Reward model training, we also provide the corresponding implementations. Simply run the deepspeed command directly. 

    # Reward Model training
    deepspeed ./train_rm.py \
         --save_path ./ckpt/7b_llama \
         --save_steps -1 \
         --logging_steps 1 \
         --eval_steps -1 \
         --train_batch_size 128 \
         --micro_train_batch_size 1 \
         --pretrain OpenLLMAI/Llama-2-7b-sft-model-ocra-500k \
         --bf16 \
         --max_epochs 1 \
         --max_len 2048 \
         --zero_stage 3 \
         --learning_rate 9e-6 \
         --dataset Anthropic/hh-rlhf,tasksource/oasst1_pairwise_rlhf_reward,lmsys/chatbot_arena_conversations,openai/webgpt_comparisons \
         --dataset_probs 0.72,0.08,0.12,0.08 \
         --flash_attn \
         --gradient_checkpointing

&#x200B;

    # SFT model training
    deepspeed ./train_sft.py \
        --max_len 2048 \
        --dataset Open-Orca/OpenOrca \
        --dataset_probs 1.0 \
        --train_batch_size 128 \
        --micro_train_batch_size 2 \
        --max_samples 500000 \
        --pretrain meta-llama/Llama-2-7b-hf \
        --save_path ./ckpt/7b_llama \
        --save_steps -1 \
        --logging_steps 1 \
        --eval_steps -1 \
        --zero_stage 2 \
        --max_epochs 1 \
        --bf16 \
        --flash_attn \
        --learning_rate 5e-6 \
        --gradient_checkpointing

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b01uc/p_d_starting_the_training_journey_an_opensource/,1,11,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E32B86E90>
19bbda8,kekkimo,2024-01-20 13:06:19+00:00,[D] How does Mixtral outperform Chatgpt 3.5?,"Chatgpt was supervised fine-tuned then RLHF. While Mixtral was just supervised fine-tuned.

How does Mixtral have a better performance with just SFT?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bbda8/d_how_does_mixtral_outperform_chatgpt_35/,11,1,0.52,<praw.models.comment_forest.CommentForest object at 0x0000026E32D4C190>
19bunqj,thatsadsid,2024-01-21 04:08:25+00:00,Do i need a 3090 for machine learning as a beginner? [D],"Hi.
I have 3090 + 2x3080 ti. 

I was thinking to sell both the 3080 ti and to get 3090 instead.

Do you think i need 3090 to learn ML as a beginner?

Or what i have is enough?

Thanks",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bunqj/do_i_need_a_3090_for_machine_learning_as_a/,37,0,0.28,<praw.models.comment_forest.CommentForest object at 0x0000026E32D87390>
19b2o00,Distinct-Temp6557,2024-01-20 03:52:33+00:00,[D] Is there any software that can detect and clean up speech impediments?,"I want to start a YT channel, but I have speech related disabilities including stuttering, hyperfast speech, and soft ""r""s.

Is there any software that can detect all of that and clean it up so that I can use it for voiceovers?

I tried looking up generated AI voices to use one of those instead, but that landed me in a sea of console commands, arguably expensive subscription services, and/or questionable commercial licensing policies.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b2o00/d_is_there_any_software_that_can_detect_and_clean/,4,5,0.73,<praw.models.comment_forest.CommentForest object at 0x0000026E32D74450>
19av05k,piml-guy,2024-01-19 21:58:31+00:00,physics-informed machine learning applications [D],"Hi all,

I'm  eager to learn what applications people are using or wanting to use physics-informed machine learning (PIML) for. I'm developing a new  platform for building and running PIMLs to help people speed-up and  scale-up their physics simulations. I've been working with a few  companies/university groups on PIMLs for circuit design, but I'm curious  what else people are thinking of using them for and what problems they  have faced. For example, are you using PIMLs for air flow modeling or  maybe even for building a video game engine?

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19av05k/physicsinformed_machine_learning_applications_d/,9,14,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E32D74510>
19bj2l0,janksm1,2024-01-20 19:02:35+00:00,"[P] RngMon: Pokemon Showdown clone but with randomly generated creatures, like pokemon.","I would like people's thoughts but also i would like to know if anyone would be interested in making this with me. I have already started working on the name and description generator. I don't think this is a project for people who are new to ML so if your interested please keep that in mind.

RngMon:

The idea is to use a model to generate a text based team. Then make a turn based simulator to read the teams and battle with them. Use a text2img model to create sprites using the generated descriptions. Users will be able modify any part of the team and the model will be able to fill in the blanks. Being able to have users edit the descriptions or the names would make for funny teams generated based on it. same thing would be for the abilities. I have a sample format for the features a team/creature would have.

Implementation ideas:

* Have a basic auto encoder that's takes sentence embeddings for each of the features that needs to be generated and compress them into a single embedding. The decoder will take the embedding and would have different heads for each feature (name, desc, type, move1, ...). This is good for being able to generating samples from a latent space. This is not good for when users want to edit the name or description because it wont necessarily stay consistent with the output.
* Use a casual transformer to generate the team. Input would be a template string that the model would then try to fill in the blanks with. This is good for when users want to edit the name or description because the transformer will not change its input values. This is not good for  generating random samples.  

Team format Example:

creature1:

  name: string

  desc: string

  type1: string

  type2: string

  hp: int

  atk: int

  def: int

  move1:

name: string

desc: string

atk: int

type: string

creature2:

  ...",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bj2l0/p_rngmon_pokemon_showdown_clone_but_with_randomly/,3,0,0.14,<praw.models.comment_forest.CommentForest object at 0x0000026E32D70BD0>
19al3e9,zy415,2024-01-19 15:03:47+00:00,[D] AISTATS 2024 Paper Acceptance Result,AISTATS 2024 paper acceptance results are supposed to be released today. Creating a discussion thread for this year's results.,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19al3e9/d_aistats_2024_paper_acceptance_result/,46,38,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DB0B10>
19aml6l,APaperADay,2024-01-19 16:09:56+00:00,[R] Sources of Uncertainty in Machine Learning -- A Statisticians' View,"**Paper**: [https://arxiv.org/abs/2305.16703](https://arxiv.org/abs/2305.16703)

**Abstract**:

>Machine Learning and Deep Learning have achieved an impressive standard  today, enabling us to answer questions that were inconceivable a few  years ago. Besides these successes, it becomes clear, that beyond pure  prediction, which is the primary strength of most supervised machine  learning algorithms, the quantification of uncertainty is relevant and  necessary as well. While first concepts and ideas in this direction have  emerged in recent years, this paper adopts a conceptual perspective and  examines possible sources of uncertainty. By adopting the viewpoint of a  statistician, we discuss the concepts of aleatoric and epistemic  uncertainty, which are more commonly associated with machine learning.  The paper aims to formalize the two types of uncertainty and  demonstrates that sources of uncertainty are miscellaneous and can not  always be decomposed into aleatoric and epistemic. Drawing parallels  between statistical concepts and uncertainty in machine learning, we  also demonstrate the role of data and their influence on uncertainty.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aml6l/r_sources_of_uncertainty_in_machine_learning_a/,6,29,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E32DA6490>
19b4yjd,ch4m4njheenga,2024-01-20 06:03:15+00:00,[P] Any unsupervised classification algorithms for segments/arrays?,"I need to catalog N time series segments of fixed length M and wondering if there are any pointers to find anomalies in the data. Most segments would be random noise signals, with some segments of interest that deviate away from the random noise.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b4yjd/p_any_unsupervised_classification_algorithms_for/,1,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DA8690>
19bm72m,Easy-Huckleberry7091,2024-01-20 21:18:57+00:00,[D] Best degrees for ML?, Title. It's better to get a DS degree or better a CS or Stats? What other majors are good for ML? ,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19bm72m/d_best_degrees_for_ml/,37,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E32D92790>
19ak2w9,Username912773,2024-01-19 14:16:12+00:00,"[D] Creatures 1996, an early artificial life simulation game utilizing Machine Learning. Thoughts?","I guess to preface, I was scrolling through Reddit when I came across this description of the game:

“This game has some seriously complicated systems in it for the time. It has a chemistry system, immune systems for your creatures, behavior and personalities for them, DNA and breeding systems for them, you have to teach them actual language and words through object-word and behavior association, you have to punish and reward their behaviors correctly or they will develop maladaptive behaviors or become violent and kill your other creatures, they can become depressed too if you don't manage that, and much more. In fact, there's even an entire system of emotions in the game that they can experience and you have to try to manage that or your creatures become isolated and unresponsive to you. On top of this, there are violent and diseased races of enemy creatures called grendels that roam the world and can kill/harass your creatures.”

Per the Wikipedia page:

“Creatures is an artificial life simulation where the user hatches small furry animals and teaches them how to behave, or leaves them to learn on their own. These ""Norns"" can talk, feed themselves, and protect themselves against vicious creatures called Grendels. It was the first popular application of machine learning in an interactive simulation. Neural networks are used by the creatures to learn what to do. The game is regarded as a breakthrough in artificial life research, which aims to model the behavior of creatures interacting with their environment.”

https://en.m.wikipedia.org/wiki/Creatures_(1996_video_game)

Is there any other more advanced artificial life simulation game? These seem genuinely incredibly interesting especially with several decades of advancement in machine learning between us.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ak2w9/d_creatures_1996_an_early_artificial_life/,8,29,0.82,<praw.models.comment_forest.CommentForest object at 0x0000026E32DB2F10>
19allss,TheRealBracketMaster,2024-01-19 15:27:10+00:00,[D] [P] Stockpile of GPU Servers,"  
I have a stockpile of 22 8 GPUs servers with AMD Mi50s(see notes about Mi50s below). I've been able to get PyTorch working on these GPUs and have been able to do inference for different large language models. I originally wanted to use these GPUs to serve up LLMs, but VLLM cuda kernels don't work out of the box with the Mi50s, and Llama CPP has a bug where it only supports up to 4 AMD GPUs at once.

So TLDR, I don't want these servers sitting around and if anybody has any creative useful ideas for the servers, I'm happy to grant them SSH access to piddle around.

Mi50 Specs:

 \- 16GB VRAM

 \- 1TB/s VRAM BW

 \- 25 TFLOPs",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19allss/d_p_stockpile_of_gpu_servers/,13,18,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32DB3C50>
19akxwp,topcodemangler,2024-01-19 14:57:09+00:00,[R] Self-Rewarding Language Models,,MachineLearning,https://arxiv.org/pdf/2401.10020.pdf,0,19,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32DCEDD0>
19aet3w,anaccountforthemasse,2024-01-19 08:57:02+00:00,"[D] What is the current SOTA for bootstrapping models to work on niche tasks, in vision?","It used to be that when you needed to train a model on some relatively niche classification/detection/segmentation task, you took a Resnet50 which was pretrained on ImageNet1K/COCO and finetuned it to whatever small-to-medium dataset you had, and that would be enough to jump-start your performance to something reasonable. Of course, you could always improve upon that by using a larger Resnet, improving your hyperparameter choices, or cleaning noise from your proprietary dataset.

Well, it's been years since this practice began; newer architectures have been released, newer optimizers, we have big VL models like CLIP now, etc.. and I wonder if there's a new consensus I had missed.

If you choose to answer, I would greatly appreciate if you also elaborate in the context of the following criteria:

1. Is your method of choice overly sensitive to hyperparameters? / how hard is it to converge on a proper model? For example, from my experience (which of course is not absolute), ResNets are much more forgiving than, say, EfficientNets, when it comes to hyperparameter choices.
2. How is your method sensitive to small amounts of data? For example, I recall that the original transformer was pretty bad in the small training-set scenario, and results were reported on IN22K.
3. How fast and/or memory-efficient is your choice? Small niche tasks don't tend to justify models with 1B parameters.

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/,16,41,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE37D0>
19b702w,_OBAMA_IS_REAL,2024-01-20 08:15:51+00:00,Loss function for discrete/categorical image data in torch [P],"Hello everyone,

I am trying to fine tune a pretrained vae on rgb segmentation maps, where each colour represents a class. So the key thing here is that the reconstructed colors must be **exactly** the same as the input colours and a find just tuning on MSE is not really working as on occasion I will get colours that are near but dont quiet correspond to a class. Evidently, I need use some form of cross entropy loss. Do I really need do the whole semantic segmentation ordeal of giving the number of class labels, one hot encoding and so. Surely there in an easy way in pytorch just to do a crossentroopy loss between tensor values eg. crossentropy(x,xhat). **Basically, please help me define my loss function in pytorch, I need sharp images for reconstructing segmentation maps.**",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b702w/loss_function_for_discretecategorical_image_data/,1,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E32B91CD0>
19amch3,APaperADay,2024-01-19 15:59:59+00:00,[R] Brain-inspired learning in artificial neural networks: a review,"**Paper**: [https://arxiv.org/abs/2305.11252](https://arxiv.org/abs/2305.11252)

**Abstract**:

>Artificial neural networks (ANNs) have emerged as an essential tool in  machine learning, achieving remarkable success across diverse domains,  including image and speech generation, game playing, and robotics.  However, there exist fundamental differences between ANNs' operating  mechanisms and those of the biological brain, particularly concerning  learning processes. This paper presents a comprehensive review of  current brain-inspired learning representations in artificial neural  networks. We investigate the integration of more biologically plausible  mechanisms, such as synaptic plasticity, to enhance these networks'  capabilities. Moreover, we delve into the potential advantages and  challenges accompanying this approach. Ultimately, we pinpoint promising  avenues for future research in this rapidly advancing field, which  could bring us closer to understanding the essence of intelligence.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19amch3/r_braininspired_learning_in_artificial_neural/,1,10,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE0B90>
19adrgv,Comfortable_Use_5033,2024-01-19 07:43:22+00:00,"[D] Facebook shuts down ParlAI, a framework for dialogue research","I just have learned that Facebook has archived ParlAI, the team behind BlenderBot. The repository was archived on Nov 3, 2023 and is now read-only, the project's Twitter account didn't have any update since then.

So Facebook abandoned idea behind engineered and modular dialogue system and go all in for LLM, I also heard that other modular dialogue team from other big companies are also being laid off. What do you think?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19adrgv/d_facebook_shuts_down_parlai_a_framework_for/,5,35,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E32DD1E90>
19b7gi9,sushilkhadakaanon,2024-01-20 08:46:55+00:00,[D] Adding noise to a p(x1) to obtain p(x2). Does the conditional p(x2|x1) denote the noise distribution?,We have a distribution p(x1) and we add another distribution as a noise (say a uniform dist)  to p(x1) to obtain a new distribution p(x2). My question is what does the conditional distribution p(x2|x1) represent? Is p(x2|x1) equivalent to the noise we added earlier? Any mathematics to understand this?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b7gi9/d_adding_noise_to_a_px1_to_obtain_px2_does_the/,1,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E32B94D10>
19amot7,Elven77AI,2024-01-19 16:13:56+00:00,[2401.10187] Fast Kronecker Matrix-Matrix Multiplication on GPUs,,MachineLearning,https://arxiv.org/abs/2401.10187,0,6,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E32DD5AD0>
19aquwt,MLJungle,2024-01-19 19:05:10+00:00,[R] [D] Self Consistency for COT majority vote calculation,"""Self Consistency Improves of Chain of Thought Reasoning in Language Models"" (Wang et al. 2022) calculates a majority vote to determine the most consistent answer from a set of answer. They state that after sampling multiple (r\_i ,a\_i ), where r is the reasoning path and a is the answer, they apply a marginalization over r\_i by taking a majority vote $argmax\_a \\sum 1{a\_i = a}$.

I don't understand how the probability distribution for the indicator variable $a\_i = a$ is calculated? Intuitively there should be some way to measure how similar $a\_i$ is to $a$.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aquwt/r_d_self_consistency_for_cot_majority_vote/,6,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE5750>
19b71ck,NetMoist1826,2024-01-20 08:18:21+00:00,[P] How do I improve the current Facenet model ? Current accuracy is 20%,"import numpy as np
from sklearn.datasets import fetch_lfw_people
from keras_facenet import FaceNet

data = fetch_lfw_people(min_faces_per_person=20, color=False, resize=0.5)
images = data.images
labels = data.target

images_normalized = images / 255.0

embedder = FaceNet()

images_preprocessed = np.stack([np.stack([image]*3, axis=-1) for image in images_normalized])

embeddings = []
for image in images_preprocessed:
    image = image.reshape(1, image.shape[0], image.shape[1], 3)  # Add batch dimension
    embedding = embedder.embeddings(image)[0]
    embeddings.append(embedding)

embeddings_array = np.array(embeddings)

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

X_train, X_test, y_train, y_test = train_test_split(embeddings_array, labels, test_size=0.2, random_state=42)

svm_classifier = SVC(kernel='linear', probability=True)
svm_classifier.fit(X_train, y_train)

y_pred = svm_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy}"")
print(classification_report(y_test, y_pred))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

nn_classifier = Sequential([
     Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
     Dropout(0.5),
     Dense(32, activation='relu'),
     Dropout(0.5),
     Dense(len(np.unique(data_y)), activation='softmax')
])

nn_classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
nn_classifier.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)
nn_accuracy = nn_classifier.evaluate(X_test, y_test)[1]
print(f""Neural Network Accuracy: {nn_accuracy}"")",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19b71ck/p_how_do_i_improve_the_current_facenet_model/,1,0,0.27,<praw.models.comment_forest.CommentForest object at 0x0000026E32B96CD0>
19ajjll,lusinn,2024-01-19 13:50:58+00:00,[D] AWS courses,"Hi everyone, I'm an ML engineer trying to change my job but it seems like everywhere they are requiring cloud experience. Unfortunately I didn't work with clouds but I want to learn it, specifically AWS. Which AWS courses do you recommend?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ajjll/d_aws_courses/,4,6,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E32832710>
19aw2tt,Ok_Leading_1361,2024-01-19 22:43:30+00:00,[D] LDM model architecture,"In a LDM model, the image is first passed through an encoder which sends it to latent space where the diffusion process occurs. And then the denoised latent space representation is passed through the decoder in order to bring it back to the pixel space. Is the denoised latent space representation multi dimensional? Or is it a 1-D vector?

TLDR: What's the input shape of the decoder? Or what's the output shape of the encoder?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aw2tt/d_ldm_model_architecture/,2,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE5B50>
19aw0zo,yippppeeee,2024-01-19 22:41:22+00:00,[D] Handling long sequences,"I am coming to the end of my Graduate studies and contemplating ideas for my capstone. One text classification idea would require training on sequences that exceed the typical 512 max input length. Initial research has revealed models/concepts like longT5, longformer, mistral, and sliding window but I also understand that this stuff evolves rapidly. What are the current best practices for handling long sequences, and what are your ""go-to"" pretrained models designed for lengthy inputs but that retain high performance/accuracy?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aw0zo/d_handling_long_sequences/,3,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE5850>
19a1mup,perone,2024-01-18 21:37:58+00:00,[P] PyTorch 2 Internals,"Hi, just sharing a [slide deck about PyTorch internals](https://blog.christianperone.com/2023/12/pytorch-2-internals-talk/) covering recent projects such as Dynamo, Inductor, ExecuTorch, etc, as I think there might be some folks here interested.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a1mup/p_pytorch_2_internals/,10,88,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DF4A50>
19a6nsd,Enzo-chan,2024-01-19 01:16:39+00:00,"[D] Is Strong A.I. actually a serious and real field being research, or just another hype people are promoting?","Is Strong A.I. (General A.I.) actually a serious field of research, or is it just pure hype that came from people who just read/watched to sci-fi books?

Is Strong AI/a.k.a. AGI actually being taken serious by some researchers/institutions who think that It can eventually being done, or is It another one of these fancy tech vaporwares who people are hyping till can't no more, but actually, those who are working in the field know that such an Idea can't actually work due to hard physical constraints, or If ever happens it's gonna take centuries to come into fruition?

Because there have been a Lot of hysteria in the past for many futuristic technologies, which were hyped by lots of people who didn't knew squat about It, however It could not work in practice(i.e. Em Drive, Graphene, Fulerenes, Nanobots, Bussard Ramjet, Fusion Energy, etc.).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/,275,45,0.66,<praw.models.comment_forest.CommentForest object at 0x0000026E32E8AED0>
19alo6a,Vivid-Art6939,2024-01-19 15:30:12+00:00,[Discussion] Network that combines video upscaling with video retiming/interpolation?,"If there was a network trained to perform video upscaling/denoising as well as create intermediate frames for frame interpolation, it seems obvious that training for one task would also increase accuracy on the other. Has this been done before, is there a paper I can read that shows this result?

All the papers Ive seen so far seem to treat these 2 problems separately, such as the DAIN (Depth-Aware Video Frame Interpolation) network.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19alo6a/discussion_network_that_combines_video_upscaling/,0,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE5590>
19ancq9,pegasi320,2024-01-19 16:41:13+00:00,[P] Cold start recommendations - XGBoost or something else?,"I have a dataset of approximately 100k different products. These products can either be whole units or accessories. Like complete computers vs buying cases, mouse, keyboard, ram, cpu etc.

I want to build a recommendation system that finds similar products given the input of 1 product.

The data is tabular (price, length/width/height, category, subtype, etc. with some text portions like title and description that can be variable… there are some columns 100% in common across everything but different categories have different specifications/columns)

Eventually this will go on a website - but assume 0 user traffic right now. Which I think rules out collaborative filtering since there’s no feedback loop. Although long term that’s probably ideal.

Since it’s tabular data, can I use XGBoost? Do I BM25 any free form text fields and covert categories/types to numbers? Or is embeddings + kNN better? Any YouTube videos or documentation would help.

I’m also considering having multiple separate recommendation match providers based on category since their columns differ. Similar to how StockX has recommendations based on shoes, or clothes etc.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ancq9/p_cold_start_recommendations_xgboost_or_something/,7,2,0.63,<praw.models.comment_forest.CommentForest object at 0x0000026E32BA1210>
19asntt,Advanced_Cancel_1566,2024-01-19 20:19:42+00:00,Counting down for the AISTATS 2024 decision! [D],"Hey, I know the decision is supposed to be out today, possibly by the end of AOE time or even the next day. Let's keep an eye out and hope to discuss it here...  
If accepted, congratulations! If not, don't be disheartened—cheer up! :) ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19asntt/counting_down_for_the_aistats_2024_decision_d/,18,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32E1B290>
19aj9is,BarkingBot,2024-01-19 13:37:21+00:00,[D] GPT 2 paper question (Language Models are Unsupervised Multitask Learners),"in 2.2. Input Representation section, it uses byte-level version of BPE, how does it handle the other language that could be handled in Unicode version?(you know there is many more characters than 256 in Unicode, so I was wondering)

\+

'Since our approach can assign a probability to any Unicode string'(from the same section), and how is it possible when it could only represent 256 characters from the entire Unicode?

&#x200B;

please tell me if I misunderstood anything. thank you",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aj9is/d_gpt_2_paper_question_language_models_are/,4,3,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E32E09090>
19a03ax,ZachVorhies,2024-01-18 20:35:44+00:00,[R] How do you train your LLM's?,"Hi there, I'm a senior python dev getting into LLM training. My boss is using a system that requires question and answer pairs to be fed into it.  


Is this how all training is done? Transforming all our text data into Q&A pairs is a major underpinning. I was hoping we could just feed it mountains of text and then pre-train it on this. But the current solution we are using doesn't work like this.  


How do you train your LLM's and what should I look at?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a03ax/r_how_do_you_train_your_llms/,36,54,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E32F0C410>
19ahe92,OutplayOutlast,2024-01-19 11:53:06+00:00,[P] NLP Symptom Checker for Dutch Speakers: From Data to Deployment,"I would like to share a project I have worked on recently, using XGBoost Classifiers, Flask API, Docker, Google Cloud Container Registry and Google Cloud run. Feel free to comment if it interests you:

[https://christiangrech.medium.com/building-a-robust-dutch-nlp-symptom-checker-from-data-to-deployment-e389d874a247](https://christiangrech.medium.com/building-a-robust-dutch-nlp-symptom-checker-from-data-to-deployment-e389d874a247)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19ahe92/p_nlp_symptom_checker_for_dutch_speakers_from/,0,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32EAB150>
19aud03,ahsaor8,2024-01-19 21:31:23+00:00,[D] does anyone tried to fine-tune llm in translation task,I would like to know if I fine-tune llm in translation task gonna works well,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aud03/d_does_anyone_tried_to_finetune_llm_in/,0,0,0.2,<praw.models.comment_forest.CommentForest object at 0x0000026E32EE7B10>
19alf6m,ade17_in,2024-01-19 15:18:51+00:00,[R] Seeking insights on my possible thesis topics!,"Short intro - Hello! I'm a masters student with major in AI and have secured few thesis topics. I have a modest research experience and want to pursue a PhD after my masters (more inclined towards an industrial phd position).

I have secured few positions where my thesis topics are listed below (I received 8 offers but I'm considering these 4 keeping my interests in mind). As thesis plays a cruicial role for phd applications as well as job application, I want a topic which is well relevant and has a potential prospect. (Ofc can't share exact title and details.) 

1. Video segmentation on a fairly new dataset, no papers on the methodology yet (medical) (in industry)

2. Frame interpolation for videos (medical - surgery) (in industry)

3. Making synthetic dataset using diffusion models (medical) (in university under infamous Prof.) 

4. 3D to 2D mapping using transformers (autonomous driving) (industry)

Any insights on these topics might be useful. Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19alf6m/r_seeking_insights_on_my_possible_thesis_topics/,0,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E32819010>
19abdeo,Chris8080,2024-01-19 05:14:45+00:00,[D] How to extract event information from unstructured text?,"Hello,

I've got something similar to press releases and I'd need to extract event information.I'm looking for events from one specific industry. But the press releases can contain none, one or mutliple event details and they don't neecessarily relate to my industry.

As a human, I'd go through the PRcheck each event infoand based on the title (sometimes the description) decide whether it's for my industryand then look for the details (date / time / location / event name / description / etc).

What would be a good approach to do this offline / locally?I just tried around with llama.cpp and that just gives me a mess (probably I've done it wrong).A few years ago, I've used Spacy for NER - which is basically just a small part of step 4 I guess.Is there something that ""understands"" my data better and gives me great results?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19abdeo/d_how_to_extract_event_information_from/,4,6,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E2F2FBA50>
19a8klj,Melodic_Stomach_2704,2024-01-19 02:48:35+00:00,[D] Transformer multi-head attention implementation,"I've been following [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) to implement the transformer architecture. In the multi-head attention class's `forward()` method, Query, Key & Value are being multiplied with corresponding projection matrix; `W_q, W_k, W_v`.

            query, key, value = [
                lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
                for lin, x in zip(self.linears, (query, key, value))
            ]

Here, `lin(x)` is being reshaped into `(nbatches, -1, self.h, self.d_k)`and dimension 1 & 2 is being transposed which makes the dimension `(nbatches, self.h, -1, self.d_k).`

I'm failing to understand why don't they directly do `lin(x).view(nbatches, self.h, -1, self.d_k)`?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a8klj/d_transformer_multihead_attention_implementation/,1,8,0.79,<praw.models.comment_forest.CommentForest object at 0x0000026E32EFBC10>
19av6g3,LyPreto,2024-01-19 22:05:36+00:00,[D] Transformers-Based AI Road Safety Copilot,"I’m not an ML engineer and have only done very basic behavior centered fine-tunes but I was wondering if something like this was feasible— training a transformer-based AI model to predict road safety, using GPS, traffic, weather, and historical crash data. Integrated with navigation systems for real-time alerts.

For instance, the system would combine historical crash data with future weather forecasts to calculate risk probabilities for high-crash areas under anticipated conditions, offering tailored warnings and advice to drivers based on specific risks along their route.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19av6g3/d_transformersbased_ai_road_safety_copilot/,0,0,0.3,<praw.models.comment_forest.CommentForest object at 0x0000026E32C0F850>
19adf04,Amazing_Life_221,2024-01-19 07:19:29+00:00,[R] How to gain intermediate/advanced knowledge in NLP?,"I have a few years of industry experience. Although I'm not an expert in deep learning (or ML in general), I know how to build models and deploy them, etc. This field is constantly evolving, so I have to keep learning. I do this with the help of YouTube or blogs, which provide me with a basic understanding of the landscape, nothing more than that.

I implement a few projects here and there to gain some understanding of what I've just learned. However, with the knowledge I've acquired, I can only build the basics and don't understand how to scale it further or undertake complex projects.

For example (this is in CV but gives the gist), when learning about image segmentation, it took me around 2 weeks to learn and implement a model on this fancy data I had, which was fun. Now, without any ""need"" to develop further, that model is just a pet project. While I learned how to train and implement a custom model (UNet from scratch) and handle data on GPU, I now don't know where to look for more.

I am switching to NLP because that's where I think I would like to work. However, here I see that most of the game depends on APIs and not custom model building. I don’t want to build just a basic project in few days. 

Do you have any recommendations for projects/papers that I can implement on my own, providing proper basic knowledge of the NLP field? (like implementing a transformer model from scratch, but for advanced levels)

PS: I want to implement these projects in my free time (other than my job), and on my own. So, hardware requirements might restrict me. Also, I am aware that I know very little about the NLP landscape, so any subfield that is interesting and contains theoretical background would be beneficial (I want to learn theoretical side by side :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19adf04/r_how_to_gain_intermediateadvanced_knowledge_in/,2,3,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32E50F10>
19aersk,testuser514,2024-01-19 08:54:19+00:00,[D] - Anyone using NER masking techniques in NLI architectures ?,"Anyone building NLI architectures with NER ideas?

So I’ve been going through a bunch of literature on NER and NLI and I realized that a lot of the underlying operations are basically around manipulating the masking strategies on the tokens. I’m looking for folks to brainstorm new architectures in this regard. 

I’m thinking of something like this:

1. BERT layer to generate token.

2. NER masking layer to generate NER relationships between the tokens (kind of acts as rationale extraction and additional signals)

3. NLI logit estimation that can take advantage of all this.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aersk/d_anyone_using_ner_masking_techniques_in_nli/,1,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32ED9450>
19agmi6,portmanteau98,2024-01-19 11:04:14+00:00,[D] Mistral 7B FineTuning with Interview Data," Hi!

I’ve been wanting to fine tune this model based on transcribed zoom interviews I have as training data.

How do I approach this problem?  


How do I format the dataset? What's a good methology? What GPU will I need? Lastly, how do I upload it to huggingspace as a chat UI?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19agmi6/d_mistral_7b_finetuning_with_interview_data/,2,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32EE4A90>
19aghlb,thwack324,2024-01-19 10:55:41+00:00,[D]ML operator unit test framework,"Hi awesome folks,

I'm developing an ML operators library for GPU (like cudnn and cublas) which can be pluged into AI framework such as pytorch and tensorflow.

It has a few hundred of operators, each with tens or hundreds of unit test cases.

Right now my problem is about test efficiency.

Let me be specific.

Most of our test case looks like this:

    void TestMatMul() {
      inputA = PrepareRandomInput();
      inputB = PrepareRandomInput();
    
      // this may take 1 minute
      cpu_result = RunMatMul(inputA, inputB, DEVICE_CPU);
    
      // this may take 1 second
      gpu_result = RunMatMul(inputA, inputB, DEVICE_GPU);
    
      AssertAllClose(cpu_result, gpu_result)
    }

&#x200B;

When inputA and inputB are big matrices, it could take minutes for CPU to calculate the Matmul operation while maybe just a few milliseconds on GPU.

Matmul is just one example, this could happen to any operator.

At the end of unit tests the overall GPU utilization is really low.

For example, for a hour test time, the overall GPU hardware time is less than 1 minute.

The CPU is struggling to do the calculation.

I cannot solve this issue by parallell the test cases with multithreadings since the CPU is the major bottleneck in this scenario.

&#x200B;

This problem seems common to me because most of the unit test framework are just implemented this way.

I tried my luck on google but no much finding.

My thought is to split the test into two stages and put the cpu calculation to a CPU farm.

I'm wondering if there is any public project or research paper has addressed this issue.

&#x200B;

Thanks a lot

Kevin

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aghlb/dml_operator_unit_test_framework/,1,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32E52210>
199q0bc,Smith4242,2024-01-18 13:21:19+00:00,[R] EarthPT: a time series transformer foundation model,"Wanted to share the code release of EarthPT, a model that predicts future satellite observations in a zero shot setting! I'm the first author so please shoot any questions you have at me.

EarthPT is a 700 million parameter decoding transformer foundation model trained in an autoregressive self-supervised manner and developed specifically with EO use-cases in mind. EarthPT can accurately predict future satellite observations across the 400-2300 nm range well into the future (we found six months!).

The embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification.

The coolest takeaway for me is that EO data provides us with -- in theory -- quadrillions of training tokens. Therefore, if we assume that EarthPT follows neural scaling laws akin to those derived for Large Language Models (LLMs), there is currently no data-imposed limit to scaling EarthPT and other similar ‘Large Observation Models.’(!)

Code: https://github.com/aspiaspace/EarthPT

Paper: https://arxiv.org/abs/2309.07207",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/,16,44,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32F21050>
19afr4n,reverendCappuccino,2024-01-19 10:03:44+00:00,[D] What do we call parts of Neural Networks?,"I opened a similar discussion but I had the wrong attitude although I wanted to discuss.
This time I just need confirmations and informations.

I was calling ""head"" the first input layers of ConvNets such as ResNets, ConvNext and ConvMixers, referring to the parts that often do downsampling with large kernels, patch embeddings, and more importantly that precede the repetitive blocks in the backbone that qualify those types of models.
But the ""head"" is also the classification head after(?) the backbone, considering information flow at inference.
What is the best way to call them?

Another thing: I always refer with ""residual"" to the nonlinear function, and with ""skip connection"" to the identity mapping, when dealing with ResNets. I've never seen people use both inverting them, but I often see ""residual"" referring to the identity mapping.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19afr4n/d_what_do_we_call_parts_of_neural_networks/,2,1,0.55,<praw.models.comment_forest.CommentForest object at 0x0000026E32F1A390>
199l2m9,Expensive_Charity293,2024-01-18 08:03:13+00:00,[D] Does this paper's partitioning cause data leakage?,"I recently got into a rather heated discussion about [this study](https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.3023). The TLDR is that they used textual embeddings and gradient boosting to predict CEO personality scores from earnings call transcripts. They analyzed ~200 CEOs, segmenting each CEO's calls into multiple parts to increase data points. However, each CEO appears in both the training and validation sets with different segments of their calls. Imo, this should cause data leakage because the model may pick up on idiosyncraticities of the individual CEOs' language usage, rather than the patterns of the underlying Data Generating Process. What's your take on this?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/,46,117,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E32F50390>
19a4u80,Fun-Medium8799,2024-01-18 23:53:00+00:00,[D] Speaker Diarization with video recognition of lips moving,"Hello! I'm currently using whisperx for speaker recognition and it's pretty good. Still, I remember reading there is another speaker diarization framework that uses image recognition to identify when the lips of the speaker are moving to give a more precise identification. Does anyone know what framework this is? I've been searching all week but can't find it. Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a4u80/d_speaker_diarization_with_video_recognition_of/,0,6,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F0F310>
19afcch,Periplokos,2024-01-19 09:34:55+00:00,[D] When will the tentative agreement on the AI act in Europe become formally adopted?," A month or so ago European Union agreed upon a set of tentative rules regarding the regulation of AI, the so-called AI-act.

While certain countries like Germany and France initially resisted some of the more dubious measures of the AI-act such as the regulation of the foundation models, they eventually agreed on such regulation.

>Does anyone know when the tentative agreement on the AI act in Europe become formally adopted?

 ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19afcch/d_when_will_the_tentative_agreement_on_the_ai_act/,0,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E32F3E010>
199yknn,APaperADay,2024-01-18 19:34:05+00:00,[R] Context-Aware Meta-Learning,"**arXiv**: [https://arxiv.org/abs/2310.10971](https://arxiv.org/abs/2310.10971)

**OpenReview**:

[https://openreview.net/forum?id=lJYAkDVnRU](https://openreview.net/forum?id=lJYAkDVnRU)

[https://openreview.net/forum?id=SAu298HU2I](https://openreview.net/forum?id=SAu298HU2I)

**Abstract**:

>Large Language Models like ChatGPT demonstrate a remarkable capacity to  learn new concepts during inference without any fine-tuning. However,  visual models trained to detect new objects during inference have been  unable to replicate this ability, and instead either perform poorly or  require meta-training and/or fine-tuning on similar objects. In this  work, we propose a meta-learning algorithm that emulates Large Language  Models by learning new visual concepts during inference without  fine-tuning. Our approach leverages a frozen pre-trained feature  extractor, and analogous to in-context learning, recasts meta-learning  as sequence modeling over datapoints with known labels and a test  datapoint with an unknown label. On 8 out of 11 meta-learning  benchmarks, our approach -- without meta-training or fine-tuning --  exceeds or matches the state-of-the-art algorithm, P>M>F, which is  meta-trained on these benchmarks.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199yknn/r_contextaware_metalearning/,1,14,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E32E02890>
199y7y9,APaperADay,2024-01-18 19:19:32+00:00,[R] Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation,"**Paper**: [https://arxiv.org/abs/2310.15961](https://arxiv.org/abs/2310.15961)

**Code**: [https://github.com/llm-random/llm-random](https://github.com/llm-random/llm-random)

**Blog post**: [https://llm-random.github.io/posts/mixture\_of\_tokens/](https://llm-random.github.io/posts/mixture_of_tokens/)

**Abstract**:

>Despite the promise of Mixture of Experts (MoE) models in increasing  parameter counts of Transformer models while maintaining training and  inference costs, their application carries notable drawbacks. The key  strategy of these models is to, for each processed token, activate at  most a few experts - subsets of an extensive feed-forward layer. But  this approach is not without its challenges. The operation of matching  experts and tokens is discrete, which makes MoE models prone to issues  like training instability and uneven expert utilization. Existing  techniques designed to address these concerns, such as auxiliary losses  or balance-aware matching, result either in lower model performance or  are more difficult to train. In response to these issues, we propose  **Mixture of Tokens**, a fully-differentiable model that retains the  benefits of MoE architectures while avoiding the aforementioned  difficulties. Rather than routing tokens to experts, this approach mixes  tokens from different examples prior to feeding them to experts,  enabling the model to learn from all token-expert combinations.  Importantly, this mixing can be disabled to avoid mixing of different  sequences during inference. Crucially, this method is fully compatible  with both masked and causal Large Language Model training and inference.

**Previous discussion**: [https://www.reddit.com/r/mlscaling/comments/17ha25s/mixture\_of\_tokens\_efficient\_llms\_through/](https://www.reddit.com/r/mlscaling/comments/17ha25s/mixture_of_tokens_efficient_llms_through/)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199y7y9/r_mixture_of_tokens_efficient_llms_through/,1,12,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F30850>
199zyfh,Direct-Touch469,2024-01-18 20:30:16+00:00,How costly is it to obtain labeled data? [D],"

Doing my masters thesis in Active Learning. A key point in the literature is active learning may be useful in situations where there’s lots of unlabeled data, and the cost associated with labeling is high, so active learning can effectively same time and effort in labeling, if the model can “choose” a subset of samples which are the most “informative” and then these can be labeled.

However, I kinda realized, as much as this active learning stuff is interesting and I’m probably continuing, I just don’t quite get when it would be a realistic scenario in a company for labeled data not being available/being highly costly. Of course, I know when I read it there are specific instances where this occurs:

NLP - tasks like speech recognition may require audio to be labeled, or in information extraction requires annotations and certain things within a corpus to be annotated


However, the literature I’m reading is a survey from like 2009, I’d imagine since then problems like these just don’t exist really. So I’m wondering how often there’s just a pool of unlabeled data waiting to be labeled. Is there even a demand for active learning these days?

I think one area I’m “pivoting” to is to maybe looking at active learning in online “streaming” data where I’d imagine stuff isn’t labeled as quickly.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/,15,7,0.74,<praw.models.comment_forest.CommentForest object at 0x0000026E32F2A3D0>
199n479,great_waldini,2024-01-18 10:28:18+00:00,"[D] How did OpenAI increase context length of the GPT-4 iterations? Did they retrain GPT-4-1106 from scratch? Or was it a hackier mix of techniques like sparse attention, chunking, etc?","As the title states, got to thinking about the GPT-4 derivative models and how they were made. I know things are moving fast, and OpenAI is anything but ""open"", but what's the speculation on how it was done?

I'm not up on all the latest details of LLM progress, but from my understanding of the attention mechanism, typically you'd have to retrain a transformer from scratch to increase context size.

But if that's the case, wouldn't they have to redo all the RLHF too? Or are there efficient transfer learning techniques for the RLHF step?

I'd love to see some papers comparing evals of the GPT-4 iterations to one another, if ya'll know of any you can link. Even assuming the RLHF were perfectly transferable, wouldn't we still expect there to be measurable differences between the models in the GPT-4 family?

I wonder if there's any insightful performance quirks between the models, e.g. for coding tasks perhaps the 32k 0613 model performs better than the 8k base model, but the 128k 1106 is worse than 0613 due to depreciating returns of context size given the same number of parameters, same training data, etc.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199n479/d_how_did_openai_increase_context_length_of_the/,6,48,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E32F148D0>
199v9kr,eusben,2024-01-18 17:17:59+00:00,[P] WhisperSpeech - An Open Source text-to-speech system,"An Open Source text-to-speech system built by inverting Whisper. 

[https://github.com/collabora/WhisperSpeech](https://github.com/collabora/WhisperSpeech)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199v9kr/p_whisperspeech_an_open_source_texttospeech_system/,0,9,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E32F35550>
19aac2d,sushilkhadakaanon,2024-01-19 04:18:02+00:00,[D] what does q(x_t-1|x_t) indicates intuitively in diffusion process?,"I understand the forward and riverse diffusion process and the meaning of intermediate marvok's transition.
q(x_t|x_t-1) is a Gaussian , and it's a distribution when x_t-1 is know, a transition upon adding of noise.
P(x_t-1|x_t) is the transition from x_t to x_t-1 indicating reverse process. This is the one we seek to optimize (2015 paper) and is represented as a NN. 
But while formulating a loss function, a new distribution comes into picture, q(x_t-1|x_t) , and we can compute this with bayes rule. But I wonder what q(x_t-1|x_t) indicates? Any intuitive definition?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19aac2d/d_what_does_qx_t1x_t_indicates_intuitively_in/,11,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32E96490>
19a9txq,holdenonthehill,2024-01-19 03:51:28+00:00,[D] Tool that combines voice cloning + audio track editing?," **Question:** is there a software/tool that combines audio track editing with built-in voice cloning? If not, how could I make one?

**Problem:** I do a lot of work with audiobooks and podcasts for clients. If the narrator of an audiobook messes up a word or sentence, it requires having them come back into the studio and re-record it. Recently, I've been using voice AI cloning to generate the line I need in the author's voice. However, I still have to then export the audio file, open the original track in an audio editing software like Audition, and then insert the new line where I need. Then repeat the process for any other instances. It's time consuming. I'm looking for a software that allows you to edit the audio track with built-in access to the voice cloning tool like Photoshop's generative fill.

**Idea:** If there's nothing else out there, I'd like to create a software/website portal that allows audio editors to use voice cloning the same as generative fill in Photoshop. Essentially, they can cut and edit their audio files like normal, but they can also select a portion of the audio track, type a word or sentence, and then generate a voice-cloned replacement that fits directly at the location selected in the original speaker's voice. This would streamline audio editing and voice cloning integration.

Thanks for any ideas and/or direction on how to develop this!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a9txq/d_tool_that_combines_voice_cloning_audio_track/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32F52A50>
19af18f,create4drawing,2024-01-19 09:12:57+00:00,Looking for eli5 [R],"I work with a machine learning model, and I always have problems explaining what is going on with the model, some key points:

How important are training sets (and accuracy)
What happens when retraining
What Is machine learning 

I am not a model developer, but I keep looking for sources for eli5 (explain it like I'm 5) type materials for explaining these and other things, does anyone have some interesting materials I can look at, maybe someone has already done some work that I can coat tail",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19af18f/looking_for_eli5_r/,1,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E32DD08D0>
199y7g2,Fluid-Physics-5663,2024-01-18 19:18:56+00:00,[D] Creating Shadows on Foreground Objects,"I'm experimenting with computer vision and learning the basics. Right now, I'm trying to add shadows to a foreground object in a .png file and then put it on a light background. I looked for research papers about adding shadows to objects but couldn't find any, except this [one](https://github.com/bcmi/libcom/blob/main/docs/shadow_generation.md). There don't seem to be any Python libraries for this either. I'm wondering why. Is it too difficult, or is it something that doesn't need machine learning?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199y7g2/d_creating_shadows_on_foreground_objects/,2,4,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F2B3D0>
199xzlb,Outlandish_MurMan,2024-01-18 19:09:49+00:00,[D] Blog on Systematic approach to debugging Machine Learning Projects,"Hi,

I wrote an article on Systematic approach to debugging ML projects. Please let me know your thoughts. Anything to improve or anymore debugging tricks are much appreciated..

[https://medium.com/@gitlostmurali/debugging-your-machine-learning-project-8d1897676050?sk=5d30bfe483b97eb0dc4275565234ccad](https://medium.com/@gitlostmurali/debugging-your-machine-learning-project-8d1897676050?sk=5d30bfe483b97eb0dc4275565234ccad)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199xzlb/d_blog_on_systematic_approach_to_debugging/,0,4,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E32DB3CD0>
199qw5i,ashenone420,2024-01-18 14:05:30+00:00,[R] EPU-CNN: Generalized Additive CNN for Interpretable Computer Vision,"Paper: [https://www.nature.com/articles/s41598-023-38459-1](https://www.nature.com/articles/s41598-023-38459-1)

Code: [https://github.com/innoisys/EPU-CNN](https://github.com/innoisys/EPU-CNN)

Abstract: The adoption of convolutional neural network (CNN) models in high-stake domains is hindered by their inability to meet society’s demand for transparency in decision-making. So far, a growing number of methodologies have emerged for developing CNN models that are interpretable by design. However, such models are not capable of providing interpretations in accordance with human perception, while maintaining competent performance. In this paper, we tackle these challenges with a novel, general framework for instantiating inherently interpretable CNN models, named E pluribus unum interpretable CNN (EPU-CNN). An EPU-CNN model consists of CNN sub-networks, each of which receives a different representation of an input image expressing a perceptual feature, such as color or texture. The output of an EPU-CNN model consists of the classification prediction and its interpretation, in terms of relative contributions of perceptual features in different regions of the input image. EPU-CNN models have been extensively evaluated on various publicly available datasets, as well as a contributed benchmark dataset. Medical datasets are used to demonstrate the applicability of EPU-CNN for risk-sensitive decisions in medicine. The experimental results indicate that EPU-CNN models can achieve a comparable or better classification performance than other CNN architectures while providing humanly perceivable interpretations. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199qw5i/r_epucnn_generalized_additive_cnn_for/,0,10,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32DB2C90>
199wwhq,lightSpeedBrick,2024-01-18 18:25:38+00:00,[D] What Causes LLM Performance To Degrade When Exceeding Training Context Length?,"Hello folks

I am going through the StreamingLLMs paper [https://arxiv.org/pdf/2309.17453.pdf](https://arxiv.org/pdf/2309.17453.pdf) and came back to a question I've been wondering about for some time. Is there a good understanding what ""limits"" the context length within a transformer? Why can't it generalize beyond the sequence length that it was trained on.

One guess I had was that it was to do with original absolute positional embeddings. Once you exceed a certain positional index you can't assign a unique positional embedding to the newest token (since the sin/cos functions used are periodic) - please correct me if that hunch is incorrect.

However, newer models use relative positional embeddings such as RoPE, AliBi and YaRN. If I am not mistaken the motivation behind those works, at least partially, is to help models generalize beyond their original training context length. However, based on what the Streaming LLM paper demonstrates, this isn't really the case for RoPE or AliBi embeddings. They don't touch upon YaRN as far as I can tell.

What is the reason that this happens? How does introducing new tokens that push the input sequence length beyond that at training mess with the performance of the model? My two best wild guesses are that maybe it's a) due to the SoftMax distribution within the attention taking on values that the model isn't used to seeing as the length exceeds the training window or maybe b) as the sequences gets longer and longer more and more information is packed into the intermediate token representations within the transformer and going beyond the context length used at training adds extra information that the model that it can't handle?

As I mentioned, these are just random wild guesses, so I would love to know if there's a proper answer to this or what the current line of thinking might be!

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199wwhq/d_what_causes_llm_performance_to_degrade_when/,2,4,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32F28490>
19a1c1n,stardoge42,2024-01-18 21:25:35+00:00,Continous Learning MARL in Fighting Game Research [D]," 

My friend and I are doing research on using MARL in the context of a fighting game where the actors / agents submit inputs simeltaneously and are then resolved by the fighting game physics engine. There are numerous papers that talk about DL / RL / some MARL in the context of fighting games, but notably they do not include source code or actually talk about their methodologies so much as they do talk about generalized findings / insights.

Right now were looking at using Pytorch (running on CUDA for training speed) using Petting Zoo (extension of gymnasium for MARL) specifically using the AgileRL library for hyperparameter optimization. We are well aware that there are so many hyperparameters that knowing what to change is tricky as we try to refine the problem. We are envisioning that we have 8 or so instances of the research game engine (I have 10 core CPU) connected to 10 instances of a Petting Zoo (possibly Agile RL modified) training environment where the inputs / outputs are continuously fed back and forth between the engine and the training environment, back and forth.

I guess I'm asking for some general advice / tips and feedback on the tools we're using. If you know of specific textbooks, research papers of GitHub repos that have tackled a similar problem, that could be very helpful. We have some resources on Hyperparameter optimziation and some ideas for how to fiddle with the settings, but the initial structure of the project / starting code just to get the AI learning is a little tricky. We do have a Connect 4 training example of MARL working, provided by AgileRL. But we're seeking to adapt this from turn by turn input submission to simeltaneous input submission (which is certainly possible, MARL is used in live games such as MOBAs and others).

ANY information you can give us is a blessing and is helpful. Thanks so much for your time.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19a1c1n/continous_learning_marl_in_fighting_game_research/,2,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32F5A950>
19932kw,RobbinDeBank,2024-01-17 18:01:17+00:00,[R] AlphaGeometry: An Olympiad-level AI system for geometry,"Blog: https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/

Paper: https://www.nature.com/articles/s41586-023-06747-5

Github: https://github.com/google-deepmind/alphageometry

Abstract: 

Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19932kw/r_alphageometry_an_olympiadlevel_ai_system_for/,67,252,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E32FA5290>
199r1py,borna_ahmadzadeh,2024-01-18 14:12:58+00:00,[P] OpenCLIP JAX - CLIP models in JAX/Flax,"*Excerpt from* [GitHub](https://github.com/BobMcDear/open-clip-jax/)

# CLIP in JAX/Flax

## Introduction

`open_clip_jax` is an open source JAX/Flax implementation of OpenAI's [CLIP](https://arxiv.org/abs/2103.00020), including image and text towers, pre-trained parameters, training utilities, and more. It is inspired by but not affiliated with [OpenCLIP](https://github.com/mlfoundations/open_clip) and aims to deliver similar functionalities with a JAX backend.

## Installation

The JAX installation process may differ depending on one's machine, so JAX needs to be [installed manually](https://github.com/google/jax#installation) by the user. Afterwards, `open_clip_jax` can be installed through `pip install git+https://github.com/BobMcDear/open-clip-jax.git`.

## Usage

`CLIPInference` is a convenience class for conducting inference, which can be called on raw images and texts to compute their similarity scores, as demonstrated below.

    import jax
    from PIL import Image
    from open_clip_jax import CLIPInference
    
    
    clip = CLIPInference(
        'vit-base-patch32',
        softmax_temp=100.,
        pretrained='laion2b-s34b-b79k',
        )
    image = Image.open('CLIP.png').convert('RGB')
    text = ['A diagram', 'A dog', 'A cat']
    
    # image and text can be single data points or lists.
    probs, _ = clip(image, text)
    print(probs)

Under the hood, `CLIPInference` utilizes `create_model_with_params` to create the CLIP model, `create_image_transforms` to pre-process the image(s), and `tokenize` to tokenize the text(s). A sample usage of these functions, equivalent to the code above, is exhibited in the following snippet. Breaking `CLIPInference` into these smaller components can offer greater flexibility.

    from typing import Dict
    
    import jax
    from PIL import Image
    from jax import Array
    from open_clip_jax import create_image_transforms, create_model_with_params, tokenize
    
    
    model, vars = create_model_with_params(
        'vit-base-patch32',
        pretrained='laion2b-s34b-b79k',
        )
    image_transforms = create_image_transforms(
        train=False,
        input_format='image',
        do_batch_transforms=False,
        )
    
    image = image_transforms(Image.open('CLIP.png').convert('RGB'))._numpy()
    image = np.expand_dims(image, axis=0)
    text = tokenize(['A diagram', 'A dog', 'A cat'])._numpy()
    
    def calculate_similarity(vars: Dict, image: Array, text: Array) -> Array:
        # CLIP returns L2-normalized image and text features.
        image_proj, text_proj = model.apply(vars, image, text)
        return nn.softmax(100 * image_proj @ text_proj.T)
    
    probs = jax.jit(calculate_similarity)(vars, image, text)
    print(probs)

## Training

This repository also supports training CLIP models from scratch, using either the utilities supplied by `open_clip_jax.training` for more fine-grained control or `main.py` for a fully-featured training script. The following are sample commands for training on a single-worker machine using a CSV dataset and training on a TPU pod using a TFRecord dataset respectively.

    # Single-worker CSV training
    wget https://raw.githubusercontent.com/BobMcDear/open-clip-jax/main/main.py -q
    python3 open-clip-jax/main.py \
        --train-path train.csv \
        --valid-path valid.csv \
        --image-key image_path \
        --text-key caption \
        --global-batch-size 128 \
        --model-name vit-base-patch32 \
        --learning-rate 1e-3 \
        --n-epochs 30
    
    # TPU pod TFRecord training
    NAME=open_clip_jax
    ZONE=us-central1-a
    TYPE=v3-32
    VERSION=v2-alpha
    gcloud compute tpus tpu-vm create $NAME \
        --zone=$ZONE \
        --accelerator-type=$TYPE \
        --version=$VERSION
    gcloud compute tpus tpu-vm ssh $NAME \
        --zone $ZONE \
        --worker=all \
        --command ""
            pip install -U pip &&
            pip install -U jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html &&
            pip install git+https://github.com/BobMcDear/open-clip-jax.git &&
            wget https://raw.githubusercontent.com/BobMcDear/open-clip-jax/main/main.py -q &&
            python3 main.py \
                --train-path gs://open_clip_jax/train_tfrecords/ \
                --valid-path gs://open_clip_jax/valid_tfrecords/ \
                --image-key jpg \
                --text-key caption \
                --global-batch-size 1024 \
                --model-name vit-base-patch32 \
                --learning-rate 3e-3 \
                --n-epochs 30 \
                --checkpoint-dir gs://open_clip_jax/checkpoints/
            ""
    gcloud compute tpus tpu-vm delete $NAME \
        --zone $ZONE

## Available Models

There are three functions related to listing available models and pre-trained parameters:

* `list_models`: Returns the name of every model, but some, such as ViT-Small, do not have associated pre-trained parameters.
* `list_pretrained`: Returns tuples of (name of model, name of pre-trained parameters). A model may have several groups of pre-trained parameters, so there may be multiple entries with identical model names but different pre-trained parameters.
* `list_pretrained_by_model`: Returns a particular model's pre-trained parameters.

&#8203;

    >>> import open_clip_jax
    >>> open_clip_jax.list_models()
    ('convnext-base-w',
     'convnext-base',
     'convnext-large-d',
     'vit-base-patch16',
     'vit-base-patch32',
     'vit-huge-patch14',
     'vit-huge-patch16',
     'vit-large-patch14',
     'vit-large-patch16',
     'vit-nano-patch32',
     'vit-small-patch16',
     'vit-small-patch32')
    >>> open_clip_jax.list_pretrained()
    (('convnext-base', 'laion400m-s13b-b51k'),
     ('convnext-base-w', 'laion-aesthetic-s13b-b82k'),
     ('convnext-base-w', 'laion-aesthetic-s13b-b82k-320'),
     ('convnext-base-w', 'laion-aesthetic-s13b-b82k-augreg-320'),
     ('convnext-base-w', 'laion2b-s13b-b82k'),
     ('convnext-base-w', 'laion2b-s13b-b82k-augreg'),
     ('convnext-large-d', 'laion2b-s26b-b102k-augreg'),
     ('convnext-large-d', 'laion2b-s29b-b131k-ft-320'),
     ('convnext-large-d', 'laion2b-s29b-b131k-ft-soup-320'),
     ('vit-base-patch32', 'laion400m-e31'),
     ('vit-base-patch32', 'laion400m-e32'),
     ('vit-base-patch32', 'laion2b-e16'),
     ('vit-base-patch32', 'laion2b-s34b-b79k'),
     ('vit-base-patch16', 'laion400m-e31'),
     ('vit-base-patch16', 'laion400m-e32'),
     ('vit-base-patch16', 'laion2b-s34b-b88k'),
     ('vit-large-patch14', 'laion400m-e31'),
     ('vit-large-patch14', 'laion400m-e32'),
     ('vit-large-patch14', 'laion2b-s32b-b82k'),
     ('vit-huge-patch14', 'laion2b-s32b-b79k'))
    >>> open_clip_jax.list_pretrained_by_model('vit-base-patch32')
    ('laion400m-e31', 'laion400m-e32', 'laion2b-e16', 'laion2b-s34b-b79k')

The pre-trained parameters have been ported from OpenCLIP, and more information regarding them, such as their training recipes or zero-shot performance, can be found in the OpenCLIP repository or as model cards on [Hugging Face Hub](https://huggingface.co/models?library=open_clip).

**Questions, comments, and feedback are welcome in the comments. For more information, please refer to** [**the GitHub repository**](https://github.com/BobMcDear/open-clip-jax/)**.**",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199r1py/p_openclip_jax_clip_models_in_jaxflax/,1,6,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E32F6C4D0>
199w77k,GroovyChipmunk,2024-01-18 17:57:03+00:00,Kevin Murphys book [D],"As a first year PhD student; if I want to review and solidify my understanding of a vast variety of DS topics across statistics; optimization, ML which of Kevin Murphys books (0,1,2) should I use?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199w77k/kevin_murphys_book_d/,4,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F79E90>
199r5jg,Smart-Emu5581,2024-01-18 14:18:06+00:00,[D] What analyses and anomaly detections could be automated?,"I am working on [a debugging tool for neural networks](https://github.com/FlorianDietz/comgra). Currently it is useful for visualizations and in-depth manual analysis, something that is lacking in tensorboard and other tools.

I want to extend it to automate a lot of the common analyses and anomaly detections, and I'm looking for suggestions.

How it would work:

You run a number of trials on similar networks with similar tasks, with different hyperparameters. The tool logs all relevant data and automatically detects anomalies such as ""vanishing gradients"" or ""the loss has unusually high variance"".

In a second step, it performs a correlation analysis between the hyperparameters of each trial and the anomalies detected in those trials. It then generates a list of warnings for each statistically significant finding. For example:

* ""30% of trials with learning rate above 3e-4 had vanishing gradients, versus 0% of trials with learning rate below 3e-4.""
* ""50% of trials with architectural variant X had unusually high variance in the loss, versus 10% of trials with other architectural variants.""

Having a large list of warnings like these generated automatically would allow you to identify bugs very quickly. Additionally, if no warnings are generated then you can be much more confident in the stability of your model.

Of course, many warnings would also be false positives that aren't worth investigating, but I imagine it's better to be warned for no reason than to miss a problem that actually matters.

What do you think of the idea?

What types of anomalies do you think would make the most sense to look for?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199r5jg/d_what_analyses_and_anomaly_detections_could_be/,0,5,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E32F62DD0>
199xb4b,Thamelia,2024-01-18 18:42:21+00:00,[D] Searching for summary of specific part of different versions of technical documentation depending on the searched keywords,"Hello, 

I am looking for informations on a field. 
I have several technical documentation for different software existing in several versions depending on how the documentation has evolved over time. 

The idea would be that depending on the keywords/text specified we could extract from the documentation corresponding to the correct software a summary of the part corresponding to the keywords/text refered.

The output would be a summary which would take into account the evolutions of the corresponding part over the different versions of the documentation. 

Example: if user search 'bug for the software X'' 
In the V1 documentaion it explains that bug 22 and bug 24 exist. On the V2 explains that bug 22 is corrected.

Then the ouput explains that bug 22 is corrected  but not bug 24 for the X software.


Any ideas for publications, models pre-trained or not I can refer?

Thank you.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199xb4b/d_searching_for_summary_of_specific_part_of/,1,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F6F190>
199uobn,DBAdvice123,2024-01-18 16:54:07+00:00,New Data API for Astra [N],"I saw that DataStax/Astra DB [just released a new Data API to help with building production GenAI and RAG applications](https://www.datastax.com/blog/general-availability-data-api-for-enhanced-developer-experience). This API makes the proven petabyte-scale of Apache Cassandra easy to use and available to any JavaScript, Python, or full-stack application developer.

There will also be a joint webinar with LangChain available for registration here: [https://www.datastax.com/events/wikichat-build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel](https://www.datastax.com/events/wikichat-build-a-real-time-rag-app-on-wikipedia-with-langchain-and-vercel)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199uobn/new_data_api_for_astra_n/,0,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F82310>
199ty08,dark16sider,2024-01-18 16:23:32+00:00,[D] Good dataset for MRI breast cancer voxel segmentation.,"1.I looked around TCIA, I couldn't find a dataset with actual radiologist tumor segmentation (Duke,ISPY as far I checked don't include segmenation). The  thing I found is Breast\_Cancer\_DCE-MRI\_Data from zenodo. Are there more datasets?

2. Are there Breast Mri dataset which are normal with no findings",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199ty08/d_good_dataset_for_mri_breast_cancer_voxel/,0,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32EABDD0>
199ttno,Defiant-Cockroach-59,2024-01-18 16:18:17+00:00,[D] Metrics,"I'm trying to calculate metrics for a multi class classification problem.

I have a zero shot model that classifies across a number of candidates.

I have a ground truth set with a single class (y_true). 

Currently I am picking the class with the highest prediction confidence to be my predicted result (y_pred).

What could I be missing here? Ideally I want to be focused on my precision and recall equally, but if I pay more attention to precision that's not a problem either. Losing out precision is definitely an issue.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199ttno/d_metrics/,0,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E327E9B50>
199xz5g,JellyfishPretend447,2024-01-18 19:09:18+00:00,[D] Checking Accessibility of a document,"
Hi everyone! 

I am trying to build a machine learning based system to check the accessibility of documents such as pdf but I am not sure how to approach this problem. Initially I was thinking to use python library for each of the criteria such as to check weather a pdf is scanned pdf or text based, or the contrast and text size but there are a lot of criteria’s that needs to be checked. 

Is there any better way I can approach this problem by using accessible and inaccessible document data? 

Thanks",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199xz5g/d_checking_accessibility_of_a_document/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32F76490>
199lz7g,ActuaV,2024-01-18 09:07:59+00:00,[D] Adaptation of neural networks to increasing/decreasing noise.,"Say you train a classification neural network with a fixed structure (number of neurons, hidden layers, convolutions, ...) on an image classification dataset until convergence. The performance on your test data can no longer be improved.  


Would this neural network in theory also be optimally trained to classify a more (or less) noisy version of your data, assuming that the noise is completely random for every pixel? Performance will obviously be impacted by the noise, but would the neural network have to be retrained if you are expecting higher/lower noise in future classification tasks?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199lz7g/d_adaptation_of_neural_networks_to/,6,5,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E32EB23D0>
199vm1x,neocolonialoverlord,2024-01-18 17:32:53+00:00,[Discussion] LLMs for Data Analysis,"Hi guys,

I have a question regarding LLMs and Databases. Kindly permit me to elaborate.

Inspite of all the advancements in the field of AI, one area where there seems to be a limitation is LLMs analyzing DATA and getting answers using SQL. I can explain what I mean with this simple use case:

1.A typical small independently owned convenience store serving the local neighborhood will have a POS (Point of sale). In the backend this POS will have a database with a schema that has many many tables for sales, inventory management, payment processing, customer management, etc.. Of course, large stores or for that matter any big organization will have larger backend databases.

2. Given the above scenario, A user should be about to chat with the database in natural language. they should be able to ask (Text or voice) any type questions related to data Analysis and get a response either in text or graphs or even images. In essence, the LLM is doing the work of a data Analyst.

3. Yes, It is possible now to upload a table in CHATGPT in CSV format and get some simple answers. But as the number of tables increase the LLM's capability becomes limited even with advanced LLMs like GPT4. Some people have tried to name the tables and columns in such a way so that LLMs can understand better. However, even in this case the results are not good.

Is there is currently an LLM solution for this problem? From what I understand this seems to be the holy grail for LLMs.

The potential for humans talking in natural language to an LLM for the purpose of data analysis is HUGE.

Thanking you for your time in much appreciation. Looking forward to your reply.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199vm1x/discussion_llms_for_data_analysis/,2,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32F79BD0>
199v5v3,Time_Lord7,2024-01-18 17:13:40+00:00,[D] what's the best tts rvc combo?,"currently i'm using Mangio-RVC to convert audio to audio. and it's doing wonders.

however i need some tts too. not only audio to audio. what are my best options? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199v5v3/d_whats_the_best_tts_rvc_combo/,4,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E2F173290>
198yyzy,fferflo,2024-01-17 15:20:43+00:00,[P] einx - Tensor Operations in Einstein-Inspired Notation for Python,"## What?

[einx](https://github.com/fferflo/einx) is a Python library that allows formulating many tensor operations as concise expressions using Einstein notation. It is inspired by [einops](http://einops.rocks/).

## Why?

Classical index-based notation is often overly complex and lacks readability and expressiveness. einops was introduced in 2018 to address this problem and provide an alternative way of formulating tensor operations by using Einstein-inspired notation.

While einops has transformed the way many researchers write deep learning code, it has focused mainly on few operations (e.g. `einops.{rearrange|repeat|reduce|einsum}`) and supports only a limited set of expressions. einx seeks to expand on the idea of using Einstein notation for tensor operations and fully utilize its potential.

## How?

**1. Bracket-notation:** `[]`\-brackets in einx are used similar to the `axis` argument in Numpy functions:

    einx.sum(""a [b]"", x)                               # einx
    np.sum(x, axis=1)                                  # numpy
    einops.reduce(x, ""a b -> a"", reduction=""sum"")      # einops

**2. Fully composable Einstein expressions** allow nesting different types of expressions. For example, combine brackets with ellipses

    # Global spatial average pooling
    einx.mean(""b [...] c"", x)

add a bracket inside a composed axis,

    # Flip pairs of values along the second axis
    einx.flip(""a (b [c])"", x, c=2)

combine ellipses with composed axes

    # Divide a list/image/volume into sublists/patches/cubes of size p
    einx.rearrange(""(s p)... -> s... p..."", x, p=8)

and many more. einx fully supports rearranging expressions in all operations:

    # Compute multi-head attention matrix
    einx.dot(""b q (h c), b k (h c) -> b q k h"", q, k, h=16)

**3. Support for many tensor operations** such as `einx.{sum|max|where|add|dot|flip|get_at|...}` following a Numpy-like naming convention: `einx.{OP}` typically calls `np.{OP}` internally (or the PyTorch, Tensorflow or Jax equivalent) and represents the einx interface to `np.{OP}`. Usage of Einstein notation is consistent in different operations.

**4. Just-in-time compilation of all operations into regular Python functions** using Python's [`exec()`](https://docs.python.org/3/library/functions.html#exec). This reduces the overhead of using einx to a cache lookup per call (compared to directly writing out the index-based calls), and allows inspecting the code of the compiled function (see [Just-in-time compilation](https://einx.readthedocs.io/en/latest/gettingstarted/jit.html)). For example, `einx.sum(""a [b]"", x)` compiles to:

    def reduce(i0, backend):
        x1 = backend.to_tensor(i0)
        x2 = backend.sum(x1, axis=1)
        return x2

## What's more?

einx shines especially in the context of deep learning since it allows formulating many common neural network operations as concise einx expressions ([see examples](https://einx.readthedocs.io/en/latest/gettingstarted/commonnnops.html)). Additionally, the namespace `einx.nn` provides generalized layer types that can be used with PyTorch, Flax, Haiku, Equinox and Keras to implement a variety of existing modules out-of-the-box. For example:

    import einx.nn.{torch|flax|haiku|equinox|keras} as einn
    batchnorm       = einn.Norm(""[b...] c"", decay_rate=0.9)
    layernorm       = einn.Norm(""b... [c]"") # as used in transformers
    instancenorm    = einn.Norm(""b [s...] c"")
    groupnorm       = einn.Norm(""b [s...] (g [c])"", g=8)
    rmsnorm         = einn.Norm(""b... [c]"", mean=False, bias=False)

As an example, [here is a working Mamba implementation](https://github.com/fferflo/weightbridge/blob/master/examples/mamba2flax.py) using einx with Google's [Flax](https://github.com/google/flax).

The above is only a small extract of what you can do with einx. For more information, see [Github](https://github.com/fferflo/einx), [Documentation](https://einx.readthedocs.io/) and [Tutorials](https://einx.readthedocs.io/en/latest/gettingstarted/einsteinnotation.html).

Or give it a try now: `pip install einx`

    import torch, einx
    x = torch.arange(10)
    x = einx.flip(""(a [b])"", x, b=2)
    print(x)

This library was created as a side-project during my PhD. Feel free to open issues on Github if you have any problems or feature requests. If you are wondering how to express some operation in einx notation, feel free to leave a comment and I'll give it a try!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198yyzy/p_einx_tensor_operations_in_einsteininspired/,30,112,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E32FC55D0>
199y8os,pikachuunibyo,2024-01-18 19:20:21+00:00,"[D] Java devs, where do you put your models?","I manage a maven repository, and I put models in a cloud storage to be downloaded to a volumes category that is retrieved during runtime (plus, it's easier to dockerize). I feel like loading the model during each `mvn clean install` can be really time-consuming, and having a large model in a git repo feels bad.

Does anyone put their models together with the java project, or do you do something else?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199y8os/d_java_devs_where_do_you_put_your_models/,1,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E32EAB5D0>
199qmqt,ItsjustabirdSon,2024-01-18 13:53:07+00:00,[D] Recommender System for Graph Data," I have a database filled with graphs. Each graph represents an assembly built by an engineer, where each node represents a component and each edge represents a connection between them. There is no additional information about the components except for an ID. What do you think are recommender techniques that could produce good results in recommending the next component during the assembly of new assemblies? One approach I have explored is through graph neural networks, and it works well. I am interested in more 'traditional' approaches, without neural networks. What are your ideas guys? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199qmqt/d_recommender_system_for_graph_data/,2,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32EBB090>
1999ipe,APaperADay,2024-01-17 22:17:56+00:00,[R] Scalable Pre-training of Large Autoregressive Image Models,"**Paper**: [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)

**Code and Models**: [https://github.com/apple/ml-aim](https://github.com/apple/ml-aim)

**Models**: [https://huggingface.co/apple/AIM](https://huggingface.co/apple/aim)

**Abstract**:

>This paper introduces **AIM**, a collection of vision  models pre-trained  with an autoregressive objective. These models are  inspired by their  textual counterparts, i.e., Large Language Models  (LLMs), and exhibit  similar scaling properties.  Specifically, we highlight two key findings:  (1) the performance of the  visual features scale with both the model  capacity and the quantity of  data, (2) the value of the objective  function correlates with the  performance of the model on downstream  tasks. We illustrate the  practical implication of these findings by  pre-training a 7 billion  parameter AIM on 2 billion images, that  achieves 84.0% on ImageNet-1k  with a frozen trunk. Interestingly, even  at this scale, we observe no  sign of saturation in performance,  suggesting that AIM potentially  represents a new frontier for training  large-scale vision models. The  pre-training of AIM is similar to the  pre-training of LLMs, and does  not require any image-specific strategy  to stabilize the training at  scale.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1999ipe/r_scalable_pretraining_of_large_autoregressive/,3,14,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32FC2950>
199oicr,purton_i,2024-01-18 11:57:09+00:00,[Project] Create a RAG pipeline in less than 5 minutes,"[https://www.youtube.com/watch?v=mNFd0Bur238](https://www.youtube.com/watch?v=mNFd0Bur238)

Here I walk through the steps of using [https://bionic-gpt.com](https://bionic-gpt.com/) to create a no-code retrieval augmented generation pipeline. Code is here [https://github.com/bionic-gpt/bionic-gpt](https://github.com/bionic-gpt/bionic-gpt)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199oicr/project_create_a_rag_pipeline_in_less_than_5/,0,0,0.45,<praw.models.comment_forest.CommentForest object at 0x0000026E3281EBD0>
199npa7,sushilkhadakaanon,2024-01-18 11:06:29+00:00,[D] Injecting 3D information into the reverse diffusion process,"What would be the best way to incorporate 3d model data into the diffusion process so that the diffusion-generated sample consists of the foreground object from the 3d model?   


For example, I have an image dataset that has a foreground object as a 'pencil', and I have a 3d model of a pen, and want to create a new dataset by replacing a pencil with a pen. i.e. the background would be learned from the dataset and the foreground object would be learned with the 3d model.  I  first thought of the simple projection of a 3d model, and then concatenating this during the reverse process similar to text/image conditioning. But I'm worried that our actual distribution has no picture of the pen so the reverse diffusion process would neglect the 3d information and the approximated distribution would be similar to the actual one.   


Is there any work done in the area that you're aware of? I would appreciate any suggestion.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199npa7/d_injecting_3d_information_into_the_reverse/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32C1E2D0>
198y67i,santiviquez,2024-01-17 14:46:08+00:00,[D] Confidence * may be * all you need.,"&#x200B;

[edit: https:\/\/aclanthology.org\/2023.eacl-main.75\/](https://preview.redd.it/xrx0acvqi0dc1.png?width=800&format=png&auto=webp&s=da756c351b898ae93725ca563066328b48a59003)

&#x200B;

I'm curious to know if anyone here has tried this in practice.

A simple average of the log probabilities of the output tokens from an LLM might be all it takes to tell if the model is hallucinating. The idea is that if a model is not confident (low output token probabilities), the model may be inventing random stuff. The authors claim that this simple method is the best heuristic for detecting hallucinations. The beauty is that it only uses the generated token probabilities, so it can be implemented at inference time.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198y67i/d_confidence_may_be_all_you_need/,25,42,0.84,<praw.models.comment_forest.CommentForest object at 0x0000026E32FE5650>
199rpyt,LostStudent7974,2024-01-18 14:44:32+00:00,[Discussion] Need input on my AI model,"Hello smart!  
Im creating an AI model in order to predict when is the best time to send a notification to different people. As features I have used each person's past engagement with the messages I have sent in the past and as target I have used when they read my last message (the exact hour).  
Then I use Random forest in order to train the model. I do not think using the hour of when they read my last message as target is optimal.   


Could you give me suggestions on what I could use as my target/label?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199rpyt/discussion_need_input_on_my_ai_model/,1,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E32FC7550>
1993okp,One_Definition_8975,2024-01-17 18:25:03+00:00,AlphaGeometry: An Olympiad-level AI system for geometry[D],"https://www.nature.com/articles/s41586-023-06747-5
Introducing AlphaGeometry: an AI system that solves Olympiad geometry problems at a level approaching a human gold-medallist. 📐

It was trained solely on synthetic data and marks a breakthrough for AI in mathematical reasoning",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1993okp/alphageometry_an_olympiadlevel_ai_system_for/,1,18,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E32FB2B90>
1994mio,Puzzleheaded_Stay_62,2024-01-17 19:02:31+00:00,[D] DPO Paper Potential Derivation Issue,"I wanted to point out a potential error in the derivation of gradient for the DPO Loss function. 

Loss function in Equation 7 states:

https://preview.redd.it/n2y68o10t1dc1.png?width=1130&format=png&auto=webp&s=6ec12ea6f75edc2fabee51e35c799d2c549611f6

whereas for the derivation in the appendix in Equation 21 we see that the negative sign is reversed as shown below.

https://preview.redd.it/v68pyfy1t1dc1.png?width=1218&format=png&auto=webp&s=08401b5f0c3a49d5ce97f781a89fc10e9991e1f5

However, the overall gradient used in the main section of the paper is correct and seems like only an issue with the appendix.

Please let me know if my understanding is correct (A little confused since I get a different answer when trying to derive the equations by myself.)

Paper Link: [https://arxiv.org/abs/2305.18290](https://arxiv.org/abs/2305.18290)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1994mio/d_dpo_paper_potential_derivation_issue/,4,14,0.89,<praw.models.comment_forest.CommentForest object at 0x0000026E32FD4F50>
1996gzj,Reference-Guilty,2024-01-17 20:15:15+00:00,[D] Finetune all hyperparameters in one-go or divide them in categories ?,"Hello,

I'm in the process of fine-tuning my hyperparameters. I've been wondering if there has been any strategy in the literature concerning the way to fine-tune an ensemble of hyperparameters.

I am not talking about the finetuning algorithm itself, i.e Grid Search, Random Search etc. I am talking about fine-tuning smaller sets one by one

&#x200B;

Example of categories :

data pre-processing : tokenization method, etc

training parameters : learning rate, batch size, optimizer, its momentum etc

model architecture : number of layers, neurons, activation function, batchnorm, dropout parameters etc

other algorithms inside : data augmentation, diffusion parameters etc

&#x200B;

I'd say in total I have around \~20 hyperparameters I can touch. Is it better to just fine-tune everything together or is it better practice to fine-tune categories of hyperparameters one by one ?

I have a feeling that some ""categories"" will have such a big impact/variance on the performance that it might add too much noise on other parameters

&#x200B;

Curious to see how the community handle that part of the pipeline",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1996gzj/d_finetune_all_hyperparameters_in_onego_or_divide/,7,10,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32FB4C10>
199lnnd,MustafaAlahmid,2024-01-18 08:44:32+00:00,[R] What is the state of art model for face detection and extract face embeddings ?,"I am trying to implement a face similarity app   
i have found some open source like deepface, face recognitions, opencv and others that di face detection and extract face embeddings but they are not very accurate   


so the question is what is the state of art research in this area ? face embedding and face similarity ?   
",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199lnnd/r_what_is_the_state_of_art_model_for_face/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32FD75D0>
1999kd5,APaperADay,2024-01-17 22:19:52+00:00,[R] Multi-agent Reinforcement Learning: A Comprehensive Survey,"**Paper**: [https://arxiv.org/abs/2312.10256](https://arxiv.org/abs/2312.10256)

**Abstract**:

>The prevalence of multi-agent applications pervades various   interconnected systems in our everyday lives. Despite their ubiquity,   the integration and development of intelligent decision-making agents in   a shared environment pose challenges to their effective  implementation.  This survey delves into the domain of multi-agent  systems (MAS),  placing a specific emphasis on unraveling the  intricacies of learning  optimal control within the MAS framework,  commonly known as multi-agent  reinforcement learning (MARL). The  objective of this survey is to  provide comprehensive insights into  various dimensions of MAS, shedding  light on myriad opportunities while  highlighting the inherent challenges  that accompany multi-agent  applications. We hope not only to contribute  to a deeper understanding  of the MAS landscape but also to provide  valuable perspectives for both  researchers and practitioners. By doing  so, we aim to facilitate  informed exploration and foster development  within the dynamic realm of  MAS, recognizing the need for adaptive  strategies and continuous  evolution in addressing emerging complexities  in MARL.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1999kd5/r_multiagent_reinforcement_learning_a/,0,6,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E3280E890>
198xx6o,kekkimo,2024-01-17 14:34:22+00:00,[D] Does the vocabulary size really affect the size of textual LLMs?,"Is the embedding matrix sizeable compared to the other components of the transformer?

If not, then why GPT models are relying on a 30K vocab size?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198xx6o/d_does_the_vocabulary_size_really_affect_the_size/,18,22,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E32FFA950>
199t8be,Ok_Cartographer5609,2024-01-18 15:52:34+00:00,"[D] While using function calling or tools on openai or langchain, does openai have access to the data or is it just that the output gets passed as intput to next function.",I am working on a client project and I am using langchain's tools and agents. I want to know if the data is getting passed to openai or is it just like that - Output of one function is being directly passed to the second function with the knowledge of openai.,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199t8be/d_while_using_function_calling_or_tools_on_openai/,2,0,0.3,<praw.models.comment_forest.CommentForest object at 0x0000026E32FE6AD0>
1998wr2,xiaofanlu,2024-01-17 21:53:04+00:00,why speculative decoding's output distribution is guaranteed to stay the same? speculative sample [Research],"&#x200B;

https://preview.redd.it/fk76iuoan2dc1.png?width=357&format=png&auto=webp&s=7b909330a6ccf70258e620c2cc1cfdfa11ee4c40

https://preview.redd.it/n78eend9n2dc1.png?width=353&format=png&auto=webp&s=316b4b74fed8360b5d83845a20f757d9331d131b

q(x) is draft model

p(x) is original, target model

I don't understand why after the speculative decoding algorithm the output distribution is the same as target model distribution, for example:

1. why keeping xi if q(xi) <= p(xi) will not change the output distribution from target model?
2. why we need sample x again from norm(max(0,p(x) -q(x) ) if x was rejected
   1. and why normalized p(x) is not changing the distribution from target model?

really appreciate for any hint and explanation for why the distribution will not change after speculative sampling",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1998wr2/why_speculative_decodings_output_distribution_is/,2,5,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32FF5090>
199crza,nobilis_rex_,2024-01-18 00:36:57+00:00,"[D] What proprietary datasets, not available as open source, would widely be considered as valuable and popular for integration into ML/AI applications?","Built quite a few projects using HF and I always appreciate open-source data when it comes to building models/real-world computational applciations. That got me thinking, what type of proprietary data (can come from local businesses, medium size businesses, organizations etc...) would be popular if people had access to it?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199crza/d_what_proprietary_datasets_not_available_as_open/,10,2,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E32FECF10>
199cr6b,HauntingBeach,2024-01-18 00:35:55+00:00,[P] Ramen AI - Classify Text using LLM AI as API or Google Sheets Formula,"I built this free tool for folks to classify text without any model training required out of the box. Looking for ideas to make this tool more useful! You are welcome to give it a try. Just join the waitlist first and I will approve you shortly.

[https://tryramen.com](https://tryramen.com)

&#x200B;

https://preview.redd.it/faybnrwng3dc1.jpg?width=2244&format=pjpg&auto=webp&s=1d0165fd339cd0df70a831086be7c5da0069c4ca",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199cr6b/p_ramen_ai_classify_text_using_llm_ai_as_api_or/,1,4,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E32FF4D10>
199hg4g,Shot-Button-9010,2024-01-18 04:24:40+00:00,[D] Simple question about IJCAI 2024 resubmission information,"The deadline for supplementary material and resubmission information is next week.

I'm trying to understand the website, but I couldn't find the template for resubmission file, even not included in the LaTex form. Where do you guys found it before?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199hg4g/d_simple_question_about_ijcai_2024_resubmission/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32FF6290>
199fa01,thefreemanever,2024-01-18 02:35:28+00:00,[D] How can we estimate fine tuning time of a model?,"Is there a simple formula to obtain a rough estimation of the percentage of computational power/time needed to fine-tune a model? 

For instance, if a model took 10 days to be trained on 100 of X-GPUs, how long would it take for fine tuning that model on 1 X-GPU?

I am aware that this calculation depends on various factors and is not straightforward, but I am attempting to gain an idea of whether it is worth investing in a multi-GPU setup for fine-tuning models or not?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199fa01/d_how_can_we_estimate_fine_tuning_time_of_a_model/,11,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E32FFB850>
1994md2,btcmx,2024-01-17 19:02:19+00:00,"[D] Foundation Models (including GPT-4V) aren’t ready for prime time, but they will introduce a Computer Vision Pipeline 2.0","A similar idea occurred to me I began to tweak CLIP for the first time (kudos to [this](https://www.reddit.com/r/bigsleep/comments/p15fis/tutorial_an_introduction_for_newbies_to_using_the/) old reddit post): Foundation Models will replace annotation and training (while data remains king) creating a Computer Vision pipeline 2.0 just as this well-put [article](https://tenyks.docsend.com/view/59rf5vadkifyp3bz) argues! 

What are your thoughts?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1994md2/d_foundation_models_including_gpt4v_arent_ready/,1,2,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E3300D0D0>
199ny3t,ThisIsBartRick,2024-01-18 11:22:24+00:00,[D] Why do MOE have only 8 experts and none have tried to go for 16 for ex?,"That way you increase the number of parameters without increasing the inference cost, right? Or is there a catch that I'm not aware of?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199ny3t/d_why_do_moe_have_only_8_experts_and_none_have/,13,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E33002790>
19882l2,Excusemyvanity,2024-01-16 17:13:13+00:00,[D] How do you deal with unreasonable request from an employer with unrealistic expectations of ML?,"Several months ago, I accepted a position to support a social science research project by training a ML model for them. The project involves using a dataset that the team (consisting of multiple interns, grad students, postdocs and professors) has compiled over several years and at an insane level of effort. However, the issue is that they failed to consult with anyone who actually knows ML beforehand. Their dataset is way too small (only about 200 rows) for what is a very complex task. To make things worse, most variables hold minimal predictive value and the methods used to derive them, while very labor intensive, raise concerns about their validity.

The project's MO was absolutely bewildering: amass thousands of predictors through immense effort and manpower, expecting perfect outcomes. How any model could estimate so many parameters with such a small dataset was overlooked. The project leader seems to have a somewhat magical understanding of ML in general, likely influenced by its frequent misuse in their specific field. This project in particular was inspired by a research paper that I can virtually guarantee to have overfitted on its validation set.

All of this puts me in the awkward situation that I, as the newcomer, will need to inform a team of experienced postdocs and professors, all from a social science background without quantitative expertise, that their years of work have resulted in a dataset that is entirely unsuitable for their objectives and that the preexisting literature they built upon is all wrong because they apparently didn't know what a test set is and when to use it. I also can't tell them to just expand the dataset, given that getting to 200 rows took years already.

I have to admit that I am a little nervous about that conversation.

&#x200B;

I suspect encountering unrealistic expectations regarding the capabilities of ML is a common experience. How do others handle this? Do you bluntly tell them it doesn't work and find a job elsewhere if they insist regardless? If so, how do these interactions normally go?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19882l2/d_how_do_you_deal_with_unreasonable_request_from/,122,277,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E33067390>
199ictx,HappyDataGuy,2024-01-18 05:14:15+00:00,[D] How can I make LLM plot graphs/figures on my database with RAG?,I want for LLM like ChatGPT to plot graph but not with code-interpretor which requires uploading the data to openai. is there any way to achieve this? ,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199ictx/d_how_can_i_make_llm_plot_graphsfigures_on_my/,2,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E33029010>
198eiv1,spring_m,2024-01-16 21:29:34+00:00,[P] Small Latent Diffusion Transformer from scratch,"I trained a relatively simple transformer based diffusion model to generate 256 by 256 images from scratch. Here is the repo: [https://github.com/apapiu/transformer\_latent\_diffusion/tree/main](https://github.com/apapiu/transformer_latent_diffusion/tree/main) \- the code should hopefully be fairly easy to understand and self-contained.

Here are some examples after about 30 hours of training on 1A100 from scratch:

[generated images based on various prompts](https://preview.redd.it/ncucpk1pdvcc1.png?width=1564&format=png&auto=webp&s=df65131fee3353ec0f96e9e89483b3978f6f2974)

The model is based on a DiT/Pixart-alpha architecture but with various modifications and simplifications. I also made some questionable decision in terms of the noise schedule but it seems to work OK.

The model is 100MM params so it should be very easy to experiment with it. I welcome any feedback an also open to collaborations so please do reach out! Hopefully this is helpful to folks who want to experiment with diffusion models/transformers yet are ""GPU poor"" :)

The repo also links to a colab where you can use your own inputs - feel free to try it out.

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198eiv1/p_small_latent_diffusion_transformer_from_scratch/,17,99,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E33074850>
198xe4m,sabrinaqno,2024-01-17 14:10:00+00:00,[N] New Insights on Vector Databases Benchmarks,"We’ve compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis.

The detailed report: [https://qdrant.tech/benchmarks/](https://qdrant.tech/benchmarks/)  
Here's what changed: [https://qdrant.tech/blog/qdrant-benchmarks-2024/](https://qdrant.tech/blog/qdrant-benchmarks-2024/)

If you're interested in running these benchmarks or contributing, please visit our benchmark repository.  
[https://github.com/qdrant/vector-db-benchmark](https://github.com/qdrant/vector-db-benchmark)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198xe4m/n_new_insights_on_vector_databases_benchmarks/,0,3,0.81,<praw.models.comment_forest.CommentForest object at 0x0000026E330145D0>
198v842,SantaClaus_Y,2024-01-17 12:16:33+00:00,[P] Looking for Open Source AI Project to Contribute,"Hello, So I've been diving into Deep Learning for almost a year now, and have made several projects myself. In order to improve my skill set, I currently am seeking an open-source project to contribute to and can dedicate 5-20 hours per week. 

If you know any project that i could contribute to, please hit me up.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198v842/p_looking_for_open_source_ai_project_to_contribute/,2,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E3301F8D0>
198s5l1,KiraGhoulEmperor,2024-01-17 08:51:59+00:00,[D] Audio Generation using GAN,"I want to generate music using GAN but I have this question that should I use MFCC data, Spectogram or should I use .mp3 files directly and then proccess it using pyTorch audio proccessing module and also if there is something that i should keep in mind while working on music generation using GAN and also if there are any tips that i can use.

&#x200B;

Thank you!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198s5l1/d_audio_generation_using_gan/,3,7,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E33027AD0>
1994q8s,AdMother5294,2024-01-17 19:06:37+00:00,[D] Want to learn RL,"Hi y’all,

Just a bit of background I have spent years working on problems related to traditional ML and deep learning. Have implemented SOTA papers from scratch. As I delve more into this field, I feel I need to have the ML breadth too. I have been reading for sometime on RL, through blogs and articles online. However I think a structured approach might be useful. Anyone can point me to a list of resources( not just for learning but also exercises) will be really helpful.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1994q8s/d_want_to_learn_rl/,3,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E33003410>
1993xds,MAIHfly,2024-01-17 18:34:49+00:00,[D] what's a good tool to make a custom GNN?,"I tried opencog and gave up deciphering the documentation. If I was going to make GNN where I would define in depth the behavior of the nodes and edges in a customizable way, what would be the best tool in your opinion. I'm thinking flux through Julia, but idk. Thanks in advance to anyone willing to answer.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1993xds/d_whats_a_good_tool_to_make_a_custom_gnn/,6,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E33075250>
1986k0x,APaperADay,2024-01-16 16:12:24+00:00,[R] Transformers are Multi-State RNNs,"**Paper**: [https://arxiv.org/abs/2401.06104](https://arxiv.org/abs/2401.06104)

**Code**: [https://github.com/schwartz-lab-NLP/TOVA](https://github.com/schwartz-lab-NLP/TOVA)

**Abstract**:

>Transformers are considered conceptually different compared to the  previous generation of state-of-the-art NLP models - recurrent neural  networks (RNNs). In this work, we demonstrate that decoder-only  transformers can in fact be conceptualized as infinite multi-state RNNs -  an RNN variant with unlimited hidden state size. We further show that  pretrained transformers can be converted into *finite*  multi-state RNNs by fixing the size of their hidden state. We observe  that several existing transformers cache compression techniques can be  framed as such conversion policies, and introduce a novel policy, **TOVA**,  which is simpler compared to these policies. Our experiments with  several long range tasks indicate that TOVA outperforms all other  baseline policies, while being nearly on par with the full (infinite)  model, and using in some cases only 1/8 of the original cache size. Our results indicate that transformer  decoder LLMs often behave in practice as RNNs. They also lay out the  option of mitigating one of their most painful computational bottlenecks  - the size of their cache memory. We publicly release our code at [this https URL](https://github.com/schwartz-lab-NLP/TOVA).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1986k0x/r_transformers_are_multistate_rnns/,16,104,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E3307FE10>
1991vep,knockknockwhodiss,2024-01-17 17:14:59+00:00,[D] Perspective matching of pictures taken from slightly different angles and analyzing their differences for medical research,"TL;DR: searching for a way to (perspective) match 2-4 macro photos of one person's teeth taken by hand (because they are taken by hand, there are slight perspective differences). After matching the pictures, I am searching for a way to output the differences in the two pictures.

Hello everyone, I need a little help with my PhD.

Here are a few key data to explain the project:

I have connections to a center where dental surgery is performed. Almost every surgical case is documented with macro images (before the operation, immediately after the operation, two weeks after the operation, one year after the operation).

Data is available from around 200-400 patients and this data is now to be analyzed in order to demonstrate the success of the therapy.

The therapy involves covering exposed tooth necks with tissue taken from elsewhere in the mouth.

As the images are taken by hand, the individual images are always taken from slightly different angles.

The basic idea is to automatically match the images in perspective (as I understand it, this step is necessary for the subsequent step) and then automatically compare the differences between the pre- and post-op images. As a result of the surgery, the white / light tooth necks on the pre-op images are covered with red mucosa, which can be seen on the post-op images (so there are strong color differences between before and after in this area). 

The aim is to automatically output the surfacearea that was successfully covered by the op.

If my explanations up to this point are understandable, I wonder whether it would be conceivable to implement the project or is the data situation / data quality insufficient? I estimate that the evaluation of the data with calculation of the surfacearea is largely dependent on the quality of the matching, right? Are there good and reliable tools for this (would this be a case for Kornia?)

Maybe anyone is willing to help or has a good idea - if it works I will mention you in the final paper! :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1991vep/d_perspective_matching_of_pictures_taken_from/,3,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32ECC890>
198p87h,eee-vaaah,2024-01-17 05:42:30+00:00,[N] VizWiz Launches 6 AI Challenges to help blind/low vision community,"Greetings!

We are pleased to announce the sixth annual [VizWiz Grand Challenge workshop](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fvizwiz.org%2Fworkshops%2F2024-vizwiz-grand-challenge-workshop%2F&data=05%7C02%7Cdmassiceti%40microsoft.com%7C3d3d2818230b4bab9a4e08dc16c6f158%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638410294061449015%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C62000%7C%7C%7C&sdata=DktAx8ERQ79CIn62w2ZJKgEp4tF9tbyVX%2BphCI2mCxA%3D&reserved=0), which will be held in conjunction with CVPR 2024.  We welcome you to participate and would greatly appreciate it if you could help us spread the word.

This workshop is motivated in part by our observation that people who are blind have relied on (human-based) visual assistance services to learn about images and videos they capture for over a decade.  We introduce visual question answering, few shot recognition, and object localization dataset challenges for the AI community to represent authentic use cases.  

Challenges:

* [Visual Question Answering (VQA)](https://vizwiz.org/tasks-and-datasets/vqa/)
* [VQA grounding](https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/)
* [VQA groundings with multiple answer groundings](https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/)
* [Few-shot video object recognition](https://eval.ai/web/challenges/challenge-page/2186/overview)
* [Few-shot object localization](https://vizwiz.org/tasks-and-datasets/few-shot-private-object-localization/)
* [Zero-shot image classification](https://vizwiz.org/tasks-and-datasets/image-classification/)

Key Dates:

* Friday, January 12: challenges go live
* Friday, May 3: submissions of algorithm results due to the evaluation server
* Friday, May 10: extended abstracts due
* Friday, May 17: notifications to authors about decisions for extended abstracts
* Challenge results will be announced at the VizWiz Grand Challenge workshop at CVPR 2024

Looking forward to your participation!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198p87h/n_vizwiz_launches_6_ai_challenges_to_help/,1,8,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E33075CD0>
198zeo7,varun-saha,2024-01-17 15:38:44+00:00,[P] HOEFFDING ALGORITHM AND MOA,"Has anyone here worked with MOA OSS for data streaming and mining or another similar software? I have been given the task by one of my professors to use streaming data and calculate different parameters for hoeffding algorithm. I m clueless as to how MOA works, I have downloaded the software on my m2 mac and am now having trouble running it. I would appreciate suggestions for other softwares too.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198zeo7/p_hoeffding_algorithm_and_moa/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E3307AB50>
197zbzs,Ok_Care_886,2024-01-16 09:57:50+00:00,[D] Interesting occurrence about text detection of iPhones,"I tapped on this image couple of times and it detected the second dog as the word ""dog"" written in Chinese. I don't think there is a reason for it but if anyone has any ideas, I'd love to hear them.",MachineLearning,https://www.reddit.com/gallery/197zbzs,15,234,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E330794D0>
198k375,Appropriate_Cut_6126,2024-01-17 01:25:13+00:00,[D] Offline batch serving,"I‘m confused about how to serve a machine learning model for offline batch predictions.

Here’s what I thought of doing - creating a scheduled pipeline (with e.g. Airflow, Kubeflow, …) that generates the features and then loads the trained model from some object store (e.g. s3), generates the predictions and finally saves the them to a data warehouse ready to be consumed. That’s what makes the most sense to me.

However, some resources seem to recommend to deploy the model as an endpoint even for batch use cases. Notably, this is the recommended architecture in Designing Machine Learning Systems by Chip Huyen.

Any thoughts on this? Am I missing something?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198k375/d_offline_batch_serving/,7,13,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E33087A10>
198jh0u,back-off-warchild,2024-01-17 00:57:28+00:00,[D] Project Template/Steps for rolling out a Recommender Model in an organisation,"I'm  somewhat comfortable in building a content recommendation model. I'm  less sure on how to scope out the project and the wider steps involved  in rolling out one into our app.

Are  there any blueprints out there on project managing a feature like this?  E.g. Aim, feature exploration, model selection, training, testing,  ""steps to implement the model into production""?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198jh0u/d_project_templatesteps_for_rolling_out_a/,5,11,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E33087250>
198oqcb,Diligent_Eye1248,2024-01-17 05:14:48+00:00,[D] Collaborative platform to train your models?,"Hi, with my team we are training an OS LLM and want to train it/test it together (control of the experiments, evaluation, comments etc...)

For now we use weight & biases but personally I'm not a huge fan of their UX.

Do you have any other tools that you recommend ?

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198oqcb/d_collaborative_platform_to_train_your_models/,2,4,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E33088890>
1989o55,jumpyAlucard,2024-01-16 18:16:13+00:00,[D] MAMBA models on time series data," 

Hey everyone, I've recently been enthusiastic about the MAMBA architecture, particularly because it employs linear time-independent mathematical models as a type of memory. I'm keen to apply it to time series classification or regression tasks, but most of the information I find online focuses on its use in language modeling. Despite my attempts to train these models on a time series dataset, it appears that they aren't learning anything. I'm wondering if any of you have come across examples of MAMBA models successfully being trained on time series data in order to find what i'm doing wrong. Thanks in advance!

 ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1989o55/d_mamba_models_on_time_series_data/,10,25,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E33091290>
199244h,riccardofratello,2024-01-17 17:24:20+00:00,[D] Transcribing a Spotify Podcast without downloading it,"I would like to transcribe a Spotify Podcast. The podcast is exclusively available on Spotify. 

The easiest way would of course be to rip given Podcast somehow from Spotify. Is there any way to do this without downloading the podcasts. I was imagining to use some live transcription and virtual audio drivers. It's ca. 300 episodes, so I am looking for a highly automated approach

Would appreciate any hint!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/199244h/d_transcribing_a_spotify_podcast_without/,0,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E33089D10>
1993ovf,Snoo89157,2024-01-17 18:25:22+00:00,[D],"Any opinion? I still struggle with decision to invest in Apple M3 Max , 40 NEngine and 128 MB RAM… as Nivida RTX sounds safer option though don’t know any Laptop with close power of RAM … and once it approaches CUDA M3 might be far better/ weight and design of machine matters to me a bit/ but would not take a risk of limited power with models/ based on images in my case used for architecture",MachineLearning,https://www.reddit.com/gallery/1993ovf,6,0,0.3,<praw.models.comment_forest.CommentForest object at 0x0000026E33090B10>
198t6w2,lanaaabananaaa,2024-01-17 10:05:34+00:00,[P] Sample Weights for Hierarchical Classification,"Hello, everyone!
I’m quite new to ML projects having only worked with simpler concepts of regression and classification.

At the moment I’m trying to develop a pipeline for hierarchical classification using an RFC. I have 3 category levels but for now I’m only trying to predict up to the second one. For the first category I achieved an accuracy of 81%, which is satisfactory for now as this is still a proof of concept.

For the second level, what I did was use this trained model to predict the Parent Category and use this as input in the next RFC, i.e. I have a dataset with all the attributes used previously plus a column predicted by the previous model. However, there are wrong predictions and so I have to find a way for the next model to recognise the errors. I’ve been reading and thought about using sklearn’s sample_weights during training, but I don’t know if this is the best alternative.

For context, the parent level has 5 different categories, and for children it depends on the parent. So I’m making a model for each of the parent nodes.

Thanks in advance for your help!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198t6w2/p_sample_weights_for_hierarchical_classification/,0,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E33090F10>
198rbh9,No-Trifle2470,2024-01-17 07:51:22+00:00,[D] NLP to AR/VR?,"Hi all, I am NLP applied scientist (NLP PhD + 7 yoe) working on LLM. 
I am thinking to switch to AR/VR career and I don’t know if I making a good decision or not.

Thanks for your help.

[View Poll](https://www.reddit.com/poll/198rbh9)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198rbh9/d_nlp_to_arvr/,4,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E32FB33D0>
198r9qd,Hot-Highlight8842,2024-01-17 07:48:01+00:00,[D]v8 and mixed precision,"Mix-precision calculation shares same attributes with processing instructions. Is it possible to make a v8 out of mixed-precision   calculations? If so, can the other side attach to the LLM's file or blockchain file? Aka can the crankshaft be how you process LLMs?    ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198r9qd/dv8_and_mixed_precision/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E32ED3E10>
198jyq7,Jor_ez,2024-01-17 01:19:37+00:00,[P] Supervised (maybe) clustering or multiagent traveling merchant problem,"Hi guys. I have a problem of assigning labels for group of map points. We can see them as point that we should visit. Also we have a number of scouts that will visit those points. We should do it with the leas amount of time. That is sort of traveling merchant problem but with multiple merchants.

My idea was to determine weight of each point as distance to other points and then aply Weghted K-means with N clusters. After that we can solve individual optimization problems

But I have some concerns about this approach. We have labels, how this groups where assigned by a human, and we do not use it at all. We can't solve classification problem because of different number of scouts each time. Also, this algorithm can't label points as 'noise' (like from HDBSCAN), this is not crucial but might be usefull for points that are super distant from others

Any considerations of how to solve it?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198jyq7/p_supervised_maybe_clustering_or_multiagent/,0,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32ED3D10>
197jp2b,Starks-Technology,2024-01-15 20:56:46+00:00,[D] What is your honest experience with reinforcement learning?,"In my personal experience, SOTA RL algorithms simply don't work. I've tried working with reinforcement learning for over 5 years. I remember when Alpha Go defeated the world famous Go player, Lee Sedol, and everybody thought RL would take the ML community by storm. Yet, outside of toy problems, I've personally never found a practical use-case of RL.

What is your experience with it? Aside from Ad recommendation systems and RLHF, are there legitimate use-cases of RL? Or, was it all hype?

**Edit**: Since my comments are being downvoted, [here is a link to my article](https://medium.com/p/228835689841) that better describes my position.

It's not that I don't understand RL. I released my [open-source code](https://github.com/austin-starks/Deep-RL-Stocks) and [wrote a paper on it](https://drive.google.com/file/d/1x67IaLpErVw9SwSBjWAdDtNEOcQSgje_/view).

It's the fact that it's EXTREMELY difficult to understand. Other deep learning algorithms like CNNs (including ResNets), RNNs (including GRUs and LSTMs), Transformers, and GANs are not hard to understand. These algorithms work and have **practical** use-cases outside of the lab.

Traditional SOTA RL algorithms like PPO, DDPG, and TD3 are just very hard. You need to do a bunch of research to even implement a toy problem. In contrast, the [decision transformer](https://drive.google.com/file/d/1x67IaLpErVw9SwSBjWAdDtNEOcQSgje_/view) is something anybody can implement, and it seems to match or surpass the SOTA. You don't need two networks battling each other. You don't have to go through hell to debug your network. It just naturally learns the best set of actions in an auto-regressive manner.

I also didn't mean to come off as arrogant or imply that RL is not worth learning. I just haven't seen any real-world, practical use-cases of it. I simply wanted to start a discussion, not claim that I know everything.

**Edit 2**: There's a shockingly number of people calling me an idiot for not fully understanding RL. You guys are **wayyy too comfortable** calling people you disagree with names. News-flash, not everybody has a PhD in ML. My undergraduate degree is in biology. I self-taught myself the high-level maths to understand ML. I'm very passionate about the field; I just have **VERY** disappointing experiences with RL.

Funny enough, there are very few people refuting my actual points. To summarize:

* Lack of real-world applications
* **Extremely** complex and inaccessible to 99% of the population
* Much harder than traditional DL algorithms like CNNs, RNNs, and GANs
* Sample inefficiency and instability
* Difficult to debug
* Better alternatives, such as the Decision Transformer

Are these not legitimate criticisms? Is the purpose of this sub not to have discussions related to Machine Learning?

To the few commenters that aren't calling me an idiot...thank you! Remember, **it costs you nothing to be nice!**

**Edit 3**: Lots of people seem to agree that RL is over-hyped. Unfortunately those comments are downvoted. To clear up some things:

* We've invested HEAVILY into reinforcement learning. All we got from this investment is a robot that can be super-human at (some) video games.
* AlphaFold **did not use any reinforcement learning.** SpaceX doesn't either.
* I concede that it can be useful for robotics, but still argue that it's use-cases outside the lab are **extremely limited.**

If you're stumbling on this thread and curious about an RL alternative, check out the [Decision Transformer](https://www.youtube.com/watch?v=-buULmf7dec). It can be used in any situation that a traditional RL algorithm can be used.

**Final Edit**: To those who contributed more recently, thank you for the thoughtful discussion! From what I learned, model-based models like Dreamer and IRIS MIGHT have a future. But everybody who has actually used model-free models like DDPG unanimously agree that they suck and don’t work.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197jp2b/d_what_is_your_honest_experience_with/,283,320,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E33178DD0>
198r1wd,Alyosha-Swaminathan,2024-01-17 07:33:15+00:00,Help Needed [D],"Hi, I'm writing a paper on extending language capabilities onto LLMs and I need some help/ppl to offer guidance or suggest edits and reviews. Please respond below if you think you can help w this.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198r1wd/help_needed_d/,0,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E330AC450>
1987em3,APaperADay,2024-01-16 16:46:57+00:00,[R] APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding,"**Paper**: [https://arxiv.org/abs/2401.06761](https://arxiv.org/abs/2401.06761)

**Abstract**:

>The massive adoption of large language models (LLMs) demands efficient  deployment strategies. However, the auto-regressive decoding process,  which is fundamental to how most LLMs generate text, poses challenges to  achieve efficient serving. In this work, we introduce a parallel  auto-regressive generation method. By instruct-tuning on general domain  data that contains hierarchical structures, we enable LLMs to  independently plan their generation process and perform auto-parallel  auto-regressive (**APAR**) generation, significantly reducing the number of  generation steps. APAR alone can achieve up to 2x speed-up, and when  combined with speculative decoding, the speed-up can reach up to 4x. In  addition, APAR reduces the key-value cache consumption and attention  computation during generation. This leads to a throughput increase of  20-70% and a latency reduce of 20-35% in high-throughput scenarios,  compared to state-of-the-art serving frameworks.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1987em3/r_apar_llms_can_do_autoparallel_autoregressive/,0,6,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E32FE6890>
197r6o0,toxfu,2024-01-16 02:12:50+00:00,[D] best advanced books of deep learning?,Almost all the books I come across are written to begin with.  Are there any where they go deeper into the topics?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197r6o0/d_best_advanced_books_of_deep_learning/,36,65,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E33111950>
198pfie,tried-another,2024-01-17 05:54:15+00:00,[P] Train AI chatbot using an Excel knowledge base.,"Helo everyone I would like to know if with Tock, i could do the following:I would like to creare a chat bot likehttps://web-chat.global.assistant.watson.cloud.ibm.com/preview.html?region=eu-de&integrationID=38c262cc-f146-4cc5-bd28-923289cd72d9&serviceInstanceID=dbfc3cd6-9c12-4780-8310-b6d77872b29bThe goal isTo be able to have an excel, with Categories/SubCategories/Answers that we will be able to convert to Json files and import to BOTWhen a user put a question with a keyword which belongs to many categories/answers, in order the bot gives the correct answer, we WANT to show only the FAQ buttons related to subcategories that will help the chat bot to give correct answer.The bot will do that alone when it will see that there are more than 1 answer.So for example if a user write ""forgot password"", the bot should put 2 buttons: 'Web Check In' and 'Already and Affiliate' ?Like that if user click:on 'Web Check In', the answer will be 'If you cannot find your log in details and you do not have the e-mail sent to you upon registering'on 'Already and Affiliate', the answer will be 'In case you lost your password for the check in, you will have to go to the following page and put your email'

Do you know if that’s possible or if there are any alternatives to Tock. I want to integrate this chat or in my website but it is for a very specific use, thus the need to be custom.

Thank you!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198pfie/p_train_ai_chatbot_using_an_excel_knowledge_base/,2,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E32ED3DD0>
19826qq,East_Dragonfruit7277,2024-01-16 12:50:53+00:00,[P] 📢 Automated RAG optimization with Fondant,"Hi everyone,

We've shared some practical insights on Retrieval Augmented Generation (RAG) with our custom data processing framework called Fondant in our latest blogpost.

Finetuning RAG is a complex task that requires a lot of time and effort. We built an example pipeline that indexes a custom knowledge base (PDF, Huggingface dataset, ...), processes the data (embedding, chunking,...) and evaluates the results.  We integrated different parameters search techniques for picking the best configuration which results in the best outcome for your RAG system.

To build the pipeline we leveraged Fondant which is an open-source framework data processing framework that simplifies the process of building data pipelines by providing reusable components. It comes  with a bunch of features that make it easy to develop and scale pipelines like local testing, built-in cloud compatibility, caching and more.

Checkout out the resources below:

📖  **Read the Blog Post** \-  [https://medium.com/fondant-blog/lets-tune-rag-pipelines-with-fondant-902f7215e540](https://medium.com/fondant-blog/lets-tune-rag-pipelines-with-fondant-902f7215e540)

🔗 **Fondant Blog on Medium** \- [https://medium.com/fondant-blog](https://medium.com/fondant-blog)

📂 **RAG GitHub Repository** \- [https://github.com/ml6team/fondant-usecase-RAG](https://github.com/ml6team/fondant-usecase-RAG)

📂 **Fondant GitHub Repository** \- [https://github.com/ml6team/fondant](https://github.com/ml6team/fondant)

Let us know what you think about it and if you have any questions or feedback, feel free to reach out to us on Discord or in the comments below.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19826qq/p_automated_rag_optimization_with_fondant/,0,6,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E3311B950>
197yl66,erap129,2024-01-16 09:05:33+00:00,[D] Question about Direct Preference Optimization (DPO) equation,"So this is the loss for (Direct Preference Optimization) DPO:

 

https://preview.redd.it/6ubjn8ekprcc1.png?width=1324&format=png&auto=webp&s=c932f5c030c2fb6b5f0f136934b047bc364d1dcc

I don't understand the division by pi\\\_ref (both for y\\\_w and for y\\\_l). I know the purpose is that the finetuned model won't stray too far away from the reference model, but Just looking at it mathematically - why should pi\\\_ref(y\\\_w|x) be close to pi\\\_theta(y\\\_w|x)?

At least for y\\\_w it seems like the loss would benefit from pi\\\_ref(y\\\_w|x) being as close as possible to 0 because we want to maximize the left part of the equation.

What am I missing?

Thanks.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197yl66/d_question_about_direct_preference_optimization/,1,10,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E330C4190>
197z747,ActuaV,2024-01-16 09:48:24+00:00,[D] Dynamically choosing RNN recurrences," Hi,

I am working on a research project that could benefit from some ML-based modeling. I'm wondering if anyone is aware of research on LSTM (or other RNN) models in which the number of repetitions is dynamically determined during the model's execution. For example, a neural network cell that outputs a class, a criterion deciding whether the network continues running (+ a penalty for a higher number of cell iterations).

I've tried searching for this without success. Any pointers toward keywords or studies would be much appreciated.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197z747/d_dynamically_choosing_rnn_recurrences/,6,7,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E330A7DD0>
197y6lk,rita_moura,2024-01-16 08:37:26+00:00,[D] Is this a time series problem? Or is there another approach?,"Hello,

I am trying to implement a machine learning problem coupled with finite element simulations.

I have a set of simulations (\~5000), each simulation has multiple time steps (\~20), and for each time step I want to predict the coordinates of \~50 nodes. I use each node as an observation, so it would be a multi-output regression problem where the goal is to predict the x, y, and z final coordinates for each node. I am organizing the dataset by node, so each node belongs to a specific time step and a specific simulation.

Here's an example of 5 observations from the dataset and the corresponding features (which are not relevant to the discussion):

https://preview.redd.it/4hjdyi3wjrcc1.png?width=1093&format=png&auto=webp&s=df4ba944856d9e04fd76b12adf691fca77a692e6

I was thinking about using LSTM and multi-time series, but since I am working with small time series of simulations that are not related to each other, I am not quite sure how to implement it. I was thinking of it as a time series problem, but I realized that I can't use a classical forecasting approach. I only have the information at t = 0 and with that I want to predict the whole series, so I don't have any past observations to use to predict future ones.

What would be the best model/approach to use in this case?

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197y6lk/d_is_this_a_time_series_problem_or_is_there/,16,8,0.79,<praw.models.comment_forest.CommentForest object at 0x0000026E33183550>
198v7zv,No-Sun-5534,2024-01-17 12:16:23+00:00,[D] How to frame a fair policy for spam submissions in ICLR2025?,"So, I got the following MetaReview for my ICLR2024 submission.

""Reviewers are in consensus that the submission does not make a clear contribution and that the engagement from the author(s) is disingenuous. The conference should consider adopting a policy for spam submissions next year.""

I am well aware that getting high quality reviewers for machine learning conferences is a challenge. But I expected that at least a meta reviewer would be able to play a decent role. This is a serious issue to ponder - especially when the first review I got was constructive and pointed out my strengths: ""The prose is clear, easy to follow, and enjoyable to read. The authors provide helpful figures, use sections and subsections well.""

My take on this is that every reviewer should first filter their comments to a tool like Bing Chat (see image for my case). And that labelling ICLR submissions from professionals as a ""spam"" submission sets a dangerous precedent!

https://preview.redd.it/pkwtz7niszcc1.png?width=1181&format=png&auto=webp&s=cc7225bf402e838d376876878749794b9087bf13

**Edit**: I am happy to read a more specific review in one of the comments here, this is exactly what I expected my third and fourth ICLR reviewers to be like.  


P.S. My rebuttal to a late reviewer asking about the key contribution in my work (which you Redditors may know from a previous post).

"" What is the key contribution in this work?

* Supplementary file has the first ever dataset for linear unmixing at [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets) if accepted
* I claim this is the first mathematically tractable framework for cognitive development because of the following sub-contributions -> Figure 2. introduces an adaption of Piaget's stage theory of cognitive development for chemical mixture understanding -> Section 2.1 contains a mathematical proof for the empirical finding (plotted in Figure 4) of eigenvalue cutoff being more pronounced for the rescaled sample covariance matrix -> Section 2.2 shows that ICA and NMF fail for the simple linear unmixing dataset (and I am willing to test it again any other state-of-the-art algorithm proposed and accepted in ICLR2024) -> Section 2.3 now contains an error-scaling law as requested by Reviewer gSc5

What are the baselines? In Section 2.2 the ideal result is the PseudoInverse solution. The baseline method is PCA. ICA and NMF needs new theory development to beat the baseline, and attain the ideal result. Thus, I expect the dataset provided in this paper to be a simple yet profoundly impactful challenge for the learning theory community.

How does the paper improve upon them? This paper is the first to define the problem. My bet is that many of the algorithms proposed in ICLR 2024 for variations of NMF would still fail against this dataset. Hence, it is important that this paper is accepted so learning theorists, even with low-compute resources, have a principled dataset to play with and innovate.""",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198v7zv/d_how_to_frame_a_fair_policy_for_spam_submissions/,15,0,0.23,<praw.models.comment_forest.CommentForest object at 0x0000026E33199550>
197glkm,_donau_,2024-01-15 18:56:32+00:00,Dimensionality reduction for NLP applications being forgotten..? [D],"Ok so when I was doing my thesis a few years back, I wrote a whole section on why dimensionality reduction (DR) was important, and I remember arguing for it by illustrating something along the lines of the concept of distance losing significance in high dimensionality. I also remember that some of the methods I used for clustering worked better in lower dimensions, and so, DR was not just for visualization purposes (which I feel is what I see it used for most often), but a necessary step in modeling, analysis, etc. 
In these days though, I read a lot of articles about people using LLMs and feeding embeddings of extremely high dimensionality into models, clustering methods and so on, without even mentioning DR. What's up with that? I remember (albeit vaguely) doing some testing on how dimensionality affects common distance metrics, and basically, cosine similarity for instance stopped making sense at a lot fewer dimensions than what BERT, Mistral, openAI or anything else outputs.  Have I misunderstood something here? Is DR really not that important, do people underestimate how valuable it is, don't they care, don't they know...? Thanks in advance everyone, hope someone here can shed some light on this for me :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197glkm/dimensionality_reduction_for_nlp_applications/,23,56,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E331A6B50>
19867yt,animatedclaire,2024-01-16 15:58:41+00:00,[D] Feature matrix comparing different Vector Databases,"Hey everyone,

Sharing an open-source resource we’ve been developing for anyone interested in working with vector embeddings but unsure which vector database will meet their needs.

* **What is it?** A table comparing the features available with different vector databases. The project is open-source and developed by a community of practitioners with points of contact from the different vector databases. 
* **Who is it for?** Developers, data scientists and ML engineers looking to work with vector embeddings
* **Why is it relevant?** If you’re working with vector embeddings, you will use a vector database to store and search them. Finding the right one for your use case can be tough, this table gives you an overview of the options and the features they support

**Link:** 

[**https://vdbs.superlinked.com/?utm\_source=reddit&utm\_medium=social&utm\_campaign=vdb\_launch**](https://vdbs.superlinked.com/?utm_source=reddit&utm_medium=social&utm_campaign=vdb_launch)

Let us know what you think and how we can make this more useful",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19867yt/d_feature_matrix_comparing_different_vector/,0,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E3319C850>
198615b,EfficientEffort7029,2024-01-16 15:50:31+00:00,"[P] Feedback appreciated for AI music generation, voice cloning platform","I am big fan of this reddit sub since since so many ideas appears related to the AI universe.  

It would be nice to get some honest feedback for my latest development. I added an ai music generation and also included a voice cloning.   
Limited generations could be done for free but you can contact me for more credits for a testing.

[https://www.aimastercraft.com/Audio/Generate](https://www.aimastercraft.com/Audio/Generate)

&#x200B;

&#x200B;

&#x200B;

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/198615b/p_feedback_appreciated_for_ai_music_generation/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E32E515D0>
1985lv6,MrWannabePBandJ,2024-01-16 15:32:48+00:00,Embedded AI Project [P],"Hello Everyone , I am currently working on a project involving comparing STM32CUBE AI and tflm , I am using STM32F746. It is a discovery board. I am very new to the field of Embedded , so can anyone suggest me some basic examples that I can run on this board  using both platforms ( for tflm I can use Mbed) so that I can get an understanding of the platforms and then I can implement others myself.

Thanks in advance.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1985lv6/embedded_ai_project_p/,3,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E33195910>
1988yxi,internetcookiez,2024-01-16 17:48:47+00:00,[P] Is there any way to create long form content using LLMs?,"For context, I’m trying to ingest a bunch of documents (could be lecture notes, a book, anything) and generate a 30 minute long transcript that explains the topics in detail using python.

Is there any approach like this? Currently openai has token limits and i’m not sure how to go about this using vector db. Assuming the content to be used is on a vector DB, how can we get an LLM to generate a long form transcript?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1988yxi/p_is_there_any_way_to_create_long_form_content/,4,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E331CA4D0>
197ci39,GradientSurfer,2024-01-15 16:15:32+00:00,[P] Draw2Img: draw on canvas to instantly create amazing graphics & images,"This is an open source web UI for interactive text-guided image to image generation via SDXL-Turbo, the backend is a multi-threaded HTTP + Websocket server written in Python.

You might be interested in this project if:

- you or friends/family/children are interested in learning the basics of generative art, but don't have the time/patience/skills for a1111/comfy/etc

- you have little to no artistic skill (or maybe a lot!), and simply want to create good looking custom graphics for your website or project, with minimal effort and time

- you want to quickly & creatively iterate on 512x512 base images as the first step of a more advanced workflow (eg upscaling, diffusion, etc)

GitHub link: [https://github.com/GradientSurfer/Draw2Img](https://github.com/GradientSurfer/Draw2Img)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197ci39/p_draw2img_draw_on_canvas_to_instantly_create/,2,25,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E3319FFD0>
197euq9,karun_kodes,2024-01-15 17:49:06+00:00,[D] Relative positional embedding and what's the advantage over absolute positional encoding,"So I was just reading about absolute positional encoding and then about relative positional embedding.

All I could understand is how's it done with relative to each word. But I really couldn't think of the advantages over absolute one as the ""Attention is all you need"" also states that """"We chose this function because we hypothesized it would allow the model to easily learn to attend by *relative positions.."" So what's the advantage relative positional encoding carries ?.*  


And can someone also explain the below points:

1. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. (How ?)
2. Using absolute positional information necessarily means that there is a limit to the number of tokens a model can process",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197euq9/d_relative_positional_embedding_and_whats_the/,7,14,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E331C8690>
19785i7,TutubanaS,2024-01-15 12:59:40+00:00,[P] Reducing 2048 dimensions to 2000 dimensions for PGVector," Hi people of ml,

I am working on a project that uses PGVector for efficient similarity search, and I use feature vectors I obtain from EfficientNet B5 which outputs 2048d. The issue is that I need to index my tables based on the vectors, otherwise, your typical DB hardware problems occur (Not enough RAM). However, the methods PGVector offers have a limit, the vectors can be at most 2000d. One solution I have found is PCA, but I have quite a lot of data so before I test it, I want to get some comments and suggestions. Anyone here has tried PCA for dimensionality reduction for similarity search purposes, mainly for L2 and cosine, and if so, how did it result?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19785i7/p_reducing_2048_dimensions_to_2000_dimensions_for/,29,30,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E306C67D0>
19792fl,sushilkhadakaanon,2024-01-15 13:45:19+00:00,[D] Latent distributions of Diffusion model,"Are the latent distributions in the Diffusion model, considered Gaussians? If yes, why? If not, why would they consider it Gaussian while calculating KL divergence in closed form?  


Update: Here's the paper snippet where they claim the latents to be Gaussian.

https://preview.redd.it/j9qv04qbumcc1.png?width=2428&format=png&auto=webp&s=80148d371fdc46ba5890860d8e5bc60afca15f90

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19792fl/d_latent_distributions_of_diffusion_model/,28,26,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E33215A90>
197h7em,redv,2024-01-15 19:19:54+00:00,[D] Training transforms on embeddings rather than tokens.,"What work has been done on training LLM's to predict a sequence of embedding vectors rather than tokens? For example an embedding would be created for each sentence (or phrase) in a text, and then the LLM trained on this.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197h7em/d_training_transforms_on_embeddings_rather_than/,4,7,0.64,<praw.models.comment_forest.CommentForest object at 0x0000026E306D5C10>
197mqpg,Significant_Chip_269,2024-01-15 22:55:45+00:00,[D] What is the reason for dividing weight decay values by the learning rate during hyperparameter search?,"Hello,

I have now seen at least in two high-level machine learning papers that people divide weight decay by learning rate when they do hyperparameter search. I am curious what is the idea behind it?

My only thought is that this could be done is because the weight decay term is multiplied by the learning rate during a gradient optimization step (Equation 1), so by dividing the weight decay values we can make sure that the weight decay term in the optimization step will not depend on our learning rate.

[Equation 1 ](https://preview.redd.it/euq3ncrwpocc1.png?width=690&format=png&auto=webp&s=63db620f5d1dbba8360dc713e4e9cc3738dadc80)

Here are the papers:

1. [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)
2. [Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors](https://arxiv.org/abs/2205.10279)

[Paper1](https://preview.redd.it/auorq8gloocc1.png?width=696&format=png&auto=webp&s=b89077860ee0cb43198638d94d26c62c5ea84c93)

&#x200B;

[Paper2](https://preview.redd.it/025rsd4noocc1.png?width=568&format=png&auto=webp&s=184ea79e4cd41848104df9917a5f9f4a71185983)

Thank you.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197mqpg/d_what_is_the_reason_for_dividing_weight_decay/,2,3,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E306C5690>
1974yoo,APaperADay,2024-01-15 09:41:36+00:00,"[R] ""Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc"" (2023) - Doug Lenat's final paper before his passing","**Paper**: [https://arxiv.org/abs/2308.04445](https://arxiv.org/abs/2308.04445)

**Blog post**: [https://garymarcus.substack.com/p/doug-lenat-1950-2023](https://garymarcus.substack.com/p/doug-lenat-1950-2023)

**Related Doug Lenat talks**:

2022: [https://www.youtube.com/watch?v=VjkbmLjwXO8](https://www.youtube.com/watch?v=VjkbmLjwXO8)

2019: [https://www.youtube.com/watch?v=v2rK40bNrrY](https://www.youtube.com/watch?v=v2rK40bNrrY)

**Abstract**:

>Generative AI, the most popular current approach to AI, consists of  large language models (LLMs) that are trained to produce outputs that  are plausible, but not necessarily correct. Although their abilities are  often uncanny, they are lacking in aspects of reasoning, leading LLMs  to be less than completely trustworthy. Furthermore, their results tend  to be both unpredictable and uninterpretable.   
We lay out 16 desiderata for future AI, and discuss an alternative  approach to AI which could theoretically address many of the limitations  associated with current approaches: AI educated with curated pieces of  explicit knowledge and rules of thumb, enabling an inference engine to  automatically deduce the logical entailments of all that knowledge. Even  long arguments produced this way can be both trustworthy and  interpretable, since the full step-by-step line of reasoning is always  available, and for each step the provenance of the knowledge used can be  documented and audited. There is however a catch: if the logical  language is expressive enough to fully represent the meaning of anything  we can say in English, then the inference engine runs much too slowly.  That's why symbolic AI systems typically settle for some fast but much  less expressive logic, such as knowledge graphs. We describe how one AI  system, Cyc, has developed ways to overcome that tradeoff and is able to  reason in higher order logic in real time.   
We suggest that any trustworthy general AI will need to hybridize  the approaches, the LLM approach and more formal approach, and lay out a  path to realizing that dream.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1974yoo/r_getting_from_generative_ai_to_trustworthy_ai/,16,45,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E33221F10>
197ogx3,Perfect_Natural_2540,2024-01-16 00:08:57+00:00,[p] DeepTuner,"I’m working on creating a guitar tuner that will be able to pick up guitar notes and identify their frequency in noisy environments. My aim is to add background noise to audio of open guitar strings. Initially i will add white noise, then later try training with other instruments and people speaking. The model will then reconstruct the audio of an isolated guitar.

My architecture currently involves an autoencoder, but I’ve been trying to find newer papers on audio models that can isolate specific audio(individual speakers, instrument identifiers).

I’m looking for research paper recommendations as well as musical datasets. (Nsynth is mostly garbage for guitar)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197ogx3/p_deeptuner/,3,2,0.58,<praw.models.comment_forest.CommentForest object at 0x0000026E3320E210>
196uyub,deschaussures147,2024-01-15 00:25:16+00:00,[D] ICLR 2024 decisions are coming out today,"We will know the results very soon in upcoming hours. Feel free to advertise your accepted and rant about your rejected ones.

Edit 2: AM in Europe right now and still no news. Technically the AOE timezone is not crossing Jan 16th yet so in PCs we trust guys (although I somewhat agreed that they have a full month to do all the finalization so things should move more efficiently).

Edit 3: The thread becomes a snooze fest! Decision deadline is officially over yet no results are released, sorry for the ""coming out today"" title guys!

Edit 4 (1.48pm CET): metareviews are out, check your openreview !

Final Edit: now I hope the original purpose of this thread can be fulfilled. Post your acceptance/rejection stories here!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196uyub/d_iclr_2024_decisions_are_coming_out_today/,420,164,0.98,<praw.models.comment_forest.CommentForest object at 0x0000026E3432C190>
1975kvn,OraclePred,2024-01-15 10:23:06+00:00,[D] EACL 2024 Decisions,Decisions for those who committed to EACL 2024 are coming out today (15 Jan 2024)! What are your expectations? ,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1975kvn/d_eacl_2024_decisions/,26,26,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E33274A90>
197drh9,Zawadscki,2024-01-15 17:05:52+00:00,"[D] The Hard Truth about Artificial Intelligence in Healthcare: Clinical Effectiveness is Everything, not Flashy Tech","Hi all, I think the following [blog post I wrote](https://open.substack.com/pub/mlinhealthcare/p/the-hard-truth-about-artificial-intelligence?r=7bxky&utm_campaign=post&utm_medium=web) may be helpful to a lot of people in this sub who work in the healthcare domain! Here's a quick blurb about the article:

AI in healthcare faces a critical issue: our obsession with cutting-edge technology often overshadows the actual impact on patients. Successfully bringing AI medical devices to market entails much more than excellent diagnostic performance; it requires rigorous clinical trials and comprehensive cost-effectiveness analyses. HeartFlow's AI-powered cardiac imaging product FFRCT is a perfect example of that. In this blog post, I critically review FFRCT and discuss broad lessons for the future of AI medical devices.

If you're interested in evidence-based medicine, AI/ML, health economics, and envisioning the future of healthcare, this blog post is for you. What do you think the biggest barrier for AI in healthcare is? Let me know in the comments!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197drh9/d_the_hard_truth_about_artificial_intelligence_in/,9,7,0.59,<praw.models.comment_forest.CommentForest object at 0x0000026E3322DD50>
197eoy1,MephistoPort,2024-01-15 17:42:34+00:00,[D]Sources and Resources to keep oneself up to date,"Hey everyone.

I will be soon starting an year long internship at a top University in machine learning. My experience is in computer vision, but I will be working in neuromorphic computing and spiking neural networks. 

In this one year I would like to build a huge portfolio of impressive projects in the field of computer vision and expand my knowledge into fields like NLP, RL, GNNs, etc. The projects can range from simple deployment to paper implementation. 

I would also like to keep myself up to date with latest stuff happening in machine learning. This is such a rapidly changing field and I would like to get a list of people, blogs, creators or anything you guys follow, use to keep up to date with. It will be much better if that source is using the simplest language as I don't have computer science background. Medium articles, GitHub devs, YouTube creators, popular blogs, anything. Some people I follow are two minute papers, Yannic kilcher, etc. Also if you work in the field of spike neural networks or know the field, drop some resources for that also.

TL;DR: Resources to learn and keeping upto date with the latest in machine learning.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197eoy1/dsources_and_resources_to_keep_oneself_up_to_date/,9,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E332301D0>
197f416,30299578815310,2024-01-15 17:59:33+00:00,"[D] Code vs JSON output for LLM agents? Frameworks like LangChain rely on LLMs responding with JSON syntax, while agents like Octopus, CaP, and Voyager directly control agents via code.","[CaP](https://arxiv.org/pdf/2209.07753.pdf), [Voyager](https://arxiv.org/pdf/2305.16291.pdf), [Octopus](https://arxiv.org/abs/2310.08588)

I work primarily with JSON based agents but code-as-policy agents seem to be extremely powerful. Here are some of the benefits and weaknesses I've seen

Pros of code

1. Less tool creation needed - The prebuilt math/file/string/list manipulation abilities that come with code are enormous. In a JSON based agent, you would have to formally declare each of these as a tool which you expose to the LLM and explain in your prompting, which is a lot of work and eats up a ton of the context window. 
2. Reduced number of transactions - The LLM can write scripts that invoke multiple tools and manipulate their results in ways that are difficult to do in a single transaction via JSON. For example, in one script, the model could search a DB 3 times, perform regex on the query results, convert them to integers, and add them up. Doing this in one step via JSON tool invocations is basically impossible. 
3. Less syntax errors - this might be totally just vibe-based reasoning, but it really seems like LLMs have an easier time writing valid python than valid JSON, especially when you have lots of nested arguments in your methods.

Cons

1. Crazy risky - This is the obvious one. You have a machine executing random code. There are ways to mitigate this but still. I mean seriously we all learned not to use eval, so it is crazy to basically see research tending towards just running eval on the outputs of these models. 
2. Scripts with errors - Sometimes the model tries to get too fancy and writes complex programs that have bugs, resulting in many needed retries. 

Do any of you have thoughts or experience with these approaches in the wild? 

Is anybody aware of any experiments that compare these two approaches against each other? 

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197f416/d_code_vs_json_output_for_llm_agents_frameworks/,5,5,0.73,<praw.models.comment_forest.CommentForest object at 0x0000026E34254550>
197fw4f,Nano_9a9o,2024-01-15 18:29:40+00:00,"NLI sentence transformers VS general purpose ones, for RAG applications [Discussion]","I've been looking into RAG, and have come across using sentence transformers for querying and semantic comparison.

Recently, I've discovered that NLI models are specifically designed for matching up queries to answers, which seems super useful, and yet all the ones on the sentence-transformers hugging face are like 2 years old, which is practically centuries ago in AI time, as opposed to the ""all"" models, which see much more focused on semantic similarity comparison.

Am I missing something here? Surely people aren't using years old models for modern RAG applications, right?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197fw4f/nli_sentence_transformers_vs_general_purpose_ones/,1,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E3322FED0>
1971c6h,APaperADay,2024-01-15 05:46:49+00:00,[R] COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training,"**Paper**: [https://arxiv.org/abs/2401.00849](https://arxiv.org/abs/2401.00849)

**Code**: [https://github.com/showlab/cosmo](https://github.com/showlab/cosmo)

**Models**: [https://huggingface.co/Awiny](https://huggingface.co/Awiny)

**Dataset**: [https://huggingface.co/datasets/Awiny/Howto-Interlink7M](https://huggingface.co/datasets/Awiny/Howto-Interlink7M)

**Project page**: [https://fingerrec.github.io/cosmo/](https://fingerrec.github.io/cosmo/)

**Abstract**:

>In the evolution of Vision-Language Pre-training, shifting from  short-text comprehension to encompassing extended textual contexts is  pivotal. Recent autoregressive vision-language models like \[Flamingo, PaLM-E\], leveraging the long-context capability of Large  Language Models, have excelled in few-shot text generation tasks but  face challenges in alignment tasks. Addressing this gap, we introduce  the contrastive loss into text generation models, presenting the  COntrastive-Streamlined MultimOdal framework (**CosMo**), strategically  partitioning the language model into dedicated unimodal text processing  and adept multimodal data handling components. CosMo, our unified  framework, merges unimodal and multimodal elements, enhancing model  performance for tasks involving textual and visual data while notably  reducing learnable parameters. However, these models demand extensive  long-text datasets, yet the availability of high-quality long-text video  datasets remains limited. To bridge this gap, this work introduces **Howto-Interlink7M**, an inaugural interleaved video-text dataset featuring  comprehensive captions, marking a significant step forward.  Demonstrating its impact, we illustrate how Howto-Interlink7M enhances  model performance in image-text tasks. With 34% learnable parameters  and utilizing 72% of the available data, our model demonstrates  significant superiority over OpenFlamingo. For  instance, in the 4-shot flickr captioning task, performance notably  improves from 57.2% to 65.1%. The contributions of CosMo and Howto-Interlink7M are underscored by notable performance gains across  14 diverse downstream datasets encompassing both image-text and  video-text tasks.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1971c6h/r_cosmo_contrastive_streamlined_multimodal_model/,1,6,0.81,<praw.models.comment_forest.CommentForest object at 0x0000026E33235190>
196ryle,franticpizzaeater,2024-01-14 22:16:42+00:00,[P] Trying to calculate semantic difference of words.,"I am working on a project, I have a list of target words, and need to calculate the difference of meaning between these words and new words. It is mostly a word similarity task.  


Say, the target word is ""car"", the model need to output high value for ""dog"" and low value for ""Jeep""; or vice versa. Currently I am using huggingface's sentence transformer library for this, and (1-Cosine Similarity) as difference score. But the performance is not up to the expectations. Is there any way to improve the performance? Should I use some other library/model/metrics?  


Thank you.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196ryle/p_trying_to_calculate_semantic_difference_of_words/,12,24,0.85,<praw.models.comment_forest.CommentForest object at 0x0000026E34289110>
1973l79,gxcells,2024-01-15 08:07:31+00:00,[D] LORAs for GANs,"Hi the sub.
I want to train some GAN models (like pix2pix) but it seems that it is really difficult to train a GAN with good quality.

Is it possible to train a LORA for GANs?

Edit: just found that new paper
E2GAN: Efficient Training of Efficient GANs for Image-to-Image Translation

https://arxiv.org/abs/2401.06127",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1973l79/d_loras_for_gans/,1,2,0.63,<praw.models.comment_forest.CommentForest object at 0x0000026E34299790>
197q4k8,ManInBlue7777,2024-01-16 01:23:59+00:00,ML project ideas [p],"Hi all, We urgently require a SUBSTANTIAL dataset for our school project. Unfortunately, we have exhausted our options, and the upcoming presentation is just around the corner. We are feeling anxious as we haven't finalized our project topic yet, unlike other groups who have already chosen their topics and are actively working on them. Any assistance with obtaining a LIVE dataset would be greatly appreciated.

Thank you.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197q4k8/ml_project_ideas_p/,0,0,0.08,<praw.models.comment_forest.CommentForest object at 0x0000026E3429A810>
197gdzw,ibbi1020,2024-01-15 18:48:31+00:00,[D] 365datascience Reviews?,"Hey everyone, I'm a freshman making my way to becoming an AI/ML engineer. I'm seeing alot of ads from 365datascience for their subscription sale event (ending in 2 days). I took a course when they went completely free back in decemeber (i think) and I liked it, learned alot of LR in python.

I'm thinking of buying their subscription plan while its on sale, but the issue is I'm from a third-world country and the economy is a travesty so I want to make sure if their courses are actually good or was my experience a one-off thing, I dont want to waste good money on a investment thats a waste of time (even 60 bucks a year is alot for where I'm from, unfortunatly). Drop your reviews of 365datascience if you've taken their courses. How good are they compared to free resources on the internet?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/197gdzw/d_365datascience_reviews/,0,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E3426AD50>
196wqsl,amirkasraaa,2024-01-15 01:50:07+00:00,[D] Self-implementing Rabbit tech's LAM,"I saw the hype about Rabbit R1 and humane ai. I only found a research paper for Rabbit ai ([https://www.rabbit.tech/research](https://www.rabbit.tech/research)). Unfortunately the ""research paper"" wasn't detailed to give me some insight into how I can implement an AI like this. Do you guys have any idea how somebody can effectively self train an ""LAM""? And wouldn't it be easier If you just made a launcher with this A.I that would interact with native apps directly to cut off the need of digital processing?

I'm really sorry for this unorganized post 🙏",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196wqsl/d_selfimplementing_rabbit_techs_lam/,11,7,0.65,<praw.models.comment_forest.CommentForest object at 0x0000026E342CDA50>
19776ck,BigDreamx,2024-01-15 12:03:28+00:00,[D] Workshops,"1. Is submitting to multiple workshops at the same conference allowed? What about difference conferences? I have a few that align well with my paper, but his is not mentioned elsewhere.

2. Also, is it good practice as an undergrad to submit to workshops to get reviews and extend my work given that I don't have an advisor?

3. If I don't think my work can be extended to a conference paper after my acceptance to a workshop, should I just stop or continue?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19776ck/d_workshops/,5,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E34297C90>
196f4z9,Pritish-Mishra,2024-01-14 12:46:53+00:00,"I controlled Super Mario with ""Activity Recognition"" using just my smartphone! [P]","Recently, I worked on a project involving activity recognition, **which is the process of identifying and understanding human activities based on data collected from sensors**. The only thing I had was a single old smartphone, as I had no money to invest in additional sensors.

My ultimate goal was to control Super Mario inside the game using my real-world movements. After conducting some research, I discovered that most smartphones are equipped with an accelerometer sensor, which I could leverage to train a machine learning model for activity recognition. Fortunately, my old smartphone had one. I then developed an app capable of streaming real-time sensor data from my smartphone to my laptop wirelessly (I named this app ""SensorFlow"").

Using this data, I built and trained a machine learning model that could accurately detect my actions with a remarkable 95% accuracy. In the end, I integrated this model with Super Mario, using python to programmatically hit the arrow keys based on my real-world movements. I ended up with a system where I can play Super Mario just by using my body! It is not a 100% but it works well enough. Additional suggestions are welcome.

I have open-sourced all the code related to activity recognition and the Android app I developed in the process.

For more information on this project, you can check out my YouTube. This is a self-promotion, but it has additional information on the project. You can see the final result below 👇

[https://www.youtube.com/watch?v=IpLV6uKAO98](https://www.youtube.com/watch?v=IpLV6uKAO98)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196f4z9/i_controlled_super_mario_with_activity/,32,65,0.87,<praw.models.comment_forest.CommentForest object at 0x0000026E3435AF10>
196sva4,vocdex,2024-01-14 22:54:40+00:00,[D] [R] Causality and model-based RL: possible connection?,"Hey everyone! I've been diving into the world of model-based Reinforcement Learning (RL) and its relationship with causal inference, and I find myself intrigued yet slightly puzzled.(Please let me know if my understanding makes sense at all)

On the one hand, model-based RL, with its focus on learning the dynamics of an environment, seems to naturally lend itself to answering ""what if"" questions. The ability to predict the outcomes of actions without actual real-world trials feels very much like causal inference. But then, does this mean model-based RL is inherently capable of full-blown causal inference?

My understanding is that causal inference not only involves predicting outcomes (interventions) but also delving into counterfactual reasoning - understanding what would have happened under different past actions. I'm wondering how well model-based RL handles this aspect, given its dependency on the accuracy and completeness of the learned model.

I'm curious about the community's thoughts on this:

1. Are there any limitations to the kind of causal questions that a model-based RL system can answer?
2. How might integrating explicit causal models into model-based RL frameworks enhance their capabilities?

Would love to hear your insights or any relevant research that could shed light on this intersection!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196sva4/d_r_causality_and_modelbased_rl_possible/,2,8,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E342A0290>
1971j09,AIsavvy,2024-01-15 05:57:47+00:00,"[R] How to calculate the score of a new datapoint by a score based diffusion model(song & ermon, 2019)?","I have a pretrained score based diffusion model trained on 64X64 images. Now I want to calculate the score of a new image(of same dimension) through this pre-trained neural network.

The score network takes two inputs :

1. x\_t : Sample at timestamp t
2. t : timestamp

How should I calculate the score of a new image via this pre-trained neural network ?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1971j09/r_how_to_calculate_the_score_of_a_new_datapoint/,2,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E3435BE10>
196uam1,Im_The_Tall_Guy,2024-01-14 23:55:36+00:00,[D] How should I go about implementing a custom quantization method?,"
Hey everyone! I’ve been doing research on quantizing llms and I have a couple of custom methods that I’d like to test out. Looking at existing implementations like Tim Dettmers’ bitsandbytes makes me feel as lost as ever. Looking at llama.cpp source hasn’t helped much either. Has anyone had experience with implementing and more importantly evaluating a custom quantization method? Please share any thoughts and if you have any questions please feel free to ask. Thnaks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196uam1/d_how_should_i_go_about_implementing_a_custom/,2,6,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E331B8B10>
196tumn,AdFew4357,2024-01-14 23:36:11+00:00,What kinds of departments research modern experimental design? [D],,MachineLearning,/r/statistics/comments/196kq1y/what_kinds_of_departments_research_modern/,3,4,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E330996D0>
196fnf3,kekkimo,2024-01-14 13:15:36+00:00,[D] What happens when we generate tokens beyond the training context length of LLMs?,Let's say for example an LLM was trained on 2048 tokens and we generate texts beyond 2048 tokens. What's the issue and why?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196fnf3/d_what_happens_when_we_generate_tokens_beyond_the/,19,25,0.87,<praw.models.comment_forest.CommentForest object at 0x0000026E3436EA10>
196r0ya,ade17_in,2024-01-14 21:38:51+00:00,[P] Projects on Diffusion models,"I'll be applying to thesis position and topic I'm looking forward to is something related to diffusion models (or maybe ViT). (do suggest any other topics which you feel have potential) 

I'm done with theoritcal part concerning diffusion models and building on from scratch on a MNIST, CIFAR. I watched many tutorials and participated paper discussions as well. But still don't feel much confident in it and looking forward working on a extensive project which might further improve my understanding and also my CV. 

Any project idea recommedation? Is medical image synthesis or making scenes for autonomous driving a good way to start? 

Also it will be helpful if I get github repos, links, blog, videos which you think might be helpful! 

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196r0ya/p_projects_on_diffusion_models/,2,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34363F90>
196lpy8,johnyeocx,2024-01-14 17:55:21+00:00,[P] Trying to make cloud infrastructure as simple as possible for ML engs. What do you think?,"Hey guys! Over the past few months, I talked to several ML engineers (mostly startup founders) and realised that one thing all of them disliked was setting up & managing cloud infrastructure for their backend or ML model. Although there are services like Render, for some reason, they all opted to do it hardcore with AWS / Azure / GCP. Not sure why, but anyways, these services do have a lot of overhead which make them tedious or difficult, especially for first timers.

So, I decided that I wanted to build something which makes it really easy to deploy your ML services to cloud providers like AWS, whether it’s an inference server, REST API, or some job queue, so that ML engs can focus on other more interesting things.

Right now I’ve built a really simple (maybe useless, hopefully not) first version, [www.eliseapp.com](http://www.eliseapp.com/), which helps you deploy a FastAPI app to AWS app runner (on your own account) in one click. I’d love to get feedback on it, but more so, what problems you’ve personally encountered when trying to deploy your ML application and what services you’d expect on such a platform! Thanks :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196lpy8/p_trying_to_make_cloud_infrastructure_as_simple/,0,6,0.71,<praw.models.comment_forest.CommentForest object at 0x0000026E343619D0>
196ecrb,FSoft_AIC,2024-01-14 12:00:12+00:00,[Research] RepoPilot: Multi-Agent Coding Assistant that Can Understand and Generate Code at Repository Level,"We release **RepoPilot**, a multi-agent system that can understand and interact with the whole code repository.

RepoPilot is a one-stop Python library that revolutionizes the way developers interact with and understand their codebases. Utilizing advanced Large Language Models (LLMs), RepoPilot acts as a multi-agent system, offering a next-generation **coding assistant** for comprehensive codebase exploration and impact analysis. Designed for developers who seek deeper insights into their projects, RepoPilot simplifies complex code analysis tasks, making it an indispensable tool for modern software development.

Unlike other coding assistants, such as Github Copilot, Tabnine etc, or a single CodeLLMs, RepoPilot is engineered to grasp the full context of your entire codebase, enabling a more comprehensive analysis and more accurate recommendations.

More Information can be found here: [https://github.com/FSoft-AI4Code/RepoPilot](https://github.com/FSoft-AI4Code/RepoPilot)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196ecrb/research_repopilot_multiagent_coding_assistant/,3,15,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E3434C890>
195q6lu,Successful-Western27,2024-01-13 15:16:47+00:00,[R] Google DeepMind Diagnostic LLM Exceeds Human Doctor Top-10 Accuracy (59% vs 34%),"Researchers from Google and DeepMind have developed and evaluated an LLM fine-tuned specifically for clinical diagnostic reasoning. In a new study, they rigorously tested the LLM's aptitude for generating differential diagnoses and aiding physicians.

They assessed the LLM on 302 real-world case reports from the New England Journal of Medicine. These case reports are known to be highly complex diagnostic challenges.

The LLM produced differential diagnosis lists that included the final confirmed diagnosis in the top 10 possibilities in 177 out of 302 cases, a top-10 accuracy of 59%. **This significantly exceeded the performance of experienced physicians, who had a top-10 accuracy of just 34% on the same cases when unassisted.**

According to assessments from senior specialists, the LLM's differential diagnoses were also rated to be **substantially more appropriate and comprehensive** than those produced by physicians, when evaluated across all 302 case reports.

This research demonstrates the potential for LLMs to enhance physicians' clinical reasoning abilities for complex cases. However, the authors emphasize that further rigorous real-world testing is essential before clinical deployment. Issues around model safety, fairness, and robustness must also be addressed.

[**Full summary**](https://aimodels.substack.com/p/googles-new-llm-doctor-is-right-way). [**Paper**](https://arxiv.org/abs/2401.05654).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195q6lu/r_google_deepmind_diagnostic_llm_exceeds_human/,130,542,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E343C8210>
1974bt1,Hewwo-Is-me-again,2024-01-15 08:57:42+00:00,Figure out networks best possible performance on dataset before training [D],"I have an issue where I don't have access to a lot of hardware, meaning  that training models takes forever, like at least a few days to be able  to see whether an image generation model will turn out the way I want or  not. So are there techniques to figure out roughly how a model will  preform before training or figure out the optimal model to get the  result I want?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1974bt1/figure_out_networks_best_possible_performance_on/,10,0,0.31,<praw.models.comment_forest.CommentForest object at 0x0000026E34384E10>
196np4b,tankuppp,2024-01-14 19:19:36+00:00,[D] Textbook on applied forecasting with R with exercises and solutions,"Hi,

I'm student taking a course in ""applied forecasting"". One of the challenging is identifying graphs: white noise, acf, pacf, Holts Winters and many of the variants. Is there a text book that has exercises and examples. The content I'm referring often is Hyndman book, but I need visual exercises to grasp the materials better.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196np4b/d_textbook_on_applied_forecasting_with_r_with/,2,4,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E34387790>
19655al,Salty-Dare-4821,2024-01-14 02:31:21+00:00,[D] easy to criticize papers for undergrads,"i'm TAing an intro to research class. i want to teach students how to critically review a paper (consider experimental design, results, etc) by having them walk through some examples. do y'all know of any easy to read ML papers that have some obvious flaws/shortcomings?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19655al/d_easy_to_criticize_papers_for_undergrads/,24,63,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E343DA050>
196vk5c,GraphHopper77,2024-01-15 00:52:40+00:00,[D] Validating a custom dataaset,"If i have to create a custom dataset to train a model, how can I makesure the dataset is valid",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196vk5c/d_validating_a_custom_dataaset/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E343D4390>
1969epa,APaperADay,2024-01-14 06:28:50+00:00,[R] I am a Strange Dataset: Metalinguistic Tests for Language Models,"**Paper**: [https://arxiv.org/abs/2401.05300](https://arxiv.org/abs/2401.05300)

**Code and dataset**: [https://github.com/TristanThrush/i-am-a-strange-dataset](https://github.com/TristanThrush/i-am-a-strange-dataset)

**Abstract**:

>Statements involving metalinguistic self-reference (""This paper has  six  sections."") are prevalent in many domains. Can large language  models  (LLMs) handle such language? In this paper, we present ""**I am a Strange  Dataset**"",  a new dataset for addressing this question. There are two  subtasks:  generation and verification. In generation, models continue  statements  like ""The penultimate word in this sentence is"" (where a  correct  continuation is ""is""). In verification, models judge the truth  of  statements like ""The penultimate word in this sentence is sentence.""   (false). We also provide minimally different metalinguistic   non-self-reference examples to complement the main dataset by probing   for whether models can handle metalinguistic language at all. The   dataset is hand-crafted by experts and validated by non-expert   annotators. We test a variety of open-source LLMs (7B to 70B parameters)   as well as closed-source LLMs through APIs. All models perform close  to  chance across both subtasks and even on the non-self-referential   metalinguistic control data, though we find some steady improvement with   model scale. GPT 4 is the only model to consistently do significantly   better than chance, and it is still only in the 60% range, while our   untrained human annotators score well in the 89-93% range. The dataset   and evaluation toolkit are available at [this https URL](https://github.com/TristanThrush/i-am-a-strange-dataset).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1969epa/r_i_am_a_strange_dataset_metalinguistic_tests_for/,1,27,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E343D6C90>
1966vsg,kodjima33,2024-01-14 04:03:03+00:00,"[P] I was tired of prompting AIs with the context about me, so I built an app that answers my questions while knowing everything about me by recording my conversations and structuring it as memories","Hey r/MachineLearning!

I got tired of writing long prompts into chatGPT and other AIs about who I am. I've always wanted to have an AI that is trained on my memories and has context of my life

I realized that if I need my AI to know things about me, it would require ""native"" features of the device I have always with me (my phone): audio recording and storing context.

**So I built my own app:** 60sec Demo: [https://youtu.be/MXZYaQlYm1Q](https://youtu.be/MXZYaQlYm1Q)

I created a very simple iOS app called Sama AI that would listen to whatever I say and then made it send me proactive relevant feedback. For example, today I was talking to my friend about mobile apps and reddit and this is the notification I received during the conversation:

https://preview.redd.it/u7evh9fmvbcc1.jpg?width=1170&format=pjpg&auto=webp&s=a4146a03cf29f5159940d4da7195f992703b1bed

Other very relevant feedback examples include:

* ""hey I noticed you talked too much, how about we do some work?""
* ""It seems you are procrastinating again and watching something irrelevant on youtube, let's pause that for a minute and get some fresh air""
* ""Yesterday you mentioned that you want to accomplish {X} today. How about we start the day with that?""
* ""How is your progress going? it seems you are underdelivering on your goal. How about we try work on some sales tomorrow""

I gave it many specific prompts with a ""mentor/coach"" personality. Also, I made this app ""educate itself"" to learn about me based on what I say. The more I use the app, the more useful it becomes => After few days of use, some of the feedback was really good - I have been using it non-stop.

I put this app into [App store](https://apps.apple.com/us/app/sama-ai/id6474986074) and I'd love to hear about your experience about building similar things! And also greatly appreciate any feedback to my app and how to improve it. Thank you for any insights you can provide!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1966vsg/p_i_was_tired_of_prompting_ais_with_the_context/,7,39,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E34392A90>
196qb2q,chigwag,2024-01-14 21:09:11+00:00,[D] Tool for annotating videos on a tablet,"Can anyone recommend a tool for annotating/labeling videos \*on a tablet\*, either Android or iPad?

Specifically  I'd like to draw bounding boxes around objects in videos, similar to  \`label-studio\` or \`CVAT\`, but on a tablet. The bounding boxes will later  be used to train ML models of course. 

Ideally this wouldn't just be the usual ""you can run the labeling GUI webapp in a browser on your tablet, but it pretends you still have a mouse,"" and would instead actually support the tablet's touch interface as a first-class interaction. Meaning stuff like ""define the corners of the bounding box with two finger multitouch,"" no-tiny-little-UI-elements, etc.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196qb2q/d_tool_for_annotating_videos_on_a_tablet/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E343699D0>
196q73u,zacky2004,2024-01-14 21:04:37+00:00,Looking to build a pipeline for radiology image segmentation using MonaiLabel [P],"Git repo: [https://github.com/Project-MONAI/MONAILabel](https://github.com/Project-MONAI/MONAILabel)

Full disclosure, senior software engineer with years of experience in python, and scientific programming. But straight-up noob when it comes to anything AI/Machine Learning related. I've decided to take my passion for medical imaging to the machine learning podium. I wish to build a training and inference pipeline using MonaiLabel's API to train and test against segmentation of radiology (CT and MRI images) using MonaiLabel as the API and framework. Has anyone here done anything similar? I'm looking for advice on how I can best take this difficult venture.

I'm already in the process of going through Monai labels builtin getting started tutorials.  
Thank you all in advance.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196q73u/looking_to_build_a_pipeline_for_radiology_image/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E34376ED0>
196mpjl,tminima,2024-01-14 18:37:36+00:00,[P] Playing with lognormal and normal distributions in Python,,MachineLearning,https://shivamrana.me/2024/01/lognormal-to-normal,1,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E3437FA50>
196mhx1,Pristine198,2024-01-14 18:28:34+00:00,"[P] OnnxStream running TinyLlama and Mistral 7B, with CUDA support","hi,  
I'm the author.  
I'm interested in opinions on this possible development of OnnxStream.  
URL: https://github.com/vitoplantamura/OnnxStream/blob/master/assets/LLM.md  
Thanks, Vito",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196mhx1/p_onnxstream_running_tinyllama_and_mistral_7b/,0,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E343D5110>
196loki,RedCuraceo,2024-01-14 17:53:42+00:00,[D] Question of imbalanced data containing small amount of minor emotion data,"Hello. I'm researching an emotional voice conversion.

I have gethered many dataset containing the emotion label with some bit of a auxiliary emotion (apologize, frustrated and so on).

I will use all of them to train my model but I wanna focus on 5 major emotions to evaluate and inference (angry, happy, excited...), to infer more various prosody.

In this case, I am wondering if there occurs the data imbalanced problem from the small amount of the minor emotions. I wanna ask how about you think, or is there any papers or any insight? Is it better to train with only major emotions?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196loki/d_question_of_imbalanced_data_containing_small/,1,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E331C0350>
196brdq,mrphanm,2024-01-14 09:03:53+00:00,[D] Hybrid modeling & Python packages for Universal Differential Equations (UDE),"Hi everyone, my background is in chemical engineering.  Recently I am interested in universal differential equation to develop a hybrid model. 

For example, lets say I have a simple model:
dx/dt= - k*x.

I want to to describe k as a neural network (NN), and then train the neural network to get k in order to predict x. The training is done on experimental data (t, x_exp), so we need to integrate the ODE for training the NN of k. This kind of ODE is called UDE

However, I find out that most of research papers worked for UDE are coded in Julia while I am familiar with python and pytorch for NN.

I also see some python packages as torchdyn and torchdiffeq, but they  mainly support for neural ODE. I am not sure if they are suitable for my case or not. One of my big concern is that if UDE is also a neural ODE or not. During searching papers, I really get confused with terminologies: neural ODE, universal differential equation, and Physics informed neural network (PINN).

If you have experience in UDE and hybrid modeling, I hope you can give some advice.

Thank you for reading and answers",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196brdq/d_hybrid_modeling_python_packages_for_universal/,12,5,0.65,<praw.models.comment_forest.CommentForest object at 0x0000026E343DBE10>
1969dl0,APaperADay,2024-01-14 06:26:52+00:00,[R] REBUS: A Robust Evaluation Benchmark of Understanding Symbols,"**Paper**: [https://arxiv.org/abs/2401.05604](https://arxiv.org/abs/2401.05604)

**Code**: [https://github.com/cvndsh/rebus](https://github.com/cvndsh/rebus)

**Dataset**: [https://huggingface.co/datasets/cavendishlabs/rebus](https://huggingface.co/datasets/cavendishlabs/rebus)

**Project page**: [https://cavendishlabs.org/rebus/](https://cavendishlabs.org/rebus/)

**Abstract**:

>We propose a new benchmark evaluating the performance of multimodal   large language models on rebus puzzles. The dataset covers 333 original   examples of image-based wordplay, cluing 13 categories such as movies,   composers, major cities, and food. To achieve good performance on the   benchmark of identifying the clued word or phrase, models must combine   image recognition and string manipulation with hypothesis testing,   multi-step reasoning, and an understanding of human cognition, making   for a complex, multimodal evaluation of capabilities. We find that   proprietary models such as GPT-4V and Gemini Pro significantly   outperform all other tested models. However, even the best model has a   final accuracy of just 24%, highlighting the need for substantial   improvements in reasoning. Further, models rarely understand all parts   of a puzzle, and are almost always incapable of retroactively explaining   the correct answer. Our benchmark can therefore be used to identify   major shortcomings in the knowledge and reasoning of multimodal large   language models.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1969dl0/r_rebus_a_robust_evaluation_benchmark_of/,0,9,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E34395A10>
196les5,NicoRobinFleur,2024-01-14 17:42:06+00:00,[P] Ideas on how to implement,"I have a question: I've created and trained a CNN on a dataset consisting of people ""with mask"" and ""without mask"". So far so good, now the question is how do i use my already trained model on a new dataset ""incorrect mask usage"" and only this dataset. I'm getting very low accuracy because the trained model identifies the mask.

 PS: i'm not allowed to change the already trained model so basically I want to classify the incorrect mask usage dataset to either with or without mask as far as I understand it",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196les5/p_ideas_on_how_to_implement/,2,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E331C36D0>
19628r2,ReluOrTanh,2024-01-14 00:09:41+00:00,[P] Request to XalosXandrez,"Hi u/XalosXandrez  I'd like to quote something you said 7 years ago in this subreddit in a paper & presentation.

I don't have enough karma to start a chat with you. Could you please ping me?

edit:  Could people please give me the karma needed?  I am a reasonably good human bean  ;-)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19628r2/p_request_to_xalosxandrez/,1,23,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E34392B10>
196obky,kulka12,2024-01-14 19:46:12+00:00,"[D] Any lifelike text-to-speech AI that is customizable in: whispering, pauses, slowing down?","I would like to leverage AI as text-to-speech. I don't need much accents, lifelike US/UK would be enough. The thing I'm looking for is extensible customization. I would like the voice to slow down in certain moments, whisper, make pauses, extend words.

The main goal is using it for relaxation techniques so it's pretty much necessary. Which provider or providers should I focus on?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196obky/d_any_lifelike_texttospeech_ai_that_is/,1,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E343E87D0>
195soqh,FallMindless3563,2024-01-13 17:07:46+00:00,[D] Question about Mixture of Experts in Transformers - Has anyone tried adding the router before the Multi Head Attention Blocks?,"This question came up in our Friday paper club as we read the Mixtral 8x7B paper, and don't feel like we got a satisfying answer.

It seems like the argument for MOE is that you can let certain parts of the network specialize in certain domains or tasks. This also strikes me as a similar argument people make for having multi-head attention within the transformer block. Why would you only put the router in front of the Feed Forward Layers and not in front of the multi-head attention as well?

&#x200B;

https://preview.redd.it/gyb8mevco8cc1.png?width=1666&format=png&auto=webp&s=0595120e2fdf96bbb5797bcc85646a90d1419773

Routing before the multi-head attention could allow the network to better choose what it attends to, where routing after the heads could help predict the next word based on the attention. Seems like you would get similar increases in latency if you only had to run a subset of the multi-head attention.

What am I missing? Has anyone tried this?

&#x200B;

Recap of our notes here for anyone interested:

[https://blog.oxen.ai/arxiv-dives-mixture-of-experts-moe-with-mixtral-8x7b/](https://blog.oxen.ai/arxiv-dives-mixture-of-experts-moe-with-mixtral-8x7b/)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195soqh/d_question_about_mixture_of_experts_in/,12,38,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E343DFDD0>
196doe4,DomeGIS,2024-01-14 11:15:15+00:00,[P] SemanticFinder - private in-browser semantic search based on transformers.js,"Hi folks!

I want to democratize semantic (and hybrid) search with the latest version of SemanticFinder, a private in-browser semantic search web app based on transformers.js.

GitHub: [https://github.com/do-me/SemanticFinder](https://github.com/do-me/SemanticFinder)  
App: [https://do-me.github.io/SemanticFinder/](https://do-me.github.io/SemanticFinder/)  
Huggingface Catalogue: [https://huggingface.co/datasets/do-me/SemanticFinder](https://huggingface.co/datasets/do-me/SemanticFinder)

You can index any text on the fly and query it with any (onnx) feature-extraction model on Huggingface. If the text is very long like e.g. the whole Bible and hence many chunks need to be processed, you can save the index after processing it once. Indexing the bible with \~23.000 chunks à 200 chars takes roughly 20-30 mins on my i7 laptop (32GB RAM, no GPU) with onnx-quantized versions of gte-tiny/small or bge-small models. The awesome thing is that you can use multilingual embeddings to query a text in a different language (100+). Handy when studying e.g. Latin texts and you're looking for some kind of leitmotif in English.

My idea was to create a public archive of indexed texts so documents/books of public interest only need to be processed once. For now, this place is on Huggingface where you can test all examples like The Bible (en), Les Misérables (fr), Das Kapital (de), Don Quijote (es), Divina Commedia (it), Iliad (gr), IPCC Report 2023 (en). I really like HF as a platform and the company spirit so I hope this project is within their usage policy.

If there are any folks around from the large vector DBs (Qdrant, Milvus, Pinecone, Weaviate etc.) I'd be super happy to discuss some kind of integrations or conversion tools. For every document I am automatically calculating and saving the average embedding too, so theoretically the whole HF collection of documents (per used model) could be ingested in a vector DB and made accessible there, including the metadata. Also, SemanticFinder could be used as a simple frontend for vector DBs, e.g. when you have weekly reports you'd like to ingest in your private collection.

There is also a browser plugin available indexing websites on the fly. Theoretically the whole workflow with saving indices would work for websites and pdfs too! Imagine e.g. that with one click on your browser plugin you could automatically save a whole research paper and its index in a vector DB. You'd never again worry that you can't remember where this one passage was from again. I could even imagine this as augmented scientific workflow: you read a paper and write down your notes. A plugin for your text editor of choice could then find the matching passages in the research papers you read and quote/reference it for you in your text. Of course, this opens the door for sloppy science when the best match is actually not a real match.

Generally, I was surprised myself how powerful browsers have become. I tested the app for a maximum of 100.000 chunks/embeddings and it's still super performant. I guess at some point - no idea when - the browser runs out of memory but for now it's safe to say that the app works for any book!

If you have any new books/documents you'd like to see in the public collection, you can either: 

a) index yourself and create a PR on the HF repo or   
b) create an issue on GitHub with the source URL and I'll add it.

Happy to hear your thoughts and ideas!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196doe4/p_semanticfinder_private_inbrowser_semantic/,0,1,0.56,<praw.models.comment_forest.CommentForest object at 0x0000026E343E8AD0>
196p8y3,Feralzi,2024-01-14 20:25:22+00:00,Introducing Lunai - Reinforcement Learning without any Coding [P],,MachineLearning,https://youtu.be/qzEjlsLOmnQ?si=Gy9P0T_2Lkfitnmb,0,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E343E8610>
196gw5c,Snoo_72181,2024-01-14 14:19:54+00:00,[D] AI/ML for warehouse automation discussion thread,"What are some commonplace applications of AI/ML in warehouse automation? I can think of inventory forecasting, robot route optimization, and all kinds of planning. Also CV for scanning objects and general observation

What are some good datasets for this domain on the Internet that can be used to build perosnal projects?

How about some interesting papers?

Which are some of the best companies in this domain? 1 I can think of is Amazon Robotics.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196gw5c/d_aiml_for_warehouse_automation_discussion_thread/,0,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E32BC9D90>
195lwlh,Few-Pomegranate4369,2024-01-13 11:21:51+00:00,"[D] How Do Leading AI Research Organizations Like OpenAI, Google, and Meta Track and Manage Their Large Scale AI Experiments?","I'm really interested in learning about the tools and techniques that researchers at OpenAI, Google, and Meta use to keep track of their AI experiments. This includes how they manage things like different versions of AI models and the various tests they run on them. I'd love to know what specific tools they use for these tasks. Also, it would be great to understand if there are any recommended approaches or best practices they follow to organize and handle these experiment runs effectively. Since I'm a researcher too, this information would be incredibly useful for me.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195lwlh/d_how_do_leading_ai_research_organizations_like/,28,69,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E3440B350>
196gmms,plasticlightz,2024-01-14 14:05:54+00:00,[P] Project: Help an layman architect to make a GAN works for shoe design,"4 years ago I was in a shoe design contest to develop a new shoe for a Brazilian brand, and as an architect who never designed shoes, I went trough the ""easy"" way that was creating a new shoe from all the others created. I'm a computer science enthusiastic but never was good with math and logic to become a coder, so I found on my research the Generative Adversarial Network and found this article [https://timwu-68834.medium.com/how-to-use-gan-to-create-new-fashion-synthesis-shoe-to-assist-designer-58512dd7789a](https://timwu-68834.medium.com/how-to-use-gan-to-create-new-fashion-synthesis-shoe-to-assist-designer-58512dd7789a) who left me very excited because I wanted to do the same. 

I tried my best but wasn't enough to make it work, so now I'm wondering if there's anyone here who wants to help me to make it work or at least talk about cause I'm really passionate about computer science but more I try to learn, more dumb I feel :/ 

In the end, I used AI (adobe illustration) to draw something new based on 3 shoes of the brand. I liked the result but I never forget the first idea and now, 4 years later, with all the IA generative I'm feeling like this idea can work.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196gmms/p_project_help_an_layman_architect_to_make_a_gan/,1,0,0.42,<praw.models.comment_forest.CommentForest object at 0x0000026E34400BD0>
1966c8g,hitszids,2024-01-14 03:33:21+00:00,[P] Synthetic Data Generator (SDG) Open-Source Library,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) community! We are the guys from Hitsz-ids Team

Me and some fellows have created on a synthetic data generation framework which can quickly generate high-quality tablular data ([https://github.com/hitsz-ids/synthetic-data-generator](https://github.com/hitsz-ids/synthetic-data-generator)) I hope this project will be helpful to you

If you've got any ideas or you want to discuss the implementations, feel free to hangout in our friendly synthetic data slack community at [https://app.slack.com/client/T05T8RV068Y/C05SGVCALSH](https://slack.ydata.ai/) !",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1966c8g/p_synthetic_data_generator_sdg_opensource_library/,0,2,0.63,<praw.models.comment_forest.CommentForest object at 0x0000026E34409A90>
196kv42,Sprixl,2024-01-14 17:18:51+00:00,Need help evaluating my ai model [R],"Hello, I have spent over a year implementing and training a model and I want to try to compare the model's accuracy against human prediction. Ideally what I am looking for are people to help give ""predictions"" for set scenarios, and then I'll compare that against my model's output. If you are interested, please dm me or leave a comment and I'll reach out with more info. If you are willing to help out, I will offer a free version of the software to you. Thanks",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196kv42/need_help_evaluating_my_ai_model_r/,3,0,0.11,<praw.models.comment_forest.CommentForest object at 0x0000026E331D09D0>
195uwh1,austinv11,2024-01-13 18:44:02+00:00,[D] What is the state of generative flow networks?,"Gflownets seemed to have initially blown up, but what is the consensus regarding their applicability in causal inference? 

Paper recommendations are appreciated! I haven't found anything recent that satisfyingly that incorporate gflownets in practice.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195uwh1/d_what_is_the_state_of_generative_flow_networks/,2,6,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E343EF9D0>
19534v6,we_are_mammals,2024-01-12 19:14:35+00:00,What do you think about Yann Lecun's controversial opinions about ML? [D],"Yann Lecun has some controversial opinions about ML, and he's not shy about sharing them. He wrote a position paper called ""A Path towards Autonomous Machine Intelligence"" a while ago. Since then, he also gave a bunch of talks about this. This is a screenshot

&#x200B;

https://preview.redd.it/xxmxgrdk02cc1.jpg?width=1581&format=pjpg&auto=webp&s=4a7e98f5a41f2e454e2e33881f2df93c7287d09b

from [one](https://www.youtube.com/watch?v=OKkEdTchsiE), but I've watched several -- they are similar, but not identical. The following is not a summary of all the talks, but just of his critique of the state of ML, paraphrased from memory (He also talks about H-JEPA, which I'm ignoring here):

* LLMs cannot be commercialized, because content owners ""like reddit"" will sue (Curiously prescient in light of the recent NYT lawsuit)
* Current ML is bad, because it requires enormous amounts of data, compared to humans (I think there are two very distinct possibilities: the algorithms themselves are bad, or humans just have a lot more ""pretraining"" in childhood)
* Scaling is not enough
* Autoregressive LLMs are doomed, because any error takes you out of the correct path, and the probability of not making an error quickly approaches 0 as the number of outputs increases
* LLMs cannot reason, because they can only do a finite number of computational steps
* Modeling probabilities in continuous domains is wrong, because you'll get infinite gradients
* Contrastive training (like GANs and BERT) is bad. You should be doing regularized training (like PCA and Sparse AE)
* Generative modeling is misguided, because much of the world is unpredictable or unimportant and should not be modeled by an intelligent system
* Humans learn much of what they know about the world via passive visual observation (I think this *might* be contradicted by the fact that the congenitally blind can be pretty intelligent)
* You don't need giant models for intelligent behavior, because a mouse has just tens of millions of neurons and surpasses current robot AI",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19534v6/what_do_you_think_about_yann_lecuns_controversial/,207,438,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E3449E7D0>
195rlga,NLPnerd,2024-01-13 16:20:00+00:00,"[N] Michelle Gill: AI-Assisted Drug Discovery, NVIDIA, Biofoundation | Learning from Machine Learning","Listen to Dr. Michelle Gill, Tech Lead and Applied Research Manager at NVIDIA, working on transformative projects like BioNemo to accelerate drug discovery through Al. Her team explores Biofoundation models to enable researchers to better perform tasks like protein folding and small molecule binding.
Michelle shares her incredible journey from wet lab biochemist to driving cutting edge Al at NVIDIA.

Michelle discusses the overlap and differences between NLP and Al in biology. She outlines the critical need for better machine learning representations that capture the intricate dynamics of biology.

Michelle provides advice for beginners and early career professionals in the field of machine learning, emphasizing the importance of continuous learning and staying up to date with the latest tools and techniques. She also shares insights on building successful multidisciplinary teams",MachineLearning,https://youtu.be/uXAqVifiSTw?si=sWQIx7EIu_ccepFl,1,9,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E34429250>
1961mz7,coracarm,2024-01-13 23:41:04+00:00,[D] LLM Chat Agents - Lightweight Tool Selection using BERT,"The post uses BERT and classification techniques for ‘Agent tool selection’. It presents this as an alternative approach to using an LLM, suggesting it can be effective for some use cases. On the surface, it seems like a good idea, reserving the LLM only for answer generation, saving cost and latency. Thoughts on this as an approach?",MachineLearning,https://carmine.wtf/llm-agents-lightweight-tool-selection-with-bert,0,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E331A82D0>
1958jbm,lisp-cloj,2024-01-12 23:00:47+00:00,[D] Good ML Eng question banks for interviews?,"I've been studying for ML engineering interviews (and doing some), and I've realized that the common advice of ""learn about bias, variance, cross-fold validation, etc."" is all wrong. The top companies are asking you to code simple things using Pytorch/numpy. So questions are things like: ""write a neural net to solve X problem"" or ""implement k-means using numpy"".

Given this is the case, I think it's much more useful to prepare for these interviews by doing a bunch of coding questions.

I was wondering if people here could share some of the coding questions they experienced in ML Eng interviews, or point me to good Leetcode-style MLEng question banks?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1958jbm/d_good_ml_eng_question_banks_for_interviews/,36,164,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E344A8A90>
195q259,Neuro-AI,2024-01-13 15:11:11+00:00,[D] resources for avoiding common mistakes in machine learning projects?,"There are often managers or entrepreneur types that think of ""AI"" as a magical solution and then millions of dollars get wasted because they don't understand how fragile the solutions can be, how do much more data is needed to create solutions that generalize, susceptibility to bias, training data needed, etc. What accessible information is there that would help those people understand what is involved for practicality of project and successful and ethical outcomes?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195q259/d_resources_for_avoiding_common_mistakes_in/,2,8,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E3436F890>
196mg72,SaltFalcon7778,2024-01-14 18:26:27+00:00,[D] Which neural networks is more like the human brain?,"
So I just heard about this subject and found it fascinating, and wanted to know about which one is closer to biological neural network",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/196mg72/d_which_neural_networks_is_more_like_the_human/,26,0,0.2,<praw.models.comment_forest.CommentForest object at 0x0000026E344BE550>
195z3jw,golpanda,2024-01-13 21:46:36+00:00,[P] Help with Transformer Model,So I am relatively new in NLP and my prof has asked me to go through some Transformer Models that also incorporate linguistic features alongside sentence pair to get an understanding. I tried my best to find some code but failed to manage any legit repo aside from some papers that does not have any code in them. Any help regarding this?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195z3jw/p_help_with_transformer_model/,2,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E331D3AD0>
195tut6,dev-spot,2024-01-13 17:59:13+00:00,"[N] OpenDalle v1.1, VCoder, LongAnimateDiff & More!","Hey,

AI has been going crazy lately and things are changing super fast. I created a video covering some of the latest trending huggingface spaces that you've got to check out! OpenDalle v1.1 has been released, allowing you to create stunning images. VCoder is also available now, allowing you to get a full breakdown of what is seen in the images we pass it. Other than these 2, we covered LongAnimateDiff, PASD Magnify, M\^2UGen, Pheme & PIA. Check it out to stay up to date with the latest trends!

[https://www.youtube.com/watch?v=MbLXWxbcVoc](https://www.youtube.com/watch?v=MbLXWxbcVoc)

OpenDalle is insanely good. Its based on Stable diffusion, but with some tweaks, and honestly produces some really good results. Make sure to check it out, they also provide an inference end point for ya'll to play with.

Feel free to subscribe to my newsletter which will contain weekly-monthly summary of new tech in the AI space:

[https://devspot.beehiiv.com/subscribe](https://devspot.beehiiv.com/subscribe)

Let me know what you think about it, or if you have any questions / requests for other videos as well,

cheers",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195tut6/n_opendalle_v11_vcoder_longanimatediff_more/,0,2,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E331BF990>
195cxim,FateRiddle,2024-01-13 02:18:39+00:00,[D] What is the best text-to-speech tool currently?,"Hi everyone, I need a TTS tool that sounds exactly like a human voice. I want to use it to edit some of my YouTube videos, more specifically uploading my own sample of voice in choice and generate good result from it. I see a lot of TTS platforms around. Which do you recommend? I hope this isn't too much to ask. I would gladly appreciate it.

Thanks in advance.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195cxim/d_what_is_the_best_texttospeech_tool_currently/,27,32,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E344CA650>
195smzg,ornob_50,2024-01-13 17:05:28+00:00,Looking for a research topic to apply explainable AI in the medical diagnosis sector [D],"Hi All,

I am currently an undergrad student. I am looking for a research topic, preferably in medical diagnosis, where I can apply explainable AI.

In my initial search, I found out that for various problems in the medical diagnosis sector, we have already well-performing ML/DL models. These models provide prediction with high accuracy. But these predictions don't have any explainability.  I want to work to add explainability to this model.

If anyone suggests some resources for the topics or helps me in any way, it will be much appreciated.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195smzg/looking_for_a_research_topic_to_apply_explainable/,4,2,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E344BEF90>
19667f4,lighter_links,2024-01-14 03:26:14+00:00,[P] Share the online version of you with your friends to chat with,"HI! As a big project of our team, we created a platform where **you can build your own personalized AI (kind of like your advocate) and share it with others** so they can chat with it.

Building process is really simple: Your chatbot asks you questions and by answering them, it gets to know better about you.

After that, share it with your friends, family to see if your advocate really speaks like you!

&#x200B;

create your advocate : [https://isityou.space](https://isityou.space)  
chat with my advocate(name is 이동호, also, it might speak korean first, force it to speak english by saying 'speak english!') : [https://isityou.space/c/XhgXEkBHTb](https://isityou.space/c/XhgXEkBHTb)

https://preview.redd.it/i4tdzqw7rbcc1.png?width=1088&format=png&auto=webp&s=e311050cc1137bde33cd1bf14ccbd7171bd2d8b8",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19667f4/p_share_the_online_version_of_you_with_your/,0,0,0.22,<praw.models.comment_forest.CommentForest object at 0x0000026E344C7DD0>
19600kb,RealAd1834,2024-01-13 22:27:08+00:00,[R] [D] [P] Machine learning and remote,"Hey everyone , I'm a final year student and I'll work on a project involving machine learning and remote sensing with python (Detection of trees per example) , I honeslty don't know from where to start , I've looked it up on the internet and found it so wide .
Anyone could help me please its really urgent. 
Thanks",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19600kb/r_d_p_machine_learning_and_remote/,1,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E3440B250>
195eulx,APaperADay,2024-01-13 03:54:59+00:00,"[R] UnIVAL: Unified Model for Image, Video, Audio and Language Tasks","**arXiv**: [https://arxiv.org/abs/2307.16184](https://arxiv.org/abs/2307.16184)

**OpenReview**: [https://openreview.net/forum?id=4uflhObpcp](https://openreview.net/forum?id=4uflhObpcp)

**Code**: [https://github.com/mshukor/UnIVAL](https://github.com/mshukor/UnIVAL)

**Checkpoints**: [https://github.com/mshukor/UnIVAL/blob/main/checkpoints.md](https://github.com/mshukor/UnIVAL/blob/main/checkpoints.md)

**Project page**: [https://unival-model.github.io/](https://unival-model.github.io/)

**Demo**: [https://huggingface.co/spaces/mshukor/UnIVAL](https://huggingface.co/spaces/mshukor/UnIVAL)

**Video**: [https://www.youtube.com/watch?v=mYOun92st08](https://www.youtube.com/watch?v=mYOun92st08)

**Abstract**:

>Large Language Models (LLMs) have made the ambitious quest for  generalist agents significantly far from being a fantasy. A key hurdle  for building such general models is the diversity and heterogeneity of  tasks and modalities. A promising solution is unification, allowing the  support of a myriad of tasks and modalities within one unified  framework. While few large models (e.g., Flamingo (Alayrac et al.,  2022), trained on massive datasets, can support more than two  modalities, current small to mid-scale unified models are still limited  to 2 modalities, usually image-text or video-text. The question that we  ask is: is it possible to build efficiently a unified model that can  support all modalities? To answer this, we propose **UnIVAL**, a step  further towards this ambitious goal. Without relying on fancy datasets  sizes or models with billions of parameters, the \~ 0.25B parameter  UnIVAL model goes beyond two modalities and unifies text, images, video,  and audio into a single model. Our model is efficiently pretrained on  many tasks, based on task balancing and multimodal curriculum learning.  UnIVAL shows competitive performance to existing state-of-the-art  approaches, across image and video-text tasks. The feature  representations learned from image and video-text modalities, allows the  model to achieve competitive performance when finetuned on audio-text  tasks, despite not being pretrained on audio. Thanks to the unified  model, we propose a novel study on multimodal model merging via weight  interpolation of models trained on different multimodal tasks, showing  their benefits in particular for out-of-distribution generalization.  Finally, we motivate unification by showing the synergy between tasks.  The model weights and code are released here: [this https URL](https://github.com/mshukor/UnIVAL).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195eulx/r_unival_unified_model_for_image_video_audio_and/,1,15,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E344BFE10>
195eh7b,APaperADay,2024-01-13 03:35:41+00:00,[R] PASTA: Pretrained Action-State Transformer Agents,"**arXiv**: [https://arxiv.org/abs/2307.10936](https://arxiv.org/abs/2307.10936)

**OpenReview**: 

[https://openreview.net/forum?id=ciBFYxzpBT](https://openreview.net/forum?id=ciBFYxzpBT)

[https://openreview.net/forum?id=pxK9MWuFF8](https://openreview.net/forum?id=pxK9MWuFF8)

**Abstract**:

>Self-supervised learning has brought about a revolutionary paradigm   shift in various computing domains, including NLP, vision, and biology.   Recent approaches involve pre-training transformer models on vast   amounts of unlabeled data, serving as a starting point for efficiently   solving downstream tasks. In reinforcement learning, researchers have   recently adapted these approaches, developing models pre-trained on   expert trajectories. This advancement enables the models to tackle a   broad spectrum of tasks, ranging from robotics to recommendation   systems. However, existing methods mostly rely on intricate pre-training   objectives tailored to specific downstream applications. This paper   conducts a comprehensive investigation of models, referred to as   pre-trained action-state transformer agents (**PASTA**).  Our study covers a  unified methodology and covers an extensive set of  general downstream  tasks including behavioral cloning, offline RL,  sensor failure  robustness, and dynamics change adaptation. Our  objective is to  systematically compare various design choices and offer  valuable  insights that will aid practitioners in developing robust  models. Key  highlights of our study include tokenization at the  component level for  actions and states, the use of fundamental  pre-training objectives such  as next token prediction or masked  language modeling, simultaneous  training of models across multiple  domains, and the application of  various fine-tuning strategies. In this  study, the developed models  contain fewer than 7 million parameters  allowing a broad community to  use these models and reproduce our  experiments. We hope that this study  will encourage further research  into the use of transformers with first  principle design choices to  represent RL trajectories and contribute to  robust policy learning.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195eh7b/r_pasta_pretrained_actionstate_transformer_agents/,1,9,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E344AAAD0>
1963xri,DeeDaveDee,2024-01-14 01:30:53+00:00,[D] AI & Technology.,"Hi All,

&#x200B;

Just a quick question for those interested in AI technology, specifically with Social Media.

&#x200B;

With AI, what are the smartest methods for finding language that could be seen as offensive or not considerate of other cultures?

&#x200B;

Also, what tools would suggest I use within Machine Learning?

&#x200B;

I'm open to any suggestions or pointers.

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1963xri/d_ai_technology/,3,0,0.2,<praw.models.comment_forest.CommentForest object at 0x0000026E331DFE90>
195gyqe,lostinspaz,2024-01-13 05:50:55+00:00,[D] Hypothesis: directed positioning for the vectors in models (eg:ViT-L/14) may allow for new possibilities,"I have recently been poking at the CLIP model ViT-L/14, to examine what the data looks like.

I notice that, even for definitions of things that are ""close"" to each other, the closeness is almost random in nature.   
I am guessing that, during training, values were tweaked through random motion, until objects that ""should"" be together, landed in an n-space position that was deemed ""close enough"", and things ended there.   


But that leaves the coordinates very unsatisfyingly random. Example of this, is comparing the position in 768-space, of ""cat"" vs ""kitten"" here:  


https://preview.redd.it/23v9ux27b5cc1.png?width=569&format=png&auto=webp&s=895f80682a3f6f321bcb8a2482749649c1074c8b

They have a euclidian distance of 7.22859525680542 

What if objects that truely belong ""closely"" together... actually were together on most dimentions?

What if the dataset could be reorganized, so that objects that are truely similar, reflected that more in 768-space?  


That is to say, what if ""cat"" and ""kitten"" only had a few dimensions that differed, but the rest were the same?  


It seems to me that could open up some interesting possibilities.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195gyqe/d_hypothesis_directed_positioning_for_the_vectors/,5,2,0.58,<praw.models.comment_forest.CommentForest object at 0x0000026E344D48D0>
19658xf,harith_chandra,2024-01-14 02:36:31+00:00,Need help [D],I am intermideate full stack dev. But I wanted to change to ml . Is it very complex and it has difficult mathematics. Also which can give me a job as a fresher? . Thank you,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19658xf/need_help_d/,7,0,0.14,<praw.models.comment_forest.CommentForest object at 0x0000026E344D57D0>
19549sd,gofiend,2024-01-12 20:01:27+00:00,[D] Cheapest way to scale up and down from a large GPU to just CPU with minimal overhead?,"I've got a bunch of hobbyist projects to work on while I'm away from home for a few weeks and away from my fairly powerful GPU desktop.

I'm looking to find the simplest (and cheapest) way to get a computer that I can SSH into and:

1. Do some coding/debugging (spend 80% of my time)
2. Run some fairly niche ML software in essentially batch mode that needs a low end GPU (10%)
3. Fire up a powerful 48 GB GPU to mess around with LLMs (10%)

Given the setup / config overheads, what I'd like to do is attach say 60 GB of storage to a EC2.micro, install a full Lambdalabs container (Nvidia drivers, CUDA, Pytorch etc.) , use it for #1, then swap that boot drive over to more compute or GPU intensive machines to run #2 or #3 for a few hours when I'm ready.

Can a single well configured boot drive work across vastly different compute configs? Is there a cloud provider that works best for this sort of thing?

In particular, I'd love to be able to turn the $s down as much as possible when I'm not actively doing something (I'm fine paying for a bit of storage).

I'd love to do it with someone like LamdaLabs, but they don't have cheap low end CPU only instances.

Is there a smarter (not crazy high effort with Ansible etc.) way to maintain a SSH and go setup that will just work across vastly different scale compute?

 ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19549sd/d_cheapest_way_to_scale_up_and_down_from_a_large/,12,10,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E344DE350>
195mggo,Pleasant_Ad_6267,2024-01-13 11:58:49+00:00,[D] Can I use my Server to accelerate ML workflow?,"Hi guys, 
I have a HPE Proliant DL360p Gen9 with this specs:

- 2 x Intel Xeon E5-2680V4 (14 Core, 28 Threads x 2)
- 128 GB ECC RAM DDR4 
- 4 TB HDD 15K IN RAID10
- 10GbE network card 

I was thinking about buying a dedicated GPU for it, but I have seen that the GPUs that are compatible with it are very limited in power (Tesla M4, NVIDIA Quadro P4000).
These GPUs are a little old and good only for small inference.

Despite using it for Docker and Kubernetes, that I use a lot in my daily ML workflow, do you think that I can use the processing power of the CPUs (I have seen that are pretty powerful) in some useful way or is it a waste of time? 
If you think that buying a dedicated GPU for it is a good idea, let me know.

Thanks
Giacomo",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195mggo/d_can_i_use_my_server_to_accelerate_ml_workflow/,2,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E344DD090>
195plsr,Several-Equivalent11,2024-01-13 14:49:56+00:00,[D] Quick question about interpretability and quantum computing,"if interpretability is about understanding how models works and models work on probability theory and quantum computing allows us to compute more probabilistically, how will the development of both technologies impact each other?

don't know if the question makes sense tbh, I am super new to this and only just starting my learning journey in machine learning - I was reading Rosenblatt's paper on Perceptrons and keep coming across both interpretability and quantum computing on twitter discourse so figured I'll ask  


would be great if y'all could also recommend any resources I should check out if this piques my interest",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195plsr/d_quick_question_about_interpretability_and/,4,0,0.3,<praw.models.comment_forest.CommentForest object at 0x0000026E344DC350>
195tl1n,Hugewin2022,2024-01-13 17:47:10+00:00,[D] Newbie in need of some guidance: PyTorch or TensorFlow?,"Hello, fellow machine learning enthusiasts! I am a newbie in this field and I have recently joined this subreddit to learn from your amazing posts and discussions. I hope you don't mind me asking for some advice on how to get started with machine learning.

I have done the basic maths and some theory about datasets, training, and loss functions, etc. But as soon as I was going to learn TensorFlow, I saw some posts on this subreddit that made me confused about choosing PyTorch or TensorFlow. I have read some articles that compare the two frameworks, but I still can't decide which one is better for me.

I would appreciate it if you could share your opinions and experiences with these two frameworks. Which one do you prefer and why? What are some of the projects that you have done or seen using PyTorch or TensorFlow? What are some of the resources that you would recommend for learning either of them?

Thank you for your time and help. I look forward to hearing from you and learning more about machine learning!

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195tl1n/d_newbie_in_need_of_some_guidance_pytorch_or/,6,0,0.08,<praw.models.comment_forest.CommentForest object at 0x0000026E344E0A10>
194ap95,BootstrapGuy,2024-01-11 19:52:44+00:00,Most things we have today in AI will be a irrelevant in 6 months [P],"This is the unfortunate situation when you build ""thin wrapper"" products on the top of foundational models.

Last year we built a custom Stable Diffusion pipeline for our client, did a lot of experimentation over 2 months, figured out custom solutions for edge cases and shipped a pipeline that could convert group photos to Christmas gift cards.

Today, Alibaba launched ReplaceAnything and I could build the same thing with maybe 10% quality drop in a minute (!) as our team spent couple of weeks on just a few months ago.

The progress in this space is insane.

Fortunately, this was just ""one of those small fun things"" that we built for our client.

I just can't imagine the stress of building one of these companies especially if you raised venture.

The clock is ticking and with every day you have less and less technical moat.

And this is the reason why you need to go all in creating a long-term, sustainable data moat asap.

https://preview.redd.it/7a67geld8vbc1.png?width=722&format=png&auto=webp&s=c4dc336cf2635c178ad6ccfc65d10292f5c881f4",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194ap95/most_things_we_have_today_in_ai_will_be_a/,80,385,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E3451E590>
19572jd,eatpasta_runfastah,2024-01-12 21:59:50+00:00,Node Classification in Graphs [Discussion],"Tl;Dr: I have a heterogeneous graph where edges have features and nodes of a certain type have a label. I want to predict the label. But I'm finding it hard to get results. Share your experiences with similar problems 


My data consists of agents and items, and I want to predict the quality of an item based on agents interactions. It's a binary classification problem.


More details on my problem setup:

- Edges only exist between agent and items, where an agent can interact with multiple items.

- Nodes can be either agents or items. Agents nodes have no feature, whereas items nodes have labels (which is what I'm trying to predict)

- Edges have features such has the length of the interaction and the quality of it.


I've tried with some non-deep methods, mostly ensembles and boosting methods. For those I used statistics on the incoming edges for a node (eg. Number of agents, average interaction length, ecc..). Depending on the features I use I am able to get about 0.6 F1, with varying shares of precision and recall (sometimes as high as 80%)

I'm finding it hard to get results with GNN. I've tried with Graph Attention Networks and I am experimenting with SAGEConv, but I'm not really sure how to deal with the edges features for convolutional layers. I feel like just pooling them by computing the mean or max would just be the same as using a decision forest.

At the same time I feel like using GNN could help taking advantage of the  geometry  of the data, which is kinda destroyed when using standard ML methods.

So my question is, for those of you who have encountered similar problems, what did/didn't work for you and why?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19572jd/node_classification_in_graphs_discussion/,4,3,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E344EA610>
194v0n2,whereismycatyo,2024-01-12 13:26:30+00:00,Could unsupervised clustering approximate ground truth categories or classification? [D],"Say we have a labeled dataset and we used clustering to cluster the instances. We also could use classification to categorize instances since it is labeled. Can anyone here please explain if there are cases where the clustering would result in similar results as the classification? Thanks,

There is not that much need to use clustering if we have ground truth categories, but I just wanted to know if it can ever approximate or equal classification.

EDIT:  Assuming a deterministic clustering approach and number clusters=number of categories. ([This](https://stats.stackexchange.com/a/205865) seems a nice answer for deterministic clustering approaches.)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194v0n2/could_unsupervised_clustering_approximate_ground/,13,13,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E345021D0>
1961kxj,KuneeMunee,2024-01-13 23:38:29+00:00,[D] Autofacture,"So, Artificial Intelligence (AI) is now a thing, or at  least it's becoming more prevalent and commonplace. I found that, we  have no words (in English); used to describe things made without or with  very little human intervention, that have little ambiguity. So, I decided,  why not make one?

I present, **Autofacture**.

# Definition:

## Autofacture:

*verb*

1. To create something with little-to-no human interference or influence, typically with non-human intelligent systems, like AI. *""Instead of traditional manufacturing methods, the automotive industry is exploring ways to* ***autofacture*** *certain components using advanced robotic systems.""*

## Autofactured:

*adjective*

1. Something that has been created or manufactured with minimal or no human involvement, typically by autonomous systems, machines, or artificial intelligence.                                                                                          *""The image had been* ***autofactured*** *in such a way, it resembled the work of a human.""*
2. An idea or concept conceived or offered by an artificial, non-human, system. *""The method was* ***autofactured****, but effective.""*

Hopefully this word clears up any ambiguity and can be used in this new and rapidly changing world. I would also love to hear any suggestions, examples or questions anyone has on this idea, thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1961kxj/d_autofacture/,12,0,0.11,<praw.models.comment_forest.CommentForest object at 0x0000026E344ED5D0>
1958r4n,Sad-Fondant3060,2024-01-12 23:09:54+00:00,[Discussion] MS Online Programs for ML (EE Background)," 

I am considering getting a Masters in Machine Learning (whether that be CS with ML focus, or ML+Data Science, etc). Was wondering what are good programs to look at?

I have a BS in Electrical Engineering, and a MS in Electrical Engineering as well, but during MS I focused all my courses on ML because I realized it was what I was interested in. Am currently struggling landing a ML-related job with no professional experience. Any advice would be greatly appreciated, thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1958r4n/discussion_ms_online_programs_for_ml_ee_background/,0,3,0.71,<praw.models.comment_forest.CommentForest object at 0x0000026E345208D0>
194xsl6,LaniakeaResident,2024-01-12 15:31:28+00:00,[D] Discussion on the feasibility of my project with medical imaging in spine surgery.,"  

Hello, I am trying to get an understanding of the feasibility of a project that I am planning on undertaking and would appreciate any and all input from this community.

Background: I am neurosurgeon (resident in last year of training), with a focus on spine surgery. Spine surgery is a surgical field that I think has a lot to benefit from AI implementation. More often than not, for any given spine pathology that is identified through imaging (and symptomology) there are several different types of surgical procedures/approaches that can address the pathology. Those different approaches have a wide range of associated costs and often can result in variable outcomes, both short term and long term. Surprisingly, the spine literature has reached only a minimal level of consensus regarding optimal approaches to the more straightforward pathologies. 

My objective: In very simplified terms, I want to implement machine learning to train on pre-operative patient data and patient imaging (MRIs, CTs, Xrays) and post-operative patient outcomes data, as well as post operative imaging to create predictive models that can do the following:

1) Given a new patient pre-operative data/symptomology and imaging predict the type of surgical intervention that will give the best outcome.

2) Give the surgeon optimal corrective spinal parameters to implement surgically. For example the degrees of corrections in the spinal curvature that needs to be surgically restored. 

Over the past year, I have worked with hospital administrators and the radiology department and have finally been able to establish a pathway to be able to access over 2500 patient imaging and associated data. This has all been approved by out institutional review board, and all the data will be HIPPA compliant and deidentified. I have also been able to collaborate with a state wide spine surgery outcomes database that collects very detailed, organized, pre-and post op data on spine surgery patients throughout major centers in the state. My institutional data can easily be cross referenced with the statewide data to obtain the additional outcomes data. 

I unfortunately don’t have the expertise in computer science and machine learning to understand the feasibility of developing and training ML algorithms on this kind of data sets. Despite that, I do understand that it will take significant resources to accomplish something of this scale. With that said, through some networking, I have been able to secure a pledge of 3 million dollars from one prominent local financier and doner who has deep connections with our hospital system. The investment from this individual is predicated on my ability to put together a technical team.  I have also been able to secure two meetings with other investors who have shown great interest in the project. My goal is to raise about 5 million dollars. My proposition to the hospital system (which they are fully on board with) has been that I will set-up a company, with the hospital being an equity partner, and in return the hospital will do what it can to streamline access to the patient and imaging data. The company will be structured in a way that will allow for external funding through venture investments. 

The tools that can be developed through this initiative will have significant commercial implications in the spine surgery market, and importantly have meaningful benefits on surgical outcomes for spines surgery patients. 

Now, obviously this will go nowhere without a technical team. But first, I am hoping to have a discussion here to understand the technical feasibility of a project like this, the types and amount of resources that will be needed. 

Again, for the purposes of keeping this post short I am forgoing a lot details, but I am happy to delve further into details through the comments. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194xsl6/d_discussion_on_the_feasibility_of_my_project/,12,7,0.71,<praw.models.comment_forest.CommentForest object at 0x0000026E34525E90>
1950nep,Ok_Cartographer5609,2024-01-12 17:31:16+00:00,[D] What UI library/framework/stack do you just for a RAG/LLM based MVP?,"I am building an mvp for presenting before clients. My backend is built on fastapi, utilizing both huggingface and openai. What will you suggest me to build the UI on? Should I use Streamlit (currently using) or something else? Are there any other new frameworks in the market?

I would appreciate some help on this.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1950nep/d_what_ui_libraryframeworkstack_do_you_just_for_a/,3,3,0.62,<praw.models.comment_forest.CommentForest object at 0x0000026E344EEF90>
19592m6,rem_dreamer,2024-01-12 23:23:14+00:00,[R] Trying to understand the ViTDet paper,"Hi guys, I am writing here because the MachineLearning sub is temporarily closed.  
I'm trying to understand the ViTDet model ([https://arxiv.org/abs/2203.16527](https://arxiv.org/abs/2203.16527)), that uses a ViT backbone and adds a mapping to different resolution levels to perform object detection. However, the whole object detection part is not really explained. I mean, I understand we need some prior knowledge, but I cannot picture how to implement it from the text (the Method section is joke imo).  
If some of you understand this, that would be very helpful!  
Many thanks :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19592m6/r_trying_to_understand_the_vitdet_paper/,2,0,0.43,<praw.models.comment_forest.CommentForest object at 0x0000026E344EE350>
1957nca,AshamedRecover1786,2024-01-12 22:23:48+00:00,PhD stats/ML [Discussion],"Hi everyone, I would like to ask you a question that can be quite stupid, but I am only a bachelor student and I don't know a lot. My question is: can a MSc student in computer science (artificial intelligence track with a focus on Al, machine learning, deep learning...) pursue a PHD in statistics. And do you know what are the best school where pursuing this phd. I saw also some PhD in statistical machine learning in Amsterdam or Oxford, what do you think about that? Actually I am studying at Politecnico of Milan, and probably the next year I will start my Master degree in computer science, artificial intelligence track.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1957nca/phd_statsml_discussion/,2,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E344EEBD0>
19570le,phoneixAdi,2024-01-12 21:57:24+00:00,[D] Summarizing ML/AI Lectures - Would You Find This Helpful?,"Hey everyone!

I'm toying with an idea and really need your input.

I'm planning to take AI/ML lectures and related videos and turn them into easy-to-digest summaries. The goal is to make all that deep and dense information more accessible.

Think of video lectures from MIT/NYU -> Nicely formatted eBook PDFs.

Think of it like getting the essence of a whole each lecture in few paragraphs (to get an overview or as a refresher notes for the lecture).

But first, I really want to know if this is something you'd find useful. And if yes, which specific AI lectures or talks would you want to see summarized? I'm all about making content that’s actually helpful for us here.

So, give me a shout with:

1. Your thoughts on whether quick summaries of AI/ML lectures would be something you'd use.
2. Any particular lectures or videos you've got in mind that need the summary treatment.

Looking forward to hearing from you all!

Cheers,

Adi",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19570le/d_summarizing_mlai_lectures_would_you_find_this/,14,1,0.54,<praw.models.comment_forest.CommentForest object at 0x0000026E34546390>
1955qil,Farther_father,2024-01-12 21:02:42+00:00,Tool/resources for crowd-sourcing annotated audio recordings for Whisper fine-tuning? [D],"I’m looking to fine-tune Whisper on technical (medical) terminology/jargon in a small non-English language.
I was planning to crowd-source a dataset for this task by having my colleagues (healthcare professionals) provide short voice recordings with text annotation.

I half-expected there to be a common/simple tool for this sort of task, but I can’t seem to find it. Ideally, it would be a simple web interface to record microphone input at the press of a button and text from an input box and save it (in Whisper-compatible format would be a plus, e.g. correct sampling rate and auto-truncation of >30 sec recordings). Alternatively, it could just be a local program running on a desktop or Raspberry Pi or even a smartphone app.

So basically, what are the best tools/resources for crowd-sourcing audio+text these days? Cheers Reddit!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1955qil/toolresources_for_crowdsourcing_annotated_audio/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34532250>
194zinx,ZoomerCookie,2024-01-12 16:45:18+00:00,Need help saving preprocessed data to local computer [P],"Hello, I’m a beginner in this field. I’m making a college project on sentiment analysis. As the preprocessing of data takes way too long and I am working on google colab, it’s not practical to preprocess my data everytime. I was looking for a way to save my preprocessed data to my computer.

It would be very helpful if you could list some!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194zinx/need_help_saving_preprocessed_data_to_local/,2,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34532B10>
195jsx0,R3NNUR,2024-01-13 08:54:33+00:00,[D] Your setups in 2024?,"Thinking about changing / upgrading my setup to a combo of a powerful macbook for flexibility and portability. In case I build a model that demands even more power than cloud computing kicks in. 
Anyone doing this in a similar fashion? Or do you stick with strong workstations / your own server for such tasks?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/195jsx0/d_your_setups_in_2024/,7,0,0.19,<praw.models.comment_forest.CommentForest object at 0x0000026E34544410>
194h40f,herodotick,2024-01-12 00:19:05+00:00,Thoughts on Potential of LLMs/Foundation Models for Zero-Shot Time Series Forecasting [D],"Hi all, I've stumbled upon this Neurips paper ""Large Language Models Are Zero-Shot Time Series Forecasters""   [2310.07820.pdf (arxiv.org)](https://arxiv.org/pdf/2310.07820.pdf?trk=public_post_comment-text)  and wonder what people in time series think about it. The paper's authors summarize the method: ""At its core, this method represents the time series as a string of numerical digits, and views time series forecasting as next-token prediction in text"".

The authors seem to show performance nearly matching and sometimes exceeding the standard baseline such as ARIMA on DARTS baseline, with no further training. I wonder what the time series people on here think of these results and whether it's likely that there will be foundation models for time series forecasting that will outperform current specialized forecasting methods. 

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194h40f/thoughts_on_potential_of_llmsfoundation_models/,11,21,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E3453D590>
1945r8k,dr_flint_lockwood,2024-01-11 16:30:30+00:00,Task contamination: LLMs might not be few-shot any more,,MachineLearning,https://arxiv.org/abs/2312.16337,10,74,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E34550410>
1945f0b,APaperADay,2024-01-11 16:16:15+00:00,[R] Learning Long Sequences in Spiking Neural Networks,"**Paper**: [https://arxiv.org/abs/2401.00955](https://arxiv.org/abs/2401.00955)

**Abstract**:

>Spiking neural networks (SNNs) take inspiration from the brain to enable  energy-efficient computations. Since the advent of Transformers, SNNs  have struggled to compete with artificial networks on modern sequential  tasks, as they inherit limitations from recurrent neural networks  (RNNs), with the added challenge of training with non-differentiable  binary spiking activations. However, a recent renewed interest in  efficient alternatives to Transformers has given rise to  state-of-the-art recurrent architectures named state space models  (SSMs). This work systematically investigates, for the first time, the  intersection of state-of-the-art SSMs with SNNs for long-range sequence  modelling. Results suggest that SSM-based SNNs can outperform the  Transformer on all tasks of a well-established long-range sequence  modelling benchmark. It is also shown that SSM-based SNNs can outperform  current state-of-the-art SNNs with fewer parameters on sequential image  classification. Finally, a novel feature mixing layer is introduced,  improving SNN accuracy while challenging assumptions about the role of  binary activations in SNNs. This work paves the way for deploying  powerful SSM-based architectures, such as large language models, to  neuromorphic hardware for energy-efficient long-range sequence  modelling.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1945f0b/r_learning_long_sequences_in_spiking_neural/,6,42,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E345531D0>
1945swj,APaperADay,2024-01-11 16:32:30+00:00,"[R] ""Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs"" (DiagGSM8K)","**Paper**: [https://arxiv.org/abs/2312.17080](https://arxiv.org/abs/2312.17080)

**Code**: [https://github.com/dvlab-research/DiagGSM8K](https://github.com/dvlab-research/DiagGSM8K)

**Dataset**: [https://huggingface.co/datasets/Randolphzeng/DiagGSM8K](https://huggingface.co/datasets/Randolphzeng/DiagGSM8K)

**Abstract**:

>In this work, we introduce a novel evaluation paradigm for Large  Language Models, one that challenges them to engage in meta-reasoning.  This approach addresses critical shortcomings in existing math  problem-solving benchmarks, traditionally used to evaluate the cognitive  capabilities of agents. Our paradigm shifts the focus from  result-oriented assessments, which often overlook the reasoning process,  to a more holistic evaluation that effectively differentiates the  cognitive capabilities among models. For example, in our benchmark,  GPT-4 demonstrates a performance ten times more accurate than GPT3-5.  The significance of this new paradigm lies in its ability to reveal  potential cognitive deficiencies in LLMs that current benchmarks, such  as GSM8K, fail to uncover due to their saturation and lack of effective  differentiation among varying reasoning abilities. Our comprehensive  analysis includes several state-of-the-art math models from both  open-source and closed-source communities, uncovering fundamental  deficiencies in their training and evaluation approaches. This paper not  only advocates for a paradigm shift in the assessment of LLMs but also  contributes to the ongoing discourse on the trajectory towards  Artificial General Intelligence (AGI). By promoting the adoption of  meta-reasoning evaluation methods similar to ours, we aim to facilitate a  more accurate assessment of the true cognitive abilities of LLMs.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1945swj/r_challenge_llms_to_reason_about_reasoning_a/,0,25,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E3453CBD0>
194riva,prayag_p,2024-01-12 09:59:40+00:00,Solar Panel Defects with PicStork AI [R] - Thermal UAV Images,"Solar Panel are prone to various defects such as micro crack , hot spots , shading , defective diode such defects are crucial which reduces the maximum output and overall energy generation detecting such defects as became easy using uav and AI technology . Without even writing a single line of code one can use PicStork to detect such defects here's the link for video demonstration   
[https://aeromegh.com/defect-detection/](https://aeromegh.com/defect-detection/)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194riva/solar_panel_defects_with_picstork_ai_r_thermal/,0,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E34547A90>
193zpyi,michaelmalak,2024-01-11 11:39:39+00:00,[D] Does patent lawsuit against Google's TPU imperil bfloat16 and processors (e.g. NVIDIA) that use it?,"I added a section to the Wikipedia article on TPU. https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Lawsuit

But I speculate whether if Singular Computing prevails in their lawsuit, all uses of bfloat16, including by NVIDIA, would be imperiled? Such speculation (independent of coming from reliable sources) is considered ""original research"" by Wikipedia standards, so I could not include it there.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193zpyi/d_does_patent_lawsuit_against_googles_tpu_imperil/,33,48,0.85,<praw.models.comment_forest.CommentForest object at 0x0000026E3456D850>
194cts9,SemperZero,2024-01-11 21:20:13+00:00,What are some good advanced level platforms to learn from?,"Hey. I'm 27 and I think I got most of the basics for ML. I'm very good at math, I understand statistics and probability quite deep, worked on research projects by myself, for which I had to build models on my own. Not really complex, but still requiring creativity and a good understanding of basic concepts. 

I will soon start a data science job at a FAANG company and I want to further improve my skills and use their resources to the fullest, but I'm not really sure where to go from here in terms of learning. Could you help me with some more advanced materials/forums for ML research/place with good papers/place with good articles? 

I'd also like to study the very best and see the way they code and explain advanced concepts (like Andrej Karpathy) where can I find them?? is there a Twitch for challenger level AI researchers streaming live processes? Or videos showing the entire project flow (how they do data visualizations, mining, choosing models, tuning, etc) like top digital artists show the highlights or the entire speed-up of their painting processes?

Here's a list all of my projects to get a general idea of my level and where I'm at:

* calculating the distance between hundreds of 42.000 feature objects  (containing categorical, strings, numbers, hashes, booleans as variables) and then clustering. with some vector processing and a neural network
* implemented from scratch in C some models like ARIMA (together with linear regression)
* combining a FFT with a neural network for a 42d wave classification
* T-SNE to split dataset into 2d grids -> Kullback–Leibler on grids for distance -> DBSCAN/KMEANS for clustering
* genetic algorithms for hyperparameter optimizations and reinforcement learning (neuro evolution)
* DBSCAN -> Levenberg-Marquardt for polynomial coefficients-> neural network predicting the coefficients based on different parameters
* playing with instance segmentation and some algorithms to synchronize a color and a depth camera
* simulations/statistics/probabilities for video games
* a lot of visualizations and data mining for patterns

As you can see there is no LLM/ Generative AI/ Computer Vision stuff, which I would like to get into. 

I'm also not 100% sure what else would be nice to learn in general. I know most of the basic procedures for training, balancing datasets, avoid overfit, computing error plots, comparing models, etc and I'm familiar with most of math (not insanely advanced) used in ML.

I didn't read many papers, but holy ... most of them are so unreadable and filled with pompous nonsense that 99% of the effort is de-obfuscating the bs and reading for so long just to figure out how the input is encoded, what's the output, and what's the model. Where can I find good, readable, structured papers which are actually on point?

I'm from Eastern Europe and most of my learning has been done by my self after high school, the education quality is close to zero in the universities here and I never had any mentors at the jobs I worked. There's no research in this country, and getting to work on these projects was insanely hard, some of them being done in my free time or for free just to get experience... Fortunately after a lot of hard work I got into FAANG, and I hope things will be better here. Most of what I've learned has been from very fragmented places on the internet, and now I'm looking for centralized places and communities of top quality content.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194cts9/what_are_some_good_advanced_level_platforms_to/,5,7,0.71,<praw.models.comment_forest.CommentForest object at 0x0000026E3456B3D0>
194n8gy,easydoozeit,2024-01-12 05:24:12+00:00,[P] Dataset of all names people use in AI Art Generators,"Created a dataset with all the names people use in Art Generators.  
Around 9k names.

Fantasy characters, celebs, artists, characters or made up names to help guide consistency/localization.

Whats our thoughts on if these people will sue or should be given revenue share?  


[https://netwrck.com/blog/names-used-in-ai-art-generation](https://netwrck.com/blog/names-used-in-ai-art-generation)  


Let me know if this helps anyone :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194n8gy/p_dataset_of_all_names_people_use_in_ai_art/,0,2,0.56,<praw.models.comment_forest.CommentForest object at 0x0000026E34555490>
194soet,chaitu9701,2024-01-12 11:15:39+00:00,[D] Question answering without llm,"[D] My usecase is to build a QA app without using llm's. I'm familiar with transformers(the hugging face library) but it always returns short form answers. But I was looking for some long form answers, atleast the complete sentence. Can anyone suggest approaches or papers with code to solve the approach. 

Tldr: How was qa task achieved in the era before llm to fetch long answers.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194soet/d_question_answering_without_llm/,3,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E331AA590>
1943c4q,TerryCrewsHasacrew,2024-01-11 14:44:44+00:00,"[P] Cudacanvas, a simple pytorch cuda tensor visualisation tool to avoid CPU transfer","We also uploaded it to pypi for simpler installation, One of the biggest pain point for us was always the fact that we couldn't visaulise diffusion images in real time whilst training, so this eliminate this issue for us

Github : [https://github.com/OutofAi/cudacanvas](https://github.com/OutofAi/cudacanvas)

&#x200B;

https://preview.redd.it/es3r859cptbc1.png?width=1002&format=png&auto=webp&s=e4720e1a50b512f61ee626b3ceaa00222395a0d0",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1943c4q/p_cudacanvas_a_simple_pytorch_cuda_tensor/,7,14,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E331FA210>
194uz9i,StellaarMonkey,2024-01-12 13:24:35+00:00,"[D] Invalid loss, terminating training","My program trains for the first few eopchs, but the I get this? I am using MSLE, what could be the cause?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194uz9i/d_invalid_loss_terminating_training/,1,0,0.13,<praw.models.comment_forest.CommentForest object at 0x0000026E34593650>
194l01q,Ozay0900,2024-01-12 03:23:36+00:00,finding good parameters with cross validation [D],"Hi, me and another guy argued about the usage of cross validation. We got the task to ""Train and evaluate a classifier"" for example knn. He said that that means doing a train test split, and then just try different k's and get the k with the best accuracy score/r2 score/... we found and then use cross validation with the found k to see how it performs. I however learned that cross validation should be used to find a good k itself buy running it with different k's. We argued a lot and he says im wrong and it is just used to see the performance. But isn't my method litteraly called grid cv search ? I don't want to seem like a dumbass because he insisted so much on it that i am not sure anymore",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194l01q/finding_good_parameters_with_cross_validation_d/,5,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34578410>
194k2gy,No-One6118,2024-01-12 02:36:53+00:00,[Project]Got stuck in my minor project,"Hello guys, first of all I am truly grateful to this community to bring that kind of discussion. 

I need you guys help, basically I want to build a dynamic (real time) prediction model for spam detection for normal message application. But I 
can't figure out how can I build one. I am able to build static application which get learn on limited data, and based on that it predicts. 

Tell me you guys if you have some knowledge about how can I make a real time learning(active learning) model in this field.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194k2gy/projectgot_stuck_in_my_minor_project/,1,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E331F9650>
194dpc5,tatteredsky,2024-01-11 21:56:14+00:00,[D] Master's Thesis project in Adversarial Machine Learning,"Hello people. I am a second year Master's student, and about to embark on my thesis journey this year. I am particularly interested in adversarial ml and trustworthy ml as well. I do have a research experience with CNNs, heterogeneous computing, and genetic algorithms, but due to the lack of resources and people doing research at my university, I have not been able to dive deep into research in my desired areas in ML. I do plan to pursue a PhD in computer science focusing on adversarial and trustworthy learning after this. 

I am mostly self taught in the area of adversarial learning, and have read several papers on it over the past year. 

I have a few ideas of potential topics, but I am quite indecisive about them. Without the semester starting, I am not able to consult my advisor about topics but would like input from others before I consult him.

Some of my ideas:

\* Impact of adversarial attacks on AI fairness - Explore whether adversarial attacks exacerbate existing biases in datasets and models or introduce new types of biases and could contribute to more equitable AI systems. 

\* Real-world effectiveness of adversarial attacks and defenses - effectiveness of attacks and defenses in real-world settings by simulating practical applications and highlighting the gaps between theoretical robustness and practical effectiveness. 

If there are any other ideas, or any improvements/additions to these ideas, please let me know. Thank you!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194dpc5/d_masters_thesis_project_in_adversarial_machine/,4,2,0.66,<praw.models.comment_forest.CommentForest object at 0x0000026E345D8650>
194ddhx,yarikbratashchuk,2024-01-11 21:42:59+00:00,[D] Hybrid search question,"Hello friends,   


I'm playing with hybrid search approach and embed images to dense vectors. And now I'm think of ways to use few images for single product entry. What can you recommend? Are there any other options than separate image search and semantic search?  
",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194ddhx/d_hybrid_search_question/,0,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E330AD250>
1944iz8,vatsadev,2024-01-11 15:37:46+00:00,"[P] In most Multimodal LLMs, where are the image embeddings given to the model?","I have a colab notebook with a super simple andrej karpahy GPT (https://colab.research.google.com/drive/17j0xI5n-wRK3c6BQagCEbw38EJ39M7G3?usp=sharing), and I wanted to try adding a ViT/Clip/Fuyu style embedding to it.

ViT/Clip, I would need the entire clip model, which is anywhere from 30x to 5x my transformer size, so its harder to pick Fuyu, from what I've found, runs image patches through an MLP, which is way smaller, but im not sure where the embeddings go

How do I replace tokens with embeddings?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1944iz8/p_in_most_multimodal_llms_where_are_the_image/,2,7,0.89,<praw.models.comment_forest.CommentForest object at 0x0000026E344AB0D0>
194rrzk,Able_Beautiful_7362,2024-01-12 10:16:26+00:00,Are these specs enough for ML and DL? [D],"

Hello,  
GPU:  Geforce RTX3060 12gb  
CPU: Intel i5-12400f  
MB: MSI pro b760M-P DDR5  
Ram: 32gb DDR5 4800MHZ  
PO: PSU FSP PNR 600W  
Storage: M.2 NVME 512gb + HDD 6TB   

I just started my learning journey and I'm currently at the data  collection (through webscrapping) stage but I would like to be able to  train ml on text/image data also, it's not for big data just a few  hundred million rows, and let's say 100-200gb files.  
Quick side  information I'm from a relatively poor country so I can't really afford  the $2-5k builds and this build would cost $1k.   

What are your thoughts?   ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194rrzk/are_these_specs_enough_for_ml_and_dl_d/,9,0,0.37,<praw.models.comment_forest.CommentForest object at 0x0000026E345DB910>
19488q4,qalis,2024-01-11 18:10:52+00:00,[D] Graphormer graph connectivity question,"I am reading through graph transformers papers and many (or most) of those ignore graph structure and instead rely on node structural encodings. This, combined with attention (each-node-with-each-node), as far as I understand, is equal to using a full graph, and treating nodes as a set. Is that correct?

Graphormer paper for reference: [https://arxiv.org/pdf/2106.05234.pdf](https://arxiv.org/pdf/2106.05234.pdf).",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19488q4/d_graphormer_graph_connectivity_question/,1,4,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E34576B10>
193qeup,Reference-Guilty,2024-01-11 02:21:08+00:00,[D] Best ML tracking tool to monitor LIVE a pytorch model ?,"Hello,

I want to fine tune the hyperparameters of my model and I'm looking to implement a ML tracking tool such as MLflow to keep track of my models performance.

However, each training is \~8 hours and it would be interesting to watch the metrics evolve live during the training i.e watching the loss curve grow etc

I'm really new to that part of the pipeline so I don't know which are the best tools for that yet

Is it possible to do so with MLflow ? I've implemented it but it seems to only show the graphs and plots -after- the training script is done

If not possible with MLflow can you guys advise me to the best package for that ?

The setup I have in mind after researching the topic is Hydra + MLflow + Optuna, if you guys have a a more experienced point of view on that question I'm happy to hear about it :)

Thanks a lot!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193qeup/d_best_ml_tracking_tool_to_monitor_live_a_pytorch/,29,48,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E34600D10>
194dyjg,neuralnips,2024-01-11 22:06:21+00:00,"[D] ML PhD careers which improve society -- research, teaching, or applications?","I’m currently a third-year PhD student studying computer science at a large R1 university in the US (my program typically takes five years). My research is focused on lifelong/continual machine learning, which is a subfield without many direct applications (at least so far). I’m pursuing a PhD for three main reasons: (1) I really enjoy research and deeply understanding things, (2) I didn’t want to work as a software engineer immediately after undergrad, and (3) I don’t have student loans from undergrad, so I could afford to live off the PhD stipend. **I’m wondering what I should do after finishing my PhD, and I would appreciate any advice or personal anecdotes, especially related to lesser-known/unconventional career paths.**

As corny as it sounds, I would like a career which makes the world a better place. In addition, I’m not interested in hyper-competitive jobs and/or jobs with poor work-life balance (i.e. R1 university professor, research scientist at big tech, etc). Even if I was interested in these types of jobs, I honestly don’t think I could get them given my university’s rank and my publication record (by graduation I’m aiming for 3-4 first author papers in a mix of top conferences, IEEE journals, and niche conferences like CoLLAs). I live a relatively frugal lifestyle and don’t care about making a ton of money. I’d much prefer a relatively low-paying job that I enjoy over a high-paying job that I hate. Finally, I think I’d enjoy teaching — I’ve led several workshops at hackathons and taught guest lectures for my advisor, though I’ve never actually TA-ed. 

My ideal job would be “university professor without the stress”. Obviously this job does not exist because of the publish-or-perish culture of academia, the necessity of securing funding, dealing with annoying students, etc. I think I enjoy the beauty of math and CS more than I enjoy building useful things, though this probably isn’t practical for most jobs. 

Here are a few more realistic options I’ve been considering:

1. **Working on “AI for good”** i.e. using machine learning for medicine, scientific discovery, ecological conservation, etc. This could be as a machine learning engineer, data scientist, or research scientist at any relevant company. There are lots of companies doing this, from start-ups to household brands. I’m aware that companies like Meta and ByteDance have teams that work on things like drug discovery, but again I don’t think I’d be a competitive applicant to these jobs. I am not particularly passionate about any one specific problem (though fighting climate changing and mitigating pandemics seem like pressing issues). I would be open to specific company suggestions here and/or advice from people who have worked on such problems. 
2. **Teaching at a university**, either at an R1 as a teaching professor (there are a handful at my university), at a smaller liberal arts college, or at a community college. I think (good) teaching is one of the things that adds the most value to society, albeit at a meta-level. Also, I think that a lot of teaching is so bad that people don’t realize how much better it could be. I’ve even thought about teaching high school math, though I’d be simultaneously overqualified and underqualified for that, among other things… I think I’d enjoy coming up with interesting and well-motivated ways to present material, interacting with students, and deepening my understanding of course topics. I enjoy public speaking and teaching my friends about cool stuff I’ve learned. However, I’m worried that (1) I don’t actually have that much teaching experience, (2) teaching may quickly get boring or feel stale, (3) teaching professors are overworked and underpaid/underappreciated, and (4) education will dramatically change in the next few decades. I’m interested in hearing from people who have gone this route. 
3. **Trying to find a remote job where I only have to work a few hours a day**. This is a quasi-fallback plan if the other options do not work out and/or I don’t find fulfillment in my work, in which case I want free time to do things I enjoy. I have a few friends in software engineering who do this and are happy, though I’m not sure how sustainable this is. Also curious if anyone here has a job like this.

I’m considering working in industry for a while, then trying to become a teaching professor. I’m enjoying my PhD so far, though I’d be open to mastering out if I start to hate it and don’t feel it is necessary for my career plans. Also, I’m a bit concerned that the CS (specifically ML) PhD job market is becoming oversaturated, and I’m unsure what I can do to stand out.

**Again, I’m open to advice and people’s personal experience, as well as predictions about the future ML job market.**

I’ve seen the related posts below, but thought I’d make my own as my situation differs:

* [https://www.reddit.com/r/MachineLearning/comments/v1xepz/d\_machine\_learning\_for\_good/](https://www.reddit.com/r/MachineLearning/comments/v1xepz/d_machine_learning_for_good/)
* [https://www.reddit.com/r/MachineLearning/comments/wwdqp8/d\_ml\_for\_good/](https://www.reddit.com/r/MachineLearning/comments/wwdqp8/d_ml_for_good/)
* [https://www.reddit.com/r/MachineLearning/comments/lsnphe/d\_career\_optionsadvice\_with\_ml\_phd/](https://www.reddit.com/r/MachineLearning/comments/lsnphe/d_career_optionsadvice_with_ml_phd/) 
* [https://www.reddit.com/r/MachineLearning/comments/nnpqxd/d\_is\_a\_phd\_in\_ml\_worth\_it\_if\_one\_doesnt\_plan\_on/](https://www.reddit.com/r/MachineLearning/comments/nnpqxd/d_is_a_phd_in_ml_worth_it_if_one_doesnt_plan_on/) ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194dyjg/d_ml_phd_careers_which_improve_society_research/,4,2,0.56,<praw.models.comment_forest.CommentForest object at 0x0000026E345F5A10>
1940dez,Full_Sentence_3678,2024-01-11 12:17:07+00:00,[P] ML copilot - chat with ML papers and code,"Hi all,

Just sharing a an ML copilot I’ve been working on in spare time: [https://mlcopilot.dev/](https://mlcopilot.dev/)

You can chat with it about papers and code repositories that you can link via arxiv or github.

Let me know your thoughts, and if there’s any other feature ideas you have for the site,

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1940dez/p_ml_copilot_chat_with_ml_papers_and_code/,1,4,0.84,<praw.models.comment_forest.CommentForest object at 0x0000026E345F8D10>
194qjhh,rejectedlesbian,2024-01-12 08:51:23+00:00,[D] Preformance papers rhay don't mention hardware,"Why Do people think its okay to write a preformance paper about their algorithem and not even mention if it's cpu or gpu? Or just general details about the code and hardware...

U can easily get 2x diffrence just by libarary selection on specific hardwares if you tried enough so this seems very inappropriate but relatively common which I just don't get.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194qjhh/d_preformance_papers_rhay_dont_mention_hardware/,17,0,0.36,<praw.models.comment_forest.CommentForest object at 0x0000026E3460EF50>
1944r43,Ok-Equipment9840,2024-01-11 15:47:30+00:00,[D] PhD in computer vision applied to 3D medical image reconstruction ?,"Hello,

I am thinking about doing a PhD in computer vision applied to biology and I was wondering whether this is limiting for a career in ML later on. Basically, is the fact that the PhD is really niche and problem for working on NLP later on after the PhD.
And can you publish in any conference or only those related to biology?

The PhD would be in a research lab in Paris.
I come from an Applied Maths and Machine Learning background.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1944r43/d_phd_in_computer_vision_applied_to_3d_medical/,5,2,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E345EA750>
194uggm,Dogemuskelon,2024-01-12 12:58:55+00:00,"[D] Please help me get remote ML or Data Science Internship, Please.","Hi, can anyone help me get just normal data science internship (remote preferably), I know basic ML and deep learning, other than that I know tableau, and basic data analysis using pandas, and many other python libraries like numpy, seaborn, etc. 
I have basic knowledge regarding Data Structures too.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194uggm/d_please_help_me_get_remote_ml_or_data_science/,6,0,0.14,<praw.models.comment_forest.CommentForest object at 0x0000026E34608590>
193t4w8,activescott,2024-01-11 04:38:40+00:00,[D] Anyone Tried a Tesla P100 for Fine-Tuning LLMs?,"I recently created a tool to track price/performance ratios for GPUs. I was surprised to see that NVIDIA Tesla P100 ranks surprisingly high on [$/FP16 TFLOPs](https://coinpoet.com/ml/learn/gpu/ranking/fp16-flops) and [$/FP32 TFLOPs](https://coinpoet.com/ml/learn/gpu/ranking/fp32-flops), despite not even having tensor cores. Just curious if anyone has attempted to use it for fine tuning LLMs or other neural networks for training purposes and can comment on its performance compared to other GPUs and their cost.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193t4w8/d_anyone_tried_a_tesla_p100_for_finetuning_llms/,12,15,0.89,<praw.models.comment_forest.CommentForest object at 0x0000026E3460FDD0>
193s7dq,newperson77777777,2024-01-11 03:49:56+00:00,[D] Knowledge Graph Extraction from Unstructured Medical Texts,"I'm trying to generate a knowledge graph from a set of medical articles. My prior approach was to use entity recognition/linker library like [https://allenai.github.io/scispacy/](https://allenai.github.io/scispacy/) and a zero-shot relation extractor like [https://github.com/fractalego/zero-shot-relation-extractor](https://github.com/fractalego/zero-shot-relation-extractor). However, the entity-linking metrics aren't great (as can be seen in the mentioned webpage) and the zero-shot relation extractor tends to produce a lot of noisy relations, especially if multiple relation types are passed.  


Does anyone have some good suggestions for knowledge graph extraction techniques which are more effective? My advisor suggested that we can use LLMs to generate the knowledge graph but I'm not sure which LLMs to use and if there are any published metrics for them. Ideally, I would want to avoid having to validate several LLMs on my own and use a relatively popular robust method which is easy to use.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193s7dq/d_knowledge_graph_extraction_from_unstructured/,11,15,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E34616510>
1943cfc,Junior-Bookkeeper-24,2024-01-11 14:45:04+00:00,[D] Best vision journal to submit an extended conference paper?,My paper was accepted at CVPR and then we submitted to PAMI but it got rejected for random reasons. What would be a good journal to submit to?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1943cfc/d_best_vision_journal_to_submit_an_extended/,2,2,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E34619D10>
193usfg,HappyDataGuy,2024-01-11 06:11:37+00:00,[D] [RAG] [llama-index] How to execute multiple SQL queries with SQLTableRetrieverQueryEngine in NL2SQL project?,"I am working on a project where user will ask natural language queries and this llama-index based engine will convert that natural language to sql query and execute it on my database and give answer in natural language to the user. Problem is it is only able to execute one query per question so comparison quetions are not possible to answer and also if a question does not require querying the database it will still query the database. How can I solve this. Please help me with your suggesting.   
Thanks in advance. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193usfg/d_rag_llamaindex_how_to_execute_multiple_sql/,8,8,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E3461D050>
1945n6i,perceptron333,2024-01-11 16:25:47+00:00,[D] How to request to be a reviewer to a conference/journal?,"I'm interested in reviewing for the upcoming cycles of ECCV, Neurips, ICLR, AAAI etc. 

Would also like to review for journals like T-PAMI etc. 

How does one go about this? Should I just email the editor of the journal or conference or is there a better way of doing it?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1945n6i/d_how_to_request_to_be_a_reviewer_to_a/,2,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E3461D210>
194d085,SemperZero,2024-01-11 21:28:03+00:00,[D] What are some good advanced platforms?,"Hey. I'm 27 and I think I got most of the basics for ML. I'm very good at math, I understand statistics and probability quite deep, worked on research projects by myself, for which I had to build models on my own. Not really complex, but still requiring creativity and a good understanding of basic concepts.

I will soon start a data science job at a FAANG company and I want to further improve my skills and use their resources to the fullest, but I'm not really sure where to go from here in terms of learning. Could you help me with some more advanced materials/forums for ML research/place with good papers/place with good articles?

I'd also like to study the very best and see the way they code and explain advanced concepts (like Andrej Karpathy) where can I find them?? is there a Twitch for challenger level AI researchers streaming live processes? Or videos showing the entire project flow (how they do data visualizations, mining, choosing models, tuning, etc) like top digital artists show the highlights or the entire speed-up of their painting processes?

Here's a list all of my projects to get a general idea of my level and where I'm at:

* calculating the distance between hundreds of 42.000 feature objects (containing categorical, strings, numbers, hashes, booleans as variables) and then clustering. with some vector processing and a neural network
* implemented from scratch in C some models like ARIMA (together with linear regression)
* combining a FFT with a neural network for a 42d wave classification
* T-SNE to split dataset into 2d grids -> Kullback–Leibler on grids for distance -> DBSCAN/KMEANS for clustering
* genetic algorithms for hyperparameter optimizations and reinforcement learning (neuro evolution)
* DBSCAN -> Levenberg-Marquardt for polynomial coefficients-> neural network predicting the coefficients based on different parameters
* playing with instance segmentation and some algorithms to synchronize a color and a depth camera
* simulations/statistics/probabilities for video games
* a lot of visualizations and data mining for patterns

As you can see there is no LLM/ Generative AI/ Computer Vision stuff, which I would like to get into.

I'm also not 100% sure what else would be nice to learn in general. I know most of the basic procedures for training, balancing datasets, avoid overfit, computing error plots, comparing models, etc and I'm familiar with most of math (not insanely advanced) used in ML.

I didn't read many papers, but holy ... most of them are so unreadable and filled with pompous nonsense that 99% of the effort is de-obfuscating the bs and reading for so long just to figure out how the input is encoded, what's the output, and what's the model. Where can I find good, readable, structured papers which are actually on point?

I'm from Eastern Europe and most of my learning has been done by my self after high school, the education quality is close to zero in the universities here and I never had any mentors at the jobs I worked. There's no research in this country, and getting to work on these projects was insanely hard, some of them being done in my free time or for free just to get experience... Fortunately after a lot of hard work I got into FAANG, and I hope things will be better here. Most of what I've learned has been from very fragmented places on the internet, and now I'm looking for centralized places and communities of top quality content.

TL;DR: sorry for the long rambling. had to order my thoughts and figure what i actually want: Looking for top tier AI researchers showcasing their work processes, places with clear papers/articles, tips for someone who's no longer a very beginner, and other communities like this.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194d085/d_what_are_some_good_advanced_platforms/,1,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E34617C90>
194oc4p,AI_Trader1985,2024-01-12 06:26:20+00:00,Linear Regression using TensorFlow [Project],"Created a project where people can calculate linear regression using ML techniques.

Linear Regression is vital when we have linear correlations in our dataset. Linear correlations are present when two or more variables exhibit a linear form. For example – a dataset encompassing the people's happiness levels against socio-economic parameters. It's important to plot linear regression accurately in order to understand underlying trends.

I've used TensorFlow as the machine learning library to train and create a neural network from training data. We can then access the neural network via an API (.NET 6) to obtain output - either a visual graph or the coefficient and interceptor parameters of the curve (y = mx + c).

More details are explained in here. [https://eranda.wordpress.com/2022/06/03/my-own-api-for-linear-regression-services-egal/](https://eranda.wordpress.com/2022/06/03/my-own-api-for-linear-regression-services-egal/)

Feel free to leave a feedback. Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194oc4p/linear_regression_using_tensorflow_project/,7,0,0.21,<praw.models.comment_forest.CommentForest object at 0x0000026E3462F0D0>
193ylih,Saarstriker,2024-01-11 10:28:29+00:00,[D][P] How to combine aligned embeddings for cosine similarity search?,"Hey!   


I am currently working on a project for retrieving similar images via Text or Images.   


I am using BLIP for the embeddings and this works well. So i embedded all my images for a DB (1, 256), and when doing a search i am embedding the search query (which is either a Text or an Image) into the same space and am using cosine similarity. This approach works well and easy.   


Now i want to look into combining image and text search, for example the query image is a white t-shirt and the text is 'green', should retrieve images of green t-shirts.   


However, i am quite unsure how i can combine/fuse the embeddings properly. Both embeddings are in the same space, so i do not have to 'align' them. Only a combination is necessary.   


I tried basic methods (addition, average, taking max/min, etc.) with minimal success.   
It would be great if there is a way to fuse the embeddings. Would also help to potentially add more embeddings (apart from image and text) later on.  
I also though of creating a FC layer with both embeddings as input, but the problem is that i do not really have training data for this, and am not sure how to properly create it.   
   
Do you have any ideas on a suitable approach? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193ylih/dp_how_to_combine_aligned_embeddings_for_cosine/,2,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E34627690>
1942aqu,Hugejiji,2024-01-11 13:57:03+00:00,Seeking Advice: Considering a Second RTX 3090 for NVLink SLI [D],"Hey, I'm looking to buy a second RTX 3090 for running them via NVLink. I currently have an MSI 3090 Gaming X Trio. I can't find a good second used one on eBay; most of them are kind of overpriced. So, I thought about buying an EVGA 3090 FTW3 ULTRA. Is there a way to check if I can potentially run them via SLI? I've read multiple times that the SLI connector is not standardized and therefore may be off by a few millimeters.

&#x200B;

My question is, is there any site or way to check this specifically? In the manuals, I can't find any information regarding the dimensions and position of the SLI connector.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1942aqu/seeking_advice_considering_a_second_rtx_3090_for/,2,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34580F10>
1941dtr,Trungyaphets,2024-01-11 13:11:53+00:00,[P] Any good and simple ML model to predict new products' weights?,"Hello guys. I'm new to machine learning and got tasked with predicting/estimating our new products' weights. I have past data (300k records) of product images, their categories (2000 categories), unit prices and weights from past orders. Accuracy doesn't have to be too high, probably 70% of predictions with errors below 30% of truth values is fine. Currently from the categories and unit prices I could estimate weights with 60% of estimations within +- 40% of the real values. 

Thinking about using images to find like 10 past products similar to each of the new one and take their average weight.

My questions:
- Would the approach above work? Is it possible to do in 1-2 weeks? Any suggestions/pointers on which model to use?
- Could you please help point me to any similar project?

Thank you very much! Ask me any questions if needed.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1941dtr/p_any_good_and_simple_ml_model_to_predict_new/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E345815D0>
1945rvg,emaxwell13131313,2024-01-11 16:31:15+00:00,"For those who work in ML and/or Data Science, what are your current go to techniques and methods for preparing data for analysis [D]?"," When it comes to cleaning, scaling, changing data representation, preprocessing and any other aspects for preparing data for analysis and/or ML, what techniques, mathematical models, perhaps based on linear algebra or other such facets, libraries and/or other tools are your favorites for making sure data is fully cleaned, processed, prepped and ready for analysis? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1945rvg/for_those_who_work_in_ml_andor_data_science_what/,5,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E34581390>
193ukh8,boredmonki,2024-01-11 05:59:43+00:00,[D] Managing Analytics Teams,"Hola Amigos,

My   team is considering a revision in ways of working for our Analytics   projects (Simple and ML based). Want to understand from you all, how do   you people in analytics work?

It's   agile? It's scrum? Your stand up includes Kanban, there's a new tool  in  the market. How do you people plan and execute your projects.

Throw everything at me, and help our team to understand how the rest of the market is working.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193ukh8/d_managing_analytics_teams/,2,3,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E3462F790>
193672o,Benni03155,2024-01-10 11:30:20+00:00,[D] Best Time Series models for Forecasting (alternative to TimeGPT)?,"I've recently discovered [TimeGPT](https://docs.nixtla.io/) and its really great at demand forecasting.

I am not very good with pytorch but I couldn't achieve anything even close to the results of TimeGPT.

I am now looking for similar (or even better?) models which perform really well for forecasting data (in my case demand forecasting).

Thanks ahead for your suggestions!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193672o/d_best_time_series_models_for_forecasting/,35,87,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E34658410>
193jq7k,Charlieputhfan,2024-01-10 21:28:53+00:00,[D] ML Algorithms for Time series classification and peak counting,"I'm currently working on a project that involves processing realtime accelerometer,Gyroscope, orientation data from wearable for gym exercises, which I need to classify and count the peaks , which correspond to reps. I have a few questions regarding the best technique to do this. I've read some research on this and trying to replicate the papers with best accuracies attached below.

I am using XGBoost to classify between the exercises with input being the time series data of all the sensors, this performs pretty well with 99% for some easy to classify exercises, and 92% on some difficult ones. When I initially tried this with ANN with two layers, it's accuracy was pretty bad, maybe because of the fact that I don't have much data at the moment. But **Xgboost** worked pretty well. 

**Q-** What should be the best approach for this time series classification?, (assuming I also do some more feature extraction like statistical features and FFTs, Kurtosis, which seem to increase the precision and recall as suggested by the research)

One another doubt that I had was how should I go about counting the reps/peaks in the data, which can be noisy at times, for exercises which have only acceleration component and no rotation of wrist, like overhead press. For simple exercises, its easy enough with some signal processing, smoothen the signal and check the turning points. But for the the difficult ones, the signals can have a lot of other noise and peaks, which can be hard to distinguish. Many papers suggest, using some thresholding for the time axis, where you only consider peaks with a certain time difference and similar things for amplitude. One paper uses this and the authors achieved a median of 95-96% accuracy, but it varied based on axis and other factors choosed.

**Q-** But I was wondering if there can be a machine learning model that can take the input feature signal and output the number of reps , how difficult this would be ? I tried this using a LSTM regressor with some 300 sets of jumping jacks, which seemed the easiest to count the reps for and it wasn't able to learn anything and overfitted badly.

Would appreciate inputs on this problem and the best ways to optimize accuracy.

Reference Papers:

[https://www.mdpi.com/1424-8220/23/10/4602](https://www.mdpi.com/1424-8220/23/10/4602)

[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6387025/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6387025/)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193jq7k/d_ml_algorithms_for_time_series_classification/,6,10,0.82,<praw.models.comment_forest.CommentForest object at 0x0000026E34644050>
193y6ig,Cautious-Sherbet-107,2024-01-11 10:00:35+00:00,[D][P] About prompt designation with using Gemini,"I am trying to create a LMM to reply question with using knowledge in books, Gemini is one of the better option since it allow you to create LLM with more than 1 prompt. It was fine when start but after a few days, it start to ignore the content in prompt and generate contents like ChatGPT.  Is it the limit of Gemini or did I do something wrong with the prompts?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193y6ig/dp_about_prompt_designation_with_using_gemini/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34636810>
193gug2,MaintenanceNo5993,2024-01-10 19:32:32+00:00,[D] Fine Tuning Open CLIP model causes it zero shot accuracy to drastically drop after 1st epoch,"I was fine tuning CLIP `(model_name='ViT-B-32', pretrained='laion2b_s34b_b79k)` on MSCOCO 2017 caption dataset using code from https://github.com/mlfoundations/open_clip/tree/main/src/training but I don't know why even after epoch 1 the zero shot accuracy on ImageNetV2 drops from 58.11% to 0.1% and gets stuck on this.

Any possible causes?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193gug2/d_fine_tuning_open_clip_model_causes_it_zero_shot/,13,13,0.85,<praw.models.comment_forest.CommentForest object at 0x0000026E3463F990>
193wtcq,Ok_Potential_6886,2024-01-11 08:24:19+00:00,[D] NVLink 2 RTX 3090 cards,"
I want to nvlink 2 nvidia RTX 3090 cards with memory pooling enabled. if it is possible to do,  what motherboard model, power supply and nvlink connector should I use? Will I be able to train large models with 48GB vram ?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193wtcq/d_nvlink_2_rtx_3090_cards/,3,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34659490>
194fr4m,BoogieManKn,2024-01-11 23:20:23+00:00,Does NLP have a future? [Discussion],"Hi, I am a fresh graduate student with a bachelor degree in computer engineering, and my main interest is around NLP.

I have heard a lot of people talking and debating about the future of NLP, whether it is dying or it has a lot more to offer.

My opinion on this one is that a paper published in 2018 changed the world in a way that even its publishers could not think of. The advent of chatgpt was the first consumer product that utilized the Transformers and their abilities in 3 years after the paper, and it did truly change the world. My pov in here is that if something can change the world in 3 years, we have not gone too far yet, and it has a lot more room to cover and a lot to discover and learn about.

I would love to hear about your povs in here.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194fr4m/does_nlp_have_a_future_discussion/,18,0,0.21,<praw.models.comment_forest.CommentForest object at 0x0000026E34673D50>
1932cv6,APaperADay,2024-01-10 07:07:08+00:00,[R] AdamL: A fast adaptive gradient method incorporating loss function,"**Paper**: [https://arxiv.org/abs/2312.15295](https://arxiv.org/abs/2312.15295)

**Abstract**:

>Adaptive first-order optimizers are fundamental tools in deep  learning,  although they may suffer from poor generalization due to the  nonuniform  gradient scaling. In this work, we propose **AdamL**,  a novel variant of the  Adam optimizer, that takes into account the  loss function information  to attain better generalization results. We  provide sufficient  conditions that together with the Polyak-Lojasiewicz  inequality, ensure  the linear convergence of AdamL. As a byproduct of  our analysis, we  prove similar convergence properties for the EAdam,  and AdaBelief  optimizers. Experimental results on benchmark functions  show that AdamL  typically achieves either the fastest convergence or  the lowest  objective function values when compared to Adam, EAdam, and  AdaBelief.  These superior performances are confirmed when considering  deep learning  tasks such as training convolutional neural networks,  training  generative adversarial networks using vanilla convolutional  neural  networks, and long short-term memory networks. Finally, in the  case of  vanilla convolutional neural networks, AdamL stands out from  the other  Adam's variants and does not require the manual adjustment of  the  learning rate during the later stage of the training.

**Edit**:

**An implementation**: https://github.com/andrewjc/PyTorch-AdamL",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1932cv6/r_adaml_a_fast_adaptive_gradient_method/,14,85,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E34668B90>
193klql,V1bicycle,2024-01-10 22:03:59+00:00,[D] Any paper lists for XAI and Diffusion models ?,"I have found well curated paper lists for Vision Transformers, ODD detection, and unlearning. I was curious to know whether there are any paper lists which have the important papers for Explainable AI and diffusion models",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193klql/d_any_paper_lists_for_xai_and_diffusion_models/,2,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E3466AB10>
1930r1g,Instantinopaul,2024-01-10 05:31:58+00:00,[D] Is On-Device AI the Future? NVIDIA Throws Down the Gauntlet at CES,"NVIDIA's big CES announcements focus on one key theme: bringing powerful AI capabilities directly to your PC or laptop.

**The Developer Tools:**

* AI Workbench (beta): Streamline AI development across platforms like Hugging Face, GitHub, and NVIDIA NGC.

* RTX Remix: Breathe new life into classic games with AI-powered upscaling and element modification.
* NVIDIA Avatar Cloud Engine (ACE): Create AI-powered digital avatars for games and other applications.

* Chat with RTX: Build personal assistants and chatbots that leverage local LLMs and user data.


**Is this the dawn of on-device AI dominance?** It's tempting to say yes. NVIDIA's powerful hardware and user-friendly tools make it easier than ever to run AI locally. However, challenges remain:

* Battery life: Laptops with these beefy GPUs might need an extra charger nearby.
* Software maturity: On-device AI software is still evolving, and developer adoption needs to pick up.
* Accessibility: High-end hardware comes at a cost, potentially limiting widespread adoption.



What do you think? Is on-device AI the future, or will cloud-based AI remain king? Share your thoughts in the comments below!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1930r1g/d_is_ondevice_ai_the_future_nvidia_throws_down/,29,93,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E3468E910>
193ozen,its_nehaha,2024-01-11 01:13:35+00:00,How do I validate unsupervised ML models? [R], I have been performing anomaly detection with Isolation forest on a very big dataset. My question is how do I validate this model if I don't have any labels? I am very new to ML so any help will be appreciated. Thanks ,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193ozen/how_do_i_validate_unsupervised_ml_models_r/,5,2,0.63,<praw.models.comment_forest.CommentForest object at 0x0000026E34680090>
193oj6r,vanteworldinfinity,2024-01-11 00:52:42+00:00,What's different about deploying a multimodal model vs. an LLM? [D],"As multimodal models like [Google Gemini](https://blog.google/technology/ai/google-gemini-ai/), [Pika](https://pika.art/), and [Stable Diffusion Video](https://stability.ai/news/stable-video-diffusion-open-ai-video-model) begin to be rolled out in 2024, I've been thinking about the ML ops that go into deploying them.   


What are the unique challenges compared to deploying with LLMs?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193oj6r/whats_different_about_deploying_a_multimodal/,1,1,0.56,<praw.models.comment_forest.CommentForest object at 0x0000026E34683B50>
193lc0j,GuavaAgreeable208,2024-01-10 22:33:50+00:00,[P] Transfer learning," 

Hello there,

I have a question on transfer learning. Can we apply transfer learning on a tabular dataset that has different inputs (only 4 similar features from the original dataset) and different output ?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193lc0j/p_transfer_learning/,1,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34686110>
193f0gi,kekkimo,2024-01-10 18:18:54+00:00,[D] How do we perform few-shot learning using LLMs when shots are long sequences?,"I saw several articles about in-context learning for few-shot learning using LLMs. Mostly 1 to 30 shots are provided as context.

How to do this for cases where shots are very long (e.g. summarization, document classification) since the LLM can't handle more than 2048 tokens (I am not talking about long-context LLMs)?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193f0gi/d_how_do_we_perform_fewshot_learning_using_llms/,3,5,0.73,<praw.models.comment_forest.CommentForest object at 0x0000026E34695D50>
193gndv,putinwhat,2024-01-10 19:24:44+00:00,[D] Modern OCR Handwriting Recognition Open Source Models,"ChatGPT-4 is incredibly good at pulling out multi-line handwritten text from images that also contain other subjects and I'm curious what models/tools exist in the open-source community for image-to-text for handwritten OCR?

Most of what I found when Googling were references to [tesseract](https://code.google.com/p/tesseract-ocr) but surely there have been advances since then and there must be models capable or pulling multi-line text from images. What are the current state-of-the-art methods for this?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193gndv/d_modern_ocr_handwriting_recognition_open_source/,2,3,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E346786D0>
1942524,DesperadoCoder,2024-01-11 13:49:20+00:00,[D] Step into Machine Learning without Mathematics experience,"Hello everyone,

I am an experienced software developer with over 20 years experience in back-end development with .net and c# mainly but lately also doing some python work with web scraping and DJango.  I am really enjoying python so far, and was thinking of starting to dabble with AI and ML using python.  I did some basic tutorials so far using the usual libraries such as Pandas, NumPy and SciKit-Learn, but I have barely touched the surface.

I found these quite complex although very interesting. I would like to continue on this path, however my maths is really bad.  I do not know anything about algorithms, calculus, probabilities etc.

So my question to seasoned AI and ML developers, do I really need to understand maths very well to continue on this journey, or my experience in software development can get me by?  Is there any area of AI that does not need heavy maths?

I would like to continue on this journey but I am not sure if it is worth invest my time and money in this when I can maybe continue expanding on web development.

Thank you in advance!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1942524/d_step_into_machine_learning_without_mathematics/,13,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E34592050>
192z048,APaperADay,2024-01-10 04:00:17+00:00,[R] MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts,"**Paper**: [https://arxiv.org/abs/2401.04081](https://arxiv.org/abs/2401.04081)

**Code**: [https://github.com/llm-random/llm-random](https://github.com/llm-random/llm-random)

**Abstract**:

>State Space Models (SSMs) have become serious contenders in the field  of  sequential modeling, challenging the dominance of Transformers. At  the  same time, Mixture of Experts (MoE) has significantly improved   Transformer-based LLMs, including recent state-of-the-art open-source   models. We propose that to unlock the potential of SSMs for scaling,   they should be combined with MoE. We showcase this on Mamba, a recent   SSM-based model that achieves remarkable, Transformer-like performance.   Our model, **MoE-Mamba**, outperforms both Mamba and  Transformer-MoE. In  particular, MoE-Mamba reaches the same performance  as Mamba in 2.2x less  training steps while preserving the inference  performance gains of  Mamba against the Transformer.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192z048/r_moemamba_efficient_selective_state_space_models/,4,40,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E34575290>
193bole,Dependent_Mushroom98,2024-01-10 16:03:08+00:00,Popular machine models in airlines industry…[D],"What are some of the interesting usecases that are being pursued by airlines and how is GenAI and other ML models playing a big role in it? 

Some of the use cases that I like to understand mostly stem from operations Research areas such as :

1. Forecast in real time cost for a seat to maximize profits?
2. How to optimize for flight schedules given weather delays and other airport/Airtraffic controller related cancellations? 
3. How to rebook the passenger last
Minute  on another flight for the best outcomes for the passenger and airlines (assuming #2 has happened above)?

Any ML models paradigm fits the above usecases? Appreciate the insights….",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193bole/popular_machine_models_in_airlines_industryd/,6,3,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E346A52D0>
194dua3,outeastor,2024-01-11 22:01:36+00:00,"[D] An Idea, AI That Can Identify the Top and Bottom Performers in Tech","As AI advances, many companies are looking for ways to reduce their employee costs and increase their productivity. Some employees are lazy, unproductive, or working multiple jobs, as seen in [r/overemployed](https://www.reddit.com/r/overemployed/). This makes the few hard-working and skilled employees more valuable.

My startup idea is to create an AI system that analyzes the git activity of tech employees, such as the content of the tickets you have done, commits, and project relevance, and identifies the best and worst performers every month. This system would not monitor the employees’ personal data or activities, so it would respect their privacy.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/194dua3/d_an_idea_ai_that_can_identify_the_top_and_bottom/,18,0,0.09,<praw.models.comment_forest.CommentForest object at 0x0000026E34699890>
19394mc,Any-Ad-3888,2024-01-10 14:10:32+00:00,Why packing is a good technique to find lower bounds? [R],"In learning theory, finding lower bounds for sample complexity uses techniques like defining a packing set on the hypothesis space. Concretely, given m samples and their labels, this provides m bits of information for the target model and thus cannot distinguish log\_2(m) functions which are ""reasonably"" far away (the packing set).

In learning theory, finding lower bounds for sample complexity involves defining a packing set on the hypothesis space. Concretely, given m samples and their labels, this provides m bits of information for the target model and thus cannot distinguish log\_2(m) functions that are ""reasonably"" far away (the packing set).(M) + log\_2(1- delta) by using packing sets?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19394mc/why_packing_is_a_good_technique_to_find_lower/,4,5,0.73,<praw.models.comment_forest.CommentForest object at 0x0000026E34698750>
1939mue,transformer_ML,2024-01-10 14:34:06+00:00,[D] Large Language Model 2023 Review and 2024 Outlook,"Medium: [https://medium.com/@kentsui/large-language-model-2023-review-and-2024-outlook-cbd5211cf49b](https://medium.com/@kentsui/large-language-model-2023-review-and-2024-outlook-cbd5211cf49b)

Substack: [https://paperdigest.substack.com/p/aimachine-learning-mostly-llm-2023](https://paperdigest.substack.com/p/aimachine-learning-mostly-llm-2023)

What do you think of 2024?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1939mue/d_large_language_model_2023_review_and_2024/,2,4,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E346A9190>
192yvmo,gw109,2024-01-10 03:53:29+00:00,[R] Brain-Inspired Machine Intelligence: A Survey of Neurobiologically-Plausible Credit Assignment,[https://arxiv.org/abs/2312.09257](https://arxiv.org/abs/2312.09257),MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192yvmo/r_braininspired_machine_intelligence_a_survey_of/,0,19,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E34599190>
1936hj1,MrGolran,2024-01-10 11:48:35+00:00,[D] Evaluation for Quantile Probabilistic forecast," I'm training a model that performs probabilistic forecasting where it outputs a probability distribution instead of a single point estimate for each time step. So for each timestep I get a value for each quantile I have defined (q20,q50,q80..etc) . 

 I saw that most evaluation approaches either use the median for each timestep (q50) to calculate the MAPE and other metrics or use specific probabilistic forecasting metrics like LogS, CRPS and VarS.

 In order to compare the probabilistic forecast model with other deterministic models is it valid to get the MAPE for the test set by using for each timestep the predicted value with the minimum difference from the actual target value ? This implies that for different timesteps values from different quantiles might be used to evaluate performance. Do you think that is a good approach or is that cheating ? ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1936hj1/d_evaluation_for_quantile_probabilistic_forecast/,5,4,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34598B50>
193e54u,Evermore2307,2024-01-10 17:44:20+00:00,Need to Generate Conversations [D],"I've some transcripts of conversations between agents and customers. I need to generate synthetic conversations using those conversations. Can any of you suggest me how to proceed? I need a model that can take many transcripts as input and produce similar ones. Context window is an issue. Even if that's resolved, what prompts to provide?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193e54u/need_to_generate_conversations_d/,1,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E346AE090>
193cgo4,Electronic-Letter592,2024-01-10 16:36:03+00:00,[Discussion] Translation models for longer texts,"I was trying popular MT models, such as SeamlessM4T-v2, Open-NLLB, MADLAD-400 from huggingface. It seems that they support only very short texts, like 1 sentence. I am wondering if I am missing something, or how would you use them to translate a few pages of texts?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193cgo4/discussion_translation_models_for_longer_texts/,0,1,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E3459A5D0>
192y4tf,ic10503,2024-01-10 03:14:57+00:00,[P] Machine Learning for Imbalanced Data Book + GitHub Repo,"Self-promotion alert: I recently wrote a book, ""Machine Learning for Imbalanced Data.""

The book primarily focuses on classification problems, where too little data or too much data for one or more classes leads to an imbalance. Data imbalance (unbalance) or class imbalance has been a controversial topic to write about, with criticism about sampling techniques leading to model miscalibration issues and a host of other problems. However, this book aims to do justice to both sides of the coin, going over the pros and cons of the various techniques.

📘 Here is the Amazon link: [https://www.amazon.com/Machine-Learning-Imbalanced-Data-imbalanced/dp/1801070830/](https://www.amazon.com/Machine-Learning-Imbalanced-Data-imbalanced/dp/1801070830/)

The first half of the book covers sampling techniques, weighting techniques, threshold-tuning techniques for structured data and classical models. The second half of the book caters to unstructured data and deep learning models using PyTorch. Finally, it concludes with model calibration in the context of imbalanced data (model calibration is quite important for several real-world applications but somehow remains underappreciated in technical books)

The accompanying GitHub repository offers Jupyter notebooks (one-click run on Google Colab), and additional resources complementing the book's content: [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data).

It took me some time to gather information on whether and how big companies deal with data imbalance and what strategies they use in production, which I documented [here](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/industry-applications-imbalanced-data.md).

It took me one and a half years to write the book. Your feedback and suggestions will be highly appreciated and will be invaluable for future editions (in case I happen to write it 😊)

(Official book website: [https://imbalanceddata.com/](https://imbalanceddata.com/))",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192y4tf/p_machine_learning_for_imbalanced_data_book/,2,10,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E346AF1D0>
192tj1t,ComedyIsOver,2024-01-09 23:43:22+00:00,"Why is the IAF-VAE model called ""inverse"" autoregressive flow (IAF)? [D]","What's so ""inverse"" about it? I understand section 3 in the paper (Inverse Autoregressive Transformations) but I fail to see how section 4 (Inverse Autoregressive Flow (IAF)) follows from there. Do we choose a specific ordering of latent variables, as we do in section 3? 

I'd appreciate it if someone could point me to a blog post that walks you through the details of the IAF-VAE model.

Here is the paper: [https://arxiv.org/pdf/1606.04934.pdf](https://arxiv.org/pdf/1606.04934.pdf)  
",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192tj1t/why_is_the_iafvae_model_called_inverse/,6,16,0.87,<praw.models.comment_forest.CommentForest object at 0x0000026E346B9490>
193a8uj,GraphHopper77,2024-01-10 15:01:31+00:00,[R] Adversarial example detection,"I'm an under graduate, im planning to create a vision transformer based adversarial example classification model trained on raw adversarial and clean images, what are the things that I should consider during the development process regarding model selection and feature engineering",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193a8uj/r_adversarial_example_detection/,0,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E346AF350>
1939w0b,selfpromoting,2024-01-10 14:45:27+00:00,[D] Comparing two images taken at different angles,"Hello reddit,

I am looking to compare two or more images or the same object taken over the course of a decade from slightly different angles. I would like to know whether certain characteristics of the object from the first photo remain in the last photo.

More specifically, my intent is to compare two pictures of a roof to figure out whether the various colorations/wear/deterioration shown in first photo have remained the same as in the more recent photo---my intent is to determine whether the roof was ever replaced during the interim years between the two photos.

Any application out there already doing this? Any idea what such a comparison might be called?

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1939w0b/d_comparing_two_images_taken_at_different_angles/,4,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E346B7410>
1938zb1,chamouloxx,2024-01-10 14:03:32+00:00,[D] Is there an index for machine learning conferences ?,"Hello,

I'm looking for a proper page up to date with conferences on Machine Learning, when are next ones planned, worldwide (i'm mostly interested by europe ones).

Bonus is you also have python related conferences, MLOps....

&#x200B;

I found this page: [http://conferences.visionbib.com/Iris-Conferences.html](http://conferences.visionbib.com/Iris-Conferences.html)

Thanks",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1938zb1/d_is_there_an_index_for_machine_learning/,3,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E3459A790>
192ra9a,pikachuunibyo,2024-01-09 22:12:17+00:00,Training loss decreases expectedly then goes wild after first epoch? [D],"In the first epoch the training loss is decreasing at a pleasant rate, but then since the second epoch begins wildly flailing about. I've tried 1e-5, -6, and seemed to follow the same pattern. Validation also plateaus. I've never encountered this before, is this a local minimum problem? This run is 6 epochs, but I'm currently turning it up to 20 epochs to see its behavior, since it looked optimistic at step 25k.

The model is google/electra-large-discriminator for token classification, and the optimizer is adamw. No other modifications like layer freezing, weight decay, layerwise weight decay were used.

https://preview.redd.it/f6ydsuzcnhbc1.png?width=1210&format=png&auto=webp&s=075f8da8ad5dab863cfa189bfc235b32658a459d

**\[Update\]**

I ran a couple more tests with Electra over 20 epochs, using a much smaller dataset. Below are the train/valid losses. Using LR=5e-6 (the dark blue line), electra finally reached an approximately diagonal confusion matrix.

[There seems to be a bifurcation point at 700 training steps, probably around 15-20 epochs, and the orange session \(1e-5\) is the first run to correctly predict something nontrivial \(even though it's still terrible\).](https://preview.redd.it/8ajlg1dxhibc1.png?width=1249&format=png&auto=webp&s=6c7b8845c43b5af210e842f7702eca66c2d554f8)

[Validation loss: bifurcation pt showing at the 700 step for larger step sizes.](https://preview.redd.it/9bsrk9wxhibc1.png?width=1249&format=png&auto=webp&s=deca4403db2e8028bf4c222da15ed2b3d58ead8a)

**\[Update 2\]**

Ugh... I spoke too soon. I took the most stable learning rate and switched to the 100x larger dataset, and the same freakout happens after 5 epochs.

https://preview.redd.it/niasjqib3pbc1.png?width=828&format=png&auto=webp&s=473a4b09463bd00d8748227b874b6862b646224b",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192ra9a/training_loss_decreases_expectedly_then_goes_wild/,8,11,0.76,<praw.models.comment_forest.CommentForest object at 0x0000026E346BEF90>
1935lb8,L4HPlz,2024-01-10 10:51:19+00:00,[D] Overtrained RVC Model?," I used the [guide](https://rvc-models.com/t/how-to-create-a-rvc-model-tutorial/11) from rvcmodels.com to begin training my first model, but I'm having trouble determining if there's a point of overtraining on the TensorBoard graph. The screenshot in the guide shows a noticeable indication, but I haven't observed one on mine. Is my model overtrained, and if so, at what value? It's at 650 epochs and utilized a 69-minute  dataset, if that helps. 

https://preview.redd.it/cwwe1cl2flbc1.png?width=1471&format=png&auto=webp&s=b86ec7a6fd5efe4b42884b90fcdd2ca4243476ef",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1935lb8/d_overtrained_rvc_model/,4,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E346BFAD0>
192aj3h,convolutionality,2024-01-09 09:06:52+00:00,What are weaknesses of the field currently? [D],"Hi all,

Does anyone have any concept of technical and business related gaps and weaknesses of this field? Things that if were possible or more efficient, would make projects and model optimal? For example (not necessarily a massive case anymore) lack of quality datasets. 

Thanks big time!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192aj3h/what_are_weaknesses_of_the_field_currently_d/,88,95,0.89,<praw.models.comment_forest.CommentForest object at 0x0000026E3470B290>
192k8b4,25cmderespeito,2024-01-09 17:30:37+00:00,[R] Testing MAMBA architecture KV-Retrieval and RAG capabilities," I am about to test the capabilities of MAMBA in a similar way to the paper [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf), but as it is a lot of work, I am asking if anyone did this already. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192k8b4/r_testing_mamba_architecture_kvretrieval_and_rag/,3,21,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E345A29D0>
192q8cd,Smart-Emu5581,2024-01-09 21:30:27+00:00,[R] Supervised Learning with interactions?,"I am doing research on supervised learning and I am thinking about a concept that really ought to have a name, but I can find nothing about it in the literature.

The idea is to have a supervised learning task where the model can send a limited number of queries and receive answers to them before it has to decide on the output.

As an example: The input could be an image classification task where most of the image is hidden behind a shadow. The model is allowed to specify up to three chunks for which the shadows are removed before it has to submit its classification.

This could also be represented as a reinforcement learning task, but it is much more specific than general-purpose reinforcement learning and the output is supposed to be trained on an MSE loss function, not a reward function.

Is there a name for this sort of problem in the literature?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192q8cd/r_supervised_learning_with_interactions/,7,7,0.9,<praw.models.comment_forest.CommentForest object at 0x0000026E346E0D10>
19366tw,qualaric,2024-01-10 11:30:01+00:00,[D] Any good alternatives to RVC for AI cover song maker,"For context, I haven't use RVC yet!! The problem is the install file from the official GitHub page got a **malware** according my antivirus (Bitdiffender) and also the project has been pretty much abandoned!!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19366tw/d_any_good_alternatives_to_rvc_for_ai_cover_song/,0,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E346FA490>
1937vnj,TheUnmaykr,2024-01-10 13:07:39+00:00,"""[Research]"" Need image classification models for a project","Hey  , I'm currently an undergrad doing a project under a PhD scholar on a  multi-modal classifier that takes images and text detect hateful content

For that we have used resnet50 and vgg16 for image classification but both gave an accuracy of below 70%.

So  please can you guys help me out and name a few models that have very  high imagine classification accuracy and are able to defend Adversarial  Attacks.  
Thank you",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1937vnj/research_need_image_classification_models_for_a/,6,0,0.36,<praw.models.comment_forest.CommentForest object at 0x0000026E345A4690>
193cmsa,munkyhed,2024-01-10 16:43:06+00:00,[D] good AI events vs empty hype?,Which AI-focused events are actually informative about cutting edge tech and good for professionals vs empty bluster from brands who want to be seen as thought leaders?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193cmsa/d_good_ai_events_vs_empty_hype/,5,0,0.22,<praw.models.comment_forest.CommentForest object at 0x0000026E346CFB90>
193596f,Fit-Rub3325,2024-01-10 10:28:18+00:00,"""[Discussion]"" Address Normalization implementation"," 

Hi, as the title says, I am working on developing a solution where I can develop an in house geocoding API, that can help me in cleaning faulty, unnormalized addresses.

I have a dataset of faulty addresses and used map APIs to get the correct addresses for the same.

What I want to develop is something similar to these API, where if a person enters faulty addresses ( this means, misspelling, wrong pin code, missing city, state or street or any other anomaly), he/she can retrieve the correct addresses. Of course this requires matching with correct addresses.

I have tried few solutions like:

1. levenshtein distance - but searching through database every time will be overkill
2. Using embeddings approach - but getting very poor accuracy, as the document vectors for each country is almost similar (when address taken collectively), similar results were found using word and sub word level vectors too.

My curiosity is how and what solution Google has implemented, so it's accurate almost 95% of the time?

Any suggestions are welcome :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193596f/discussion_address_normalization_implementation/,2,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E346E0B10>
1929n4f,Used-Ad-7734,2024-01-09 08:03:05+00:00,[D] Are Custom LLM RAG apps going to become redundant?,"Loks like Copilot Studio is being rolled out (https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio) with an impressive looking no code/out of the box RAG solution.

There is a phenomenal amount of development and activity in the Open Source RAG world (e.g Langchain, Llamaindex, etc), which I am a great supporter of FYI.

However, what seems strange is that this no code out of the box solution (Copilot Studio - just as an example of one) seems overwhelmingly to be the better option if you wanted to build a RAG app i.e If you compare the cost to build and productionise a custom RAG app vs the cost of using Copilot Studio, it's almost an order of magnitude lower (no matter how you cut it with the developer time and duration). 

My question is, it seems to me we are moving towards a situation where enterprise solutions will make custom RAG apps redundant (not in all cases of course, but most cases), however there seems to be very little discussion of this relative to the activity in the open source community. Do people agree this is a likely scenario? 

Obviously there will be exceptions…but on most use cases I don’t see how you can compete with an instant/minimal setup, low cost, highly scalable RAG solution.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1929n4f/d_are_custom_llm_rag_apps_going_to_become/,15,66,0.92,<praw.models.comment_forest.CommentForest object at 0x0000026E34720AD0>
1920hky,Singularian2501,2024-01-09 00:07:40+00:00,"[R] WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia - Achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4! - Stanford University 2023","Paper: [https://arxiv.org/abs/2305.14292v2](https://arxiv.org/abs/2305.14292v2) 

Github: [https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat) 

Abstract:

>This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  
>  
>WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. **We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.**  
>  
>Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM.  
>  
>**WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4,** while receiving significantly higher user ratings and more favorable comments. 

https://preview.redd.it/9mhpdh300bbc1.jpg?width=1225&format=pjpg&auto=webp&s=cb64b717e920d7bf727782f7c803500ae838d6ef

https://preview.redd.it/5dxesl200bbc1.jpg?width=862&format=pjpg&auto=webp&s=b6de0cda980eec3cf3484ff1f9cd6dc1acf13505

https://preview.redd.it/j387vl200bbc1.jpg?width=914&format=pjpg&auto=webp&s=736fb922c1f98f4c7b132f1c153f4653a8b85441

https://preview.redd.it/3hnxqi200bbc1.jpg?width=923&format=pjpg&auto=webp&s=95b40a9cf67d7f3729dae85878db67a262cc5201",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/,27,220,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E34734910>
192qlu2,jbhuang,2024-01-09 21:45:23+00:00,[D] How I understand diffusion models,"Hi all, 

I made an explainer video on diffusion models covering the basics, including training, guidance, resolution, and speed. I hope this helps people interested in learning more about diffusion models. 

[https://www.youtube.com/watch?v=i2qSxMVeVLI](https://www.youtube.com/watch?v=i2qSxMVeVLI)

Feedback/questions are welcome!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192qlu2/d_how_i_understand_diffusion_models/,1,5,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E3472F0D0>
192jxt8,IffyNibba01,2024-01-09 17:18:34+00:00,[D] An idea for an interactive website that helps people explore and discover new ML concepts,"&#x200B;

[A Figma prototype for the website idea](https://preview.redd.it/acwzpkmo6gbc1.png?width=1440&format=png&auto=webp&s=36583eabcb78cca5349a3b75df93a71d5ab02a73)

So I have an idea for a website that helps people explore complex topics from machine learning in an interactive way.

Topics would include model architecture:

* model architectures
* methods for training and fine tuning models
* novel approaches to improving model performance
* basically anythinng that is discussed in research papers

I would try to make it as interactive as possible so that people could form a deep understanding of the topics that interest them. I would also link to code and hugging face implementations so that people could get hands on experience with these topics themselves.

The goal is to help people better understand the research that is going on in the space and make it easy for them to get practical experience with the new technologies.

What are your thoughts on the idea? What else should I consider? What are some obvious problems? Would you use/contribute to this if it existed?

 Any opinion at all will help me to clarify the idea, so please share! Thanks :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192jxt8/d_an_idea_for_an_interactive_website_that_helps/,5,10,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E3471C450>
192s3a0,lmcinnes,2024-01-09 22:44:59+00:00,[P] DataMapPlot for presentation ready UMAP and t-SNE plots,"I made a small library for quickly and easily making presentation or poster ready plots of the results of UMAP, t-SNE, etc. This should work well with any clustered and labelled dataset, particularly large corpora pushed through BERTopic or other similar topic modelling tools. The aim is to make it as easy as possible to make an aesthetically pleasing plot, while providing enough ways to fine tune the style to suit your needs.

Code: https://github.com/TutteInstitute/datamapplot

Docs: https://datamapplot.readthedocs.io/

PyPI: https://pypi.org/project/datamapplot/

conda: https://anaconda.org/conda-forge/datamapplot",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192s3a0/p_datamapplot_for_presentation_ready_umap_and/,0,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34712750>
192y6kw,drroadroad,2024-01-10 03:17:17+00:00,[D] NVIDIA P106-100 for machine learning,i just got a P106 -100 for machine learning using arch linux is it possible to use it out of the box?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192y6kw/d_nvidia_p106100_for_machine_learning/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E345A7BD0>
192x6d1,Radiant-Cockroach-33,2024-01-10 02:28:28+00:00,"[D] Looking for a Binary Classification Dataset with a ""Two Beans"" Structure (~2000 Entries)","I'm currently working on a binary classification project and need a dataset with a distinctive  ""two beans"" structure.  \[IMAGE\]

&#x200B;

[Dataset](https://preview.redd.it/i0zhpe6vwibc1.png?width=260&format=png&auto=webp&s=f33f3ecfdb1608ba38b48b185ea7329cb3335cf4)

&#x200B;

Specifically, I'm looking for these 2000 data entries with features represented as (x1, x2) and corresponding class labels   (+1/-1). The data should resemble the following format:

x1 x2 class

8.0919 -1.7303 1

I've lost the file's data and can't find it anywhere online. What format of the dataset is this? Is there any name for this data distribution? Where I could find such a dataset, or another similar in shape, for training my custom classification model?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192x6d1/d_looking_for_a_binary_classification_dataset/,3,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E3471CA50>
192wjq4,Fresh_Information_87,2024-01-10 01:58:31+00:00,Multilabel classification Accuracy measurement [D],"I was wondering In Multillabel text classification using binary relevance, how does it exactly measure the accuracy, is the below method accurate, because that was what i could find on the internet. Thanks    in advance :) 

1. The dataset is divided into a training set and a test set
2. In the training phase, the model is trained to recognize the relationship between the comments and their corresponding category labels, to be able to give an accurate response to new data.
3. The trained model is evaluated on the test set, where it has to predict the labels for the comments, having access only to the text. This functions as a real-life scenario, where the model works with data it has not seen before.
4. After the model has made its predictions, it gets access to the original labels, where it can then compare the results from the test, to see how well the model has performed on unforeseen data.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192wjq4/multilabel_classification_accuracy_measurement_d/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E345B1850>
193ei6g,wyldcraft,2024-01-10 17:58:55+00:00,[D] Was the Transformer inevitable?,"Nature evolved the eye [several times](https://www.nature.com/articles/eye2017226). Whenever mutation or gene transfer gave a critter the least bit of ability to turn a photon into an electric impulse, there was a tendency to eventually develop lenses and stereo vision. Recently I thought to ask whether brains spontaneously evolve too, and sure enough, our best guess right now is [nine different lineages](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4024470/). Nature ""wants"" to make eyes and nervous systems.

Some mathematics seem inevitable. Babylon and India both invented the zero. India and China both found methods for long division. Pythagorean theorem was independently developed in both Greece and in the far East. Newton and Leibniz figured out a lot of similar calculus.

Another math example: before the LLM craze, bright high schoolers would regularly pop into programming subreddits and ask, ""can't we predict words based on the previous few words using, like, statistics?"" and the sub would collectively groan ""Markov chains are over a hundred years old dude.""

These parallel evolutions, discoveries, and inventions are extra interesting to me because there's a suggestion that they're so ""inevitable"" that even alien races could plausibly be bipedal with two eyes and two ears and Pythagorean theorem, because nature seems to select for the associated advantages.

So my actual question: are Transformers especially once-per-galaxy brilliant, or would a civilization with computers inevitably create a GPT-4?

So would someone else have invented Transformers if *Attention Is All You Need* hadn't been published in 2017?

(sci-fi nonsense follows...)

>!Alien races tending to achieve ASI less than a hundred years after computing would affect the Drake Equation, which theorizes the odds of alien contact and tries to explain why our skies aren't currently filled with UFOs.!<

>!If GPT is inevitable, it could affect the L variable, the length of time a civilization exists. If we're looking for reasons our skies are clear, perhaps ASI is how advanced civilizations tend to destroy themselves, rather than nukes or collectively disappearing into VR as we've been theorizing. I don't mean to scare-monger here. I have hopes ASI, if it emerges, will solve more problems than it creates.!<",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/193ei6g/d_was_the_transformer_inevitable/,22,0,0.35,<praw.models.comment_forest.CommentForest object at 0x0000026E34741050>
192an1m,APaperADay,2024-01-09 09:14:44+00:00,[R] Inferring neural activity before plasticity as a foundation for learning beyond backpropagation,"**Paper**: [https://www.nature.com/articles/s41593-023-01514-1](https://www.nature.com/articles/s41593-023-01514-1)

**Preprint version(s)**: [https://www.biorxiv.org/content/10.1101/2022.05.17.492325](https://www.biorxiv.org/content/10.1101/2022.05.17.492325v2)

**Code**: [https://github.com/YuhangSong/Prospective-Configuration](https://github.com/YuhangSong/Prospective-Configuration)

**Abstract**:

>For both humans and machines, the essence of learning is to pinpoint  which components in its information processing pipeline are responsible  for an error in its output, a challenge that is known as ‘credit  assignment’. It has long been assumed that credit assignment is best  solved by backpropagation, which is also the foundation of modern  machine learning. Here, we set out a fundamentally different principle  on credit assignment called ‘**prospective configuration**’. In prospective  configuration, the network first infers the pattern of neural activity  that should result from learning, and then the synaptic weights are  modified to consolidate the change in neural activity. We demonstrate  that this distinct mechanism, in contrast to backpropagation, (1)  underlies learning in a well-established family of models of cortical  circuits, (2) enables learning that is more efficient and effective in  many contexts faced by biological organisms and (3) reproduces  surprising patterns of neural activity and behavior observed in diverse  human and rat learning experiments.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192an1m/r_inferring_neural_activity_before_plasticity_as/,1,18,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E3473E1D0>
191rdwq,akshayka,2024-01-08 18:00:41+00:00,"[P] I built marimo — an open-source reactive Python notebook that’s stored as a .py file, executable as a script, and deployable as an app.","Hi! I’d like to share marimo, an open-source reactive notebook for Python. It aims to solve many well-known problems with Jupyter notebooks, while giving you new capabilities: marimo notebooks are reproducible (no hidden state), git-friendly (stored as a Python file), executable as Python scripts, and deployable as web apps[.](http://apps.it/)

GitHub Repo: [https://github.com/marimo-team/marimo](https://github.com/marimo-team/marimo)

In marimo, your notebook code, outputs, and program state are guaranteed to be consistent. Run a cell and marimo *reacts* by automatically running the cells that reference its variables. Delete a cell and marimo scrubs its variables from program memory, eliminating hidden state. If you are worried about accidentally triggering expensive computations, you can disable specific cells from auto-running.

marimo also comes with UI elements like sliders, a dataframe transformer, and interactive plots that are automatically synchronized with Python. Interact with an element and the cells that use it are automatically re-run with its latest value. Reactivity makes these UI elements substantially more useful than Jupyter widgets, not to mention easier to use.

I chose to develop marimo because I believe that the ML community deserves a better programming environment to do research and communicate it. I’ve seen lots of research start in Jupyter notebooks (much of my own has). I’ve also seen lots of that same research fail to reproduce or get slowed down by hidden bugs, due to shortcomings inherent to Jupyter notebooks.

I strongly believe that the quality of our work depends on the quality of our tools, and that the tools we use shape the way we think — better tools, for better minds. I worked at Google Brain as a software engineer in 2017-2018, when TensorFlow was transitioning to TensorFlow 2 and JAX was in its early stages. I saw firsthand the increase in productivity that PyTorch and JAX brought to our community, and later to my own research when I did a PhD at Stanford with Stephen Boyd. Our goal with marimo is to do something analogous but via a new programming environment.

marimo has been developed with the close input of scientists and engineers, and with inspiration from many tools, including Pluto.jl and streamlit. It’s just two of us working on it — we open sourced it recently because we feel it’s ready for broader use. Please try it out (pip install marimo && marimo tutorial intro). We’d really love any and all feedback you may have!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191rdwq/p_i_built_marimo_an_opensource_reactive_python/,48,260,0.98,<praw.models.comment_forest.CommentForest object at 0x0000026E347698D0>
192ns25,vatsadev,2024-01-09 19:52:43+00:00,"[P] Trying to replicate RT-2 on a smaller scale, anything that could help me?","So I was looking at the RT-2 paper, and I was interested in using the next couple of months to replicate some of their work for a different robot.

I don't really have the resources to train a transformer beyond the range of 20-100m parameters, and unlike RT-1, RT-2 was in the 6b-55b range.

I have far more scaled down functionality, including - dont need alot of conversational capability, tiny chats which models that size can already do, and some simple instruction following - don't need advanced VLM reasoning, more like basic object recognition, like say ""turn towards the red can"" and it recognizes the red can - doesnt need to be able to encode continuous values, can just call one of ~6 functions

anything that could help improve performance?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192ns25/p_trying_to_replicate_rt2_on_a_smaller_scale/,3,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E3473E190>
192chr6,ojiber,2024-01-09 11:20:26+00:00,[P] Does Google sunset their off-the-shelf models as well as their apps?,"I've been looking into semantic search recently for a personal project and I came across the Google Cloud Platform ""Gecko"" embedding model which looks like it would be able to allow me to find similar products by comparing how similar their descriptions are.

The main issue that I'm seeing with semantic search is the requirement that the embedding model remains completely unchanged and still available because otherwise, I won't be able to measure the ""closeness"" of any new products. In that case, I would have to re-vectorise all of the products I've already vectorised because the vector space representations of different embedding models are different. Seems like it could be expensive and a massive time-suck.

Given Google's reputation for canning its old products, I don't want to jump into something that will be gone soon. Does Google have back compatibility for this kind of thing? Would I be better off going somewhere else or just giving up and hosting a pre-trained version of Word2Vec on GPC or AWS instead?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192chr6/p_does_google_sunset_their_offtheshelf_models_as/,13,7,0.82,<praw.models.comment_forest.CommentForest object at 0x0000026E34743810>
192d7ml,Mr__Weasels,2024-01-09 12:04:33+00:00,[D] reconstruction loss weight vs KLD weight for VAE's? which is better?,is one better than the other?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192d7ml/d_reconstruction_loss_weight_vs_kld_weight_for/,6,7,0.77,<praw.models.comment_forest.CommentForest object at 0x0000026E34739BD0>
192agnv,APaperADay,2024-01-09 09:01:51+00:00,[R] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models,"**Paper**: [https://arxiv.org/abs/2401.01335](https://arxiv.org/abs/2401.01335)

**Abstract**:

>Harnessing the power of human-annotated data through Supervised   Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs).   In this paper, we delve into the prospect of growing a strong LLM out   of a weak one without the need for acquiring additional human-annotated   data. We propose a new fine-tuning method called Self-Play fIne-tuNing   (**SPIN**), which starts from a supervised fine-tuned  model. At the heart of  SPIN lies a self-play mechanism, where the LLM  refines its capability  by playing against instances of itself. More  specifically, the LLM  generates its own training data from its previous  iterations, refining  its policy by discerning these self-generated  responses from those  obtained from human-annotated data. Our method  progressively elevates  the LLM from a nascent model to a formidable  one, unlocking the full  potential of human-annotated demonstration data  for SFT. Theoretically,  we prove that the global optimum to the  training objective function of  our method is achieved only when the LLM  policy aligns with the target  data distribution. Empirically, we  evaluate our method on several  benchmark datasets including the  HuggingFace Open LLM Leaderboard,  MT-Bench, and datasets from  Big-Bench. Our results show that SPIN can  significantly improve the  LLM's performance across a variety of  benchmarks and even outperform  models trained through direct preference  optimization (DPO)  supplemented with extra GPT-4 preference data. This  sheds light on the  promise of self-play, enabling the achievement of  human-level  performance in LLMs without the need for expert opponents.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192agnv/r_selfplay_finetuning_converts_weak_language/,0,10,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34753C90>
192csmj,vaibhavgoel2094,2024-01-09 11:38:57+00:00,[D] Picking the right LLM model.,"Hey folks, I am looking to build internal LLM apps for different use cases. Example use cases include Product assistant, Text summarisation, Document parsing.. etc. Question: Any framework or platform to decide which LLM model to choose/pick to build these apps as per these use cases?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192csmj/d_picking_the_right_llm_model/,1,4,0.84,<praw.models.comment_forest.CommentForest object at 0x0000026E3475C6D0>
192dgk0,One_Definition_8975,2024-01-09 12:19:15+00:00,Where do I start to study graph neural networks?[D],I don't understand jure leskovoc s videos. But want to learn.Where do I start?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192dgk0/where_do_i_start_to_study_graph_neural_networksd/,3,4,0.7,<praw.models.comment_forest.CommentForest object at 0x0000026E3477AB10>
191ol1n,SpaceXRaptor42,2024-01-08 16:05:56+00:00,"[D] Human brain FLOPs estimate, is it lower than we thought?","
This post is meant to provide insight into the human brain so that it becomes easier to compare it to artificial neural networks.


Take most of what I'm about to say with a grain of salt, I could easily be of by an order of magnitude or have missed something.

1. Ray Kurzweils estimate.
10^11 neurons.
1000 synaptic connections per neuron.
100 spikes per second.

10^11 × 1000 × 100=10^16 calculations per second.

Quote from the singularity is near:
""Given the early stage of human-brain reverse engineering, I will use a more conservative figure of 10^16 CPS"".


2. My own calculation.
Things seem to have changed since 2005, now Wikipedia says 7000 synapses per neuron 
https://en.m.wikipedia.org/wiki/Neuron

Neuron firing speed is estimated to be 0.1 to 2 Hertz on average. https://aiimpacts.org/rate-of-neuron-firing/#:~:text=Assorted%20estimates-

I will use 1/s as spike frequency. The brain is also more defined at 86,000,000,000 neurons.

8,6×10^10 × 7000 × 1 = 6×10^14.
6×10^14 FLOPs (one FLOP per synapse).


3. Spike energy requirement.
Each activation of a neuron requires a certain amount of energy and that energy seems to be 2.468 × 10^−7 J
https://link.springer.com/article/10.1007/s11571-018-9503-3

So from here everything else can be figured out.
Spike energy = 2.468 × 10^−7 J
Brain energy consumption over 24 hours = 1,673,600 joule 
Seconds in 24 hours = 86400.
7000 synapses per neuron.

1,673,600÷(2.468 × 10^−7) J = 6,782×10^12.
6,782×10^12 ÷ 86400 = 78,486,103.

(78,5 million spikes per second).

78,486,103 × 7000 = 5.49×10^10 FLOPs or 549 gigaFLOPs

If 3 is correct, then that would mean that a high-end phone has more compute in the GPU than the human brain (Samsung s23, 3,681 TFLOPs at fp32. Brain 0,549 TFLOPs average over the day).

This is not a good way to compare things because the brain is a massively parallel computer where the memory basically is in the structure. 

So how much ""memory"" are we talking about for the brain?
We have:
86,000,000,000 neurons.
7000 synapses per neuron.
5 bits per synapse.
https://www.cnsnevada.com/what-is-the-memory-capacity-of-a-human-brain/#:~:text=Neurons%20are%20the%20cells%20which

86,000,000,000 × 7000 × 5 = 3×10^15 bits or 3.76×10^14 bytes.
Good luck fitting 376 terabytes of RAM on a phone.

But is 78,500,000 spikes per second really enough for the brain to process everything? Let's look at the eyes.

Each eye has a total resolution of 8 megapixels.
https://m.youtube.com/watch?v=4I5Q3UXkGd0&pp=ygUednNhdWNlIHJlc29sdXRpb24gb2YgaHVtYW4gZXll

The information sent through the optical nerve is only about 10,000,000 bits/s 
https://www.eurekalert.org/news-releases/468943

(only the most relevant information is sent through the optical nerve because the brain wants to conserve power at all costs).
So we have 20,000,000 Spikes/s for both eyes which is 25,5% of 78,5 million.

78.5 million spikes is not a hard performance ceiling, it's only the average over the day and the brain is actively modulating brain-wave frequency according to need.

Which scenario is more likely in your opinion? 1. 2. or 3.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191ol1n/d_human_brain_flops_estimate_is_it_lower_than_we/,117,134,0.79,<praw.models.comment_forest.CommentForest object at 0x0000026E347D1990>
192ckjf,Electronic-Letter592,2024-01-09 11:25:04+00:00,[Discussion] Open source model for text translation tasks?,"I am looking for an open source model, that runs locally, which is able to translate texts from different languages into English with a high accuracy. For transcription tasks it looks like Whisper is doing very well. I was wondering if a similar model exists for text translation tasks?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192ckjf/discussion_open_source_model_for_text_translation/,1,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34789250>
1924vn1,One_Definition_8975,2024-01-09 03:31:52+00:00,Mixtral paper[D],https://arxiv.org/abs/2401.04088,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1924vn1/mixtral_paperd/,1,12,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E3477B3D0>
192rfdv,Illustrious_You_5159,2024-01-09 22:18:02+00:00,[D] Menstrual period training data,"Hey everyone im developing a menstrual period tracker using react. My backend is supabase. I want to train a lstm model using dummy data with tensor flow. I'm a new software developer, so I don't have much knowledge in machine learning.

The app allows users to enter historic and current period cycles. Do I have to retrain the model every time a user adds data or is there another way to update the model?

Is also possible to generate a specific model for each user based on their tracker data? So that their predictions will be generally based on the overall data set, but specifically taking weight to the user historic data.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192rfdv/d_menstrual_period_training_data/,3,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E3478BF90>
19268kz,Entire-Fly-6957,2024-01-09 04:39:56+00:00,[D] GPT4-V VS Gemini Pro Vision Full Version!," Currently, both GPT and Gemini only support image input and do not support video input. Therefore, I selected tests related to images only from the Google Gemini demo to compare GPT-4-V and Gemini-Pro-Vision,  The tests include:  

1. Basic recognition of image content

2. Analysis of objects in the image 

3. Logical reasoning about the content in the image 

4. Recognition and analysis of content in consecutive images 

[https://youtu.be/yFK62Tn\_f4Q](https://youtu.be/yFK62Tn_f4Q)

  


 If you are interested in the open-source project demonstrated in the video, please visit [https://github.com/smalltong02/keras-llm-robot](https://github.com/smalltong02/keras-llm-robot)  
",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19268kz/d_gpt4v_vs_gemini_pro_vision_full_version/,4,6,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E3477BD10>
192kec9,paulo_zip,2024-01-09 17:37:22+00:00,[D] Is there a good open-source model for dubbing?,Are you guys trying any open-source model for AI dubbing?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192kec9/d_is_there_a_good_opensource_model_for_dubbing/,0,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E347A7390>
1926ias,Dense-Smf-6032,2024-01-09 04:54:09+00:00,[D] How do you guys evaluate LLM?,"how do you guys evaluate LLM? There is online leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

&#x200B;

Is there any script that can automatically evaluate our performance offline/benchmark?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1926ias/d_how_do_you_guys_evaluate_llm/,1,3,0.64,<praw.models.comment_forest.CommentForest object at 0x0000026E347A5E90>
192eids,AmbulatingGiraffe,2024-01-09 13:18:19+00:00,[Discussion] LLM Scaling Law Papers,"Hi all,

I'm looking for a landmark paper in the field of scaling laws for llms. This is for an upper level graduate seminar which is covering a variety of topics in machine learning by reading and discussing research papers. I thought scaling laws for LLMs would be an interesting topic to cover towards the end of the course. Unfortunately it's extremely far from my own research area so I'm hoping for advice on choosing an important or particularly well written paper in the field. I'm aware of Chinchilla but I'm not sure if that's the best choice or if the field has moved past that. Any help choosing a paper or papers is appreciated! Thanks in advance!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192eids/discussion_llm_scaling_law_papers/,0,1,0.57,<praw.models.comment_forest.CommentForest object at 0x0000026E347A8DD0>
191oujg,ejmejm1,2024-01-08 16:17:10+00:00,[D] Interview with Rich Sutton,"Over a month ago I asked this subs for some questions to ask Rich Sutton ([here](https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/)), and as of today the full interview is up to view at [https://youtu.be/4feeUJnrrYg](https://youtu.be/4feeUJnrrYg)!

Rich has some unique idea - or as he likes to say - what is does it out of fashion, but I'm curious to hear what others think after getting some of these ideas out there.

Outline:

 0:00 - Intro  
1:33 - Interview start  
2:04 - OpenMind Research Institute  
4:32 - History of AI  
7:13 - Is scaling easy?  
10:49 - The problem with backprop & representations  
21:22 - Rant on tunnel vision  
23:43 - New exciting things  
32:00 - Memory  
35:34 - Coming up with ideas  
43:47 - STOMP  
45:30 - Keen Technologies  
50:39 - The next stage of humanity & emotions  
1:06:25 - Extraterrestrial AI  
1:08:00 - A different approach to research  
1:21:30 - Rich's advice  
1:26:00 - Beef with RL  
1:27:07 - Bringing it all together ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191oujg/d_interview_with_rich_sutton/,1,46,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E347A9250>
192e7b9,hmi2015,2024-01-09 13:01:53+00:00,[D] tableofcontents not working in ICML 2024 template,"I am trying to create a table of content in the ICML 2024 latex template [(link)](https://media.icml.cc/Conferences/ICML2024/Styles/icml2024.zip) using

\\tableofcontents

but it just creates the title ""Contents"" without actually creating any table of contents. Did anyone face similar problem or know how to resolve this?  
[https://icml.cc/Conferences/2024/AuthorInstructions](https://icml.cc/Conferences/2024/AuthorInstructions)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192e7b9/d_tableofcontents_not_working_in_icml_2024/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E345BA3D0>
191uyuq,RelevantRevolution86,2024-01-08 20:23:37+00:00,[D] Choosing a pdf processing package in Python,"I am working on a document understanding using Deep Learning where I have to work with a lot of PDF documents. I did some research on various pdf processing packages in python. Here are some packages that are popular for processing and handling pdf using Python. However, I used to get confused about which package to use for different tasks like merging pdf, cropping pdf, and extracting text from pdf.  There is a tool also for converting scanned pdf to searchable PDFs which I did not know before doing my research.

* [PyPDF](http://localhost:3000/blogs/pdf-packages-comparison-all-you-need-to-know#pypdf) : Mostly pdf transformation
* [Pdfminer.six](http://localhost:3000/blogs/pdf-packages-comparison-all-you-need-to-know#pdfminersix) : PDF extraction including layout information
* [PdfPlumber](http://localhost:3000/blogs/pdf-packages-comparison-all-you-need-to-know#pdfplumber) : Adds table extraction feature on top of PDFminer
* [PyMuPDF](http://localhost:3000/blogs/pdf-packages-comparison-all-you-need-to-know#pymupdf) : Fastest PDF processing, Lots of feature including pdf transformation and text extraction, Table extraction etc.
* [OCRmyPDF](http://localhost:3000/blogs/pdf-packages-comparison-all-you-need-to-know#ocrmypdf) : Convert your scanned pdf to searchable pdf

I also tried to cover the topic in detail in this blog [https://pythonify.com/blogs/pdf-packages-comparison-all-you-need-to-know](https://pythonify.com/blogs/pdf-packages-comparison-all-you-need-to-know#pymupdf)

Happy Machine learning :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191uyuq/d_choosing_a_pdf_processing_package_in_python/,9,17,0.81,<praw.models.comment_forest.CommentForest object at 0x0000026E345BB810>
192cget,Cat-Evening,2024-01-09 11:18:08+00:00,[R] Need help from someone with a Baidu account,"I am trying to evaluate my own face restoration approach against multiple approaches. I would like to obtain the following model for comparison reasons:. Could someone with a Baidu account download the model and share it with me?

paper: [https://github.com/chenxx89/BFRffusion](https://github.com/chenxx89/BFRffusion)

model: [https://pan.baidu.com/s/1w3R9TuqmpAbP0tsMsGLOZA?pwd=r89i](https://pan.baidu.com/s/1w3R9TuqmpAbP0tsMsGLOZA?pwd=r89i)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192cget/r_need_help_from_someone_with_a_baidu_account/,1,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E3479C190>
191lu3v,That_Violinist_18,2024-01-08 14:00:05+00:00,[R] How to guess a gradient,"It's weird that you kinda know where the gradient is without knowing the objective function.

Paper: [https://arxiv.org/abs/2312.04709](https://arxiv.org/abs/2312.04709)

Abstract

>How much can you say about the gradient of a neural network without computing a loss or knowing the label? This may sound like a strange question: surely the answer is ""very little."" However, in this paper, we show that gradients are more structured than previously thought. Gradients lie in a predictable low-dimensional subspace which depends on the network architecture and incoming features. Exploiting this structure can significantly improve gradient-free optimization schemes based on directional derivatives, which have struggled to scale beyond small networks trained on toy datasets. We study how to narrow the gap in optimization performance between methods that calculate exact gradients and those that use directional derivatives. Furthermore, we highlight new challenges in overcoming the large gap between optimizing with exact gradients and guessing the gradients.

https://preview.redd.it/l7tm982c28bc1.png?width=1962&format=png&auto=webp&s=94d237353bc53eeb21489f6adeeaa8e43043f44a

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191lu3v/r_how_to_guess_a_gradient/,9,48,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E34788A90>
192iiuu,mego3310,2024-01-09 16:22:27+00:00,A Living Talking Wheatley. Project [P],"Hey guys, I am new to all this ML and only know a bit. I was thinking of learning this skill and I have a project planned for my summer time. long story short, it is supposed to be a portal 2 Wheatley core which can hear and respond to questions.  


I have a few things in mind in order to achive that.  


please help me if you can.  


I have a few things in mind to achieve that.  


1)  a locally running, fine-tuned mistral model copy trained on my data, Trained to talk like Wheatley.

2) a text-to-speech model for giving it the ability to speak, with EMOTIONS.  (I do wanna clone his voice)

3) another model for speech-to-text.

4) CV for other tasks

5)  another model (which I wanna make myself using PyTorch.) that takes in the voice synthesized by the aforementioned model and spits out values of servo positions for controlling the movement.

&#x200B;

is it possible to do this?

if yes please help me (I am a 15 year old btw...)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192iiuu/a_living_talking_wheatley_project_p/,3,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E347D5650>
192krj0,Obliviator77,2024-01-09 17:52:29+00:00,Trying to build a Chat Bot with keras [P],"I'm trying to build a bot from scratch using a NN and a dataset I built using chatgpt.  
I'm having some problems with the layers.


Here is the question I asked in StackOverflow with all the steps I took to fix it:
 https://stackoverflow.com/questions/77551635/getting-logits-and-labels-mismatch

Thank you for any help provided.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192krj0/trying_to_build_a_chat_bot_with_keras_p/,0,0,0.17,<praw.models.comment_forest.CommentForest object at 0x0000026E3478EAD0>
192bjk0,Straving-Dingus,2024-01-09 10:17:38+00:00,[P] Building a passive AI cognitive enhancement tool,"Me and my friends just built a proactive AI assistant that runs in the background on any iPhone, giving you ambient intelligence wherever you go.  


It listens to what you say throughout your day and gives you **perfect memory**,   
and it's a **mentor** that gives you useful advice after your conversations.   
It's also voice activated to **ask any question** whenever you want, either to improve your interactions or to define any term/concept, ideate for you, or summarize conversations for you.  
The app is live on the app store and its free! Check out our [product hunt](https://www.producthunt.com/posts/sama-ai) launch and upvote if you like it :)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192bjk0/p_building_a_passive_ai_cognitive_enhancement_tool/,1,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E345C1DD0>
192aeps,specializedboy,2024-01-09 08:58:31+00:00,[D] Trying to understand edge and node embeddings in the alignn paper,"I was reading this paper ALIGNN(atomistic line graph neural network) - [link](https://arxiv.org/pdf/2106.01829.pdf) . I am confused on how they are finding the edge and bond embedding. They mentioned that they are using atomistic line graph(no idea what is that) for the original graph and the node represents the edge in the atomistic line graph and edges represent the triplets of the atoms to include the bond angle. 

[line graph](https://preview.redd.it/oc9e4nmvpdbc1.png?width=2200&format=png&auto=webp&s=a9d3a0e30160d525e7700fa99a76de9b6eb0e251)

So how do we finalise these triplets for a graph? lets say we have a graph of 10 nodes and 10 edges how these triplets are finalised and how many triplets and in which order they are going to consider it ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192aeps/d_trying_to_understand_edge_and_node_embeddings/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E34788AD0>
191sfl4,coolwulf,2024-01-08 18:42:59+00:00,[P] NeuralRad: First FREE to use Organ and Tumor segmentation cloud,">***With collaboration with International Atomic Energy Agency (IAEA), we have learned that majority third-world country hospitals don't have the technology and corresponding infrastructure to have an easy-to-use solution for physicians, neurosurgens and medical physicists to use AI to easily and quickly contour Organ-At-Risk (OAR) or tumor during their patient treatment workflow. And we decide to work on this and make an impact for the field.***

**After two years hard work, we would like to introduce** [**service.neuralrad.com**](http://service.neuralrad.com/)**, the 1st ever free to use Full-body Organ-At-Risk (OAR) and Tumor segmentation cloud platform available to anyone.** 

We build this cloud platform with an array of high performance GPU servers (Most of them Nvidia Geforce 4090 and 3090) and dynamically allocate more than 100G gpu memory at any certain time for fast deep learning based segmentation inference. With this service, we would like to help medical physicists and physicians to tackle the troublesome lesion and OAR segmentation problems during radiation treatment workflow.

**p.s. This platform has been chosen by IAEA for the IAEA 2023 AI Workshop for Medical Physics program. (**[**https://www.iaea.org/events/evt2304232**](https://www.iaea.org/events/evt2304232)**)**

>Disclaimer: NeuralRad cloud service is not FDA approved at the moment. We recommend using this service for research and study purposes. All patient information of dicom files are automatically anonymized at the browser (client) side and only anonymized dicom data are sent to NeuralRad cloud servers for segmentation inference.

Enjoy and happy new year!

&#x200B;

https://preview.redd.it/t5c9755dh9bc1.png?width=1746&format=png&auto=webp&s=598b1337bcca5c8c70113003ed6679fb8b7fa78b

https://preview.redd.it/ndji155dh9bc1.jpg?width=1853&format=pjpg&auto=webp&s=53de4bdd072ed3c46229a34dcc7731425a8f1021

(Note: we require login to avoid abuse on our GPU server array. It's FREE to use, just register an account and we will approve manually.) (The platform demo video is available here: [https://www.youtube.com/watch?v=UX\_CIUcJ1uE](https://www.youtube.com/watch?v=UX_CIUcJ1uE))",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191sfl4/p_neuralrad_first_free_to_use_organ_and_tumor/,2,15,0.94,<praw.models.comment_forest.CommentForest object at 0x0000026E347D5850>
191zov1,redditdylanj,2024-01-08 23:34:13+00:00,[R] Best Resources/Model for Novel Research Project,"Hi all,

I am about to begin a new research project as a researcher at a university using ML to optimize a device that takes periodic driving waveforms.

My goal is to monitor this device over time and generate arbitrary waveforms and then pair generated waveforms with a measured performance (could be vector, number, or something else! this is a question we are investigating) => generate new waveforms to test => form an optimization loop.

I have lots of experience with doing simple regression tasks NN and tree models, but I don't know exactly what model to use here and I don't have much experience with closed-loop ML optimization frameworks. I spoke with a former project partner, who suggested cVAE or cGANs models to avoid potential issues with a small latent space associated with a single vector for performance. Do these seem reasonable? And if so, any good resources/codebases/papers to look at regarding these models or such optimization ML frameworks in general?

Any help would or advice be amazing!

Thank you,

Dylan",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191zov1/r_best_resourcesmodel_for_novel_research_project/,2,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E347E4510>
192avsi,fuka0105,2024-01-09 09:32:02+00:00,[P]【Participate in Anomaly Detection System survey - Get a $150 Amazon Gift Card!】,"Hi I am working at research company in Osaka, Japan.

We are currently helping our client researching about the trend of the trend of Anomaly Detection System Market.  
If you could cooperate with us, it would be great if you could DM me then I will share a Google Form Link for answering Screening Questions. Below is a detail of the project.

**Requirements**

* Working at the Anomaly Detection System maker

**Summary**

* Project: 60-90 minutes online interview survey.
* Topic: The trend of Anomaly Detection System Market
* Payment: 150USD Amazon Gift Card via email
* Privacy: Anonymous online interview via Zoom. (Audio only is available if you prefer). Your personal information will not be shared outside our company.

**This is the process:**

1. We will ask you a few screening questions. (I will send you a Google form URL via DM)
2. If you are selected, we will schedule an interview.
3. Interview.

I’m looking forward to hearing from you!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/192avsi/pparticipate_in_anomaly_detection_system_survey/,7,0,0.38,<praw.models.comment_forest.CommentForest object at 0x0000026E3480E710>
191s6ii,oren_a,2024-01-08 18:32:42+00:00,"[R] RTX 4500 vs A5000 benchmark, A5000 stronger?","See benchmark results, depends on the network/task, but I feel that the A5000 is stronger.

https://preview.redd.it/hyoe7vfif9bc1.png?width=1774&format=png&auto=webp&s=f4ef7df9072991fd477d5afe703a4e627622e51f",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191s6ii/r_rtx_4500_vs_a5000_benchmark_a5000_stronger/,0,6,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E345C8590>
1925m9k,Instantinopaul,2024-01-09 04:08:22+00:00,[D] Is there a holy grail for video summarization? Log & retrieve like a pro!,"Hey Reddit fam, I'm looking for the ultimate video summarization method. Not just your average bullet-point list, but something rich, detailed, and interactive. Think of it as turning every video into a searchable database of events, ready to answer any question I throw at it later.

Here's the dream workflow:

1. **Log every key event:** Capture all the important stuff – who, what, where, when, why. Did someone trip and fall? Did a robot dance? Log it!
2. **Decent detailing:** Not just ""person fell,"" but ""clumsy tourist tripped on banana peel outside museum entrance."" The more descriptive, the better.
3. **Q&A retrieval:** Later, I should be able to ask questions like ""Show me all the funny falls"" or ""What did the robot say during its speech?"" and get precise video snippets in response.

I know, it sounds ambitious, but is it even possible?

I've explored some options:

Automatic transcription & keyword extraction: Sounds promising, but can miss unspoken events (someone fell).

Human annotation: Accurate, but time-consuming and expensive.

AI/ML-based approaches: Summarizing frame by frame but we lose timeflow of an activity (ex: walking in a circle).

So, Reddit, I need your wisdom! Have you encountered any tools or techniques that come close to my video summarization dream? Or am I chasing unicorns here?

**P.S.** Feel free to hijack this thread with your own video summarization woes and wishes! The more the merrier (and the closer we get to that holy grail)!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1925m9k/d_is_there_a_holy_grail_for_video_summarization/,2,1,0.56,<praw.models.comment_forest.CommentForest object at 0x0000026E34806310>
191isia,myselfitself,2024-01-08 11:06:27+00:00,[D] 3090 vs the new 40 series equivalent,"I found some deals for 3090 (new) from:

* [MSI](https://www.msi.com/Graphics-Card/GeForce-RTX-3090-VENTUS-3X-24G-OC) (1260 USD)
* [PALIT](https://www.palit.com/palit/vgapro.php?id=3795&lang=en) (965 USD)
* [PALIT OC](https://www.palit.com/palit/vgapro.php?id=3795&lang=en) (900 USD)

I want to know if the lower models from the 40 series (mainly 4070 and 4070 TI since the 4080 is way above my budget with the power supply upgrade that is needed) are worth it for gaming/AI versus the lack of V-RAM 

Note that the card availabilities and choice are **limited** in my case, In addition, my power supply has to be changed since it's only 650W gold (open for power supply upgrade suggestions as well).

Thank you",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191isia/d_3090_vs_the_new_40_series_equivalent/,17,22,0.88,<praw.models.comment_forest.CommentForest object at 0x0000026E3484A850>
191iqxj,APaperADay,2024-01-08 11:03:54+00:00,[R] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache,"**Paper**: [https://arxiv.org/abs/2401.02669](https://arxiv.org/abs/2401.02669)

**Abstract**:

>The rapid proliferation of Large Language Models (LLMs) has been a   driving force in the growth of cloud-based LLM services, which are now   integral to advancing AI applications. However, the dynamic   auto-regressive nature of LLM service, along with the need to support   exceptionally long context lengths, demands the flexible allocation and   release of substantial resources. This presents considerable challenges   in designing cloud-based LLM service systems, where inefficient   management can lead to performance degradation or resource wastage. In   response to these challenges, this work introduces **DistAttention**,  a  novel distributed attention algorithm that segments the KV Cache  into  smaller, manageable units, enabling distributed processing and  storage  of the attention module. Based on that, we propose **DistKV-LLM**,  a  distributed LLM serving system that dynamically manages KV Cache and   effectively orchestrates all accessible GPU and CPU memories spanning   across the data center. This ensures a high-performance LLM service on   the cloud, adaptable to a broad range of context lengths. Validated in a   cloud environment with 32 NVIDIA A100 GPUs in configurations from 2 to   32 instances, **our system exhibited 1.03-2.4x end-to-end  throughput  improvements and supported context lengths 2-19x longer than  current  state-of-the-art LLM service systems, as evidenced by  extensive testing  across 18 datasets with context lengths up to 1,900K.**",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191iqxj/r_infinitellm_efficient_llm_service_for_long/,2,22,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E347EAE50>
19257jo,ZanePWD,2024-01-09 03:47:59+00:00,[D] Cyber Security - AI/ML related training resources ?,"Hello, I'm looking for cyber security related training and information in relation to AI.   


I run a very large cyber team with a number of them, including myself wanting to up skill in this area as our business is going down the ML/AI route with a number of technologies.   


Anything that could help in this area for training / career pathways would be welcomed.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19257jo/d_cyber_security_aiml_related_training_resources/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E345C1D90>
191juun,ale152,2024-01-08 12:12:07+00:00,[P] Is there an equivalent of Bayesian optimization that works only with comparative results?,"Hello everyone, I'm working on a problem where I need to find the best set of parameters (10 of them) that optimises a very costly objective function. Normally, I would use a Bayesian optimisation, but in this specific case, I don't have access to the actual objective function, the only thing that I can calculate is weather the function is higher with a certain set of parameters A or B. I don't know how the actual values of the function, nor its derivatives. All I can do is to compare the two set of parameters and tell which one produces a lower value of the function.

Any advice on what I could use to find the best of parameters to optimise this function?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191juun/p_is_there_an_equivalent_of_bayesian_optimization/,9,16,0.91,<praw.models.comment_forest.CommentForest object at 0x0000026E34844850>
191qvst,mtbarta,2024-01-08 17:41:02+00:00,[P]Retri-evals: Retrieval Evaluation Pipelines,"Hey all,

 We've been working on building retrieval pipelines for LLMs, and like many others we questioned how changes to our pipeline (e.g. chunking, cleaning) would affect the overall outcome. 

We also faced a problem of what data to evaluate against. MTEB is used academically, but using our own data would be more reliable.

Retri-evals is hoping to solve these problems. We pulled out our MTEB abstractions that let us evaluate against open source datasets, and we're going to open source the code we use to automatically generate evaluation datasets from production data.

I'd love to hear your thoughts! We're looking to complement existing solutions in this space with tooling that makes it easier to get to production.

https://github.com/DeployQL/retri-evals",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191qvst/pretrievals_retrieval_evaluation_pipelines/,0,5,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E347F13D0>
191y9lg,battlefieldanalytica,2024-01-08 22:35:25+00:00,[D] Seeking Advice on Optimal Initialization of n_neighbors for LocalOutlierFactor with scikit-learn,"Hello r/machinelearning community,

I am working on a project using the LocalOutlierFactor model from scikit-learn for anomaly detection. I am wondering about the best practices for choosing the initial value for the n_neighbors parameter.

Thank you for your help",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191y9lg/d_seeking_advice_on_optimal_initialization_of_n/,0,2,0.75,<praw.models.comment_forest.CommentForest object at 0x0000026E34841310>
1922hly,notEVOLVED,2024-01-09 01:39:20+00:00,[D] Caching predictions from a teacher model,Hello. I am training a model through knowledge distillation and I don't want to run forward passes on the teacher model every epoch on the same data. Is there an efficient way to cache the predictions to disk?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1922hly/d_caching_predictions_from_a_teacher_model/,3,0,0.43,<praw.models.comment_forest.CommentForest object at 0x0000026E3484BC90>
191tcuu,impl66,2024-01-08 19:19:45+00:00,[D] Seeking Guidance on Efficient Extraction of Relevant Tables and Columns for a Database-driven Q&A,"I am working on an application that aims to answer user queries about a database with hundreds of tables and thousands of columns. Each table and column is well-described (as in there are clear descriptions of what each table and column does). First, I want to extract the top n, most relevant tables and columns based on user queries so that I can send just those relevant tables and columns to an LLM as schema/context for it to build a SQL expression that I can further use to answer user's question.

I am facing challenges in efficiently extracting just the relevant tables and columns. My current approach using semantic search is not yielding satisfactory results for this problem.

Could anyone suggest alternative approaches or techniques for extracting relevant tables and columns from a large database for better results in a question-answering scenario? Your insights and experiences would be greatly appreciated!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191tcuu/d_seeking_guidance_on_efficient_extraction_of/,0,3,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34800A50>
191oiq2,xlext,2024-01-08 16:03:08+00:00,Low Latency Computer Vision Inference Server [P],"I am trying to deploy a computer vision model to run predictions on a live video feed (30fps). My idea was to create a 'server' app within a docker container that would load the model as the container starts and then listen for requests to run predictions. The requests would be coming from another process on the same machine (which acquires frames from several cameras). The problem I am having is that communicating images from one process to the dockerized server is way too slow because of serialization. My question is: is there a way to decrease the latency with this setup? Here is what I thought of:

1. Mounting the camera within the docker app that runs the model: unfortunately that's not possible because of other design constraints.
2. Using a volume bind and going through disk I/O: is too slow.
3. Running a simple HTTP server: serializing numpy images takes too long.
4. Using a message broker: I tried RabbitMQ and Kafka but the serialization problem remains.

Is there an option I have not considered, or is this just not the right place to use Docker?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191oiq2/low_latency_computer_vision_inference_server_p/,2,3,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E34721BD0>
191npsz,ewelumokeke,2024-01-08 15:28:06+00:00,[D] Model Drop in accuracy when converting from Native PyTorch to TensorFlow Lite and CoreML.,"Left is Native PyTorch, middle is TensorFlow Lite, right is CoreML",MachineLearning,https://www.reddit.com/gallery/191npsz,11,3,0.55,<praw.models.comment_forest.CommentForest object at 0x0000026E3484C350>
191qeq2,PhilipJanFranjo,2024-01-08 17:22:18+00:00,[R] Seeking advice for Video Machine Learning Predictive model,"Hello! I'm relatively new to machine learning, and I have an overarching goal in mind. Please let me know how feasible this is, and if so, what general approach I should take.

I have quite a large dataset of videos. Each video is an 'animatic' of an animated shot. I have another dataset that represents how long each department took, in hours, to complete their stage of the shot. How could I go about creating a model with machine learning to then predict how long a new animatic would take in each department? Ideally, the model would identify things like camera movement, amount of characters, amount of motion (or rather unique drawings in the animatic), camera placement (full body, waist high, etc.), general style, etc. to make an educated estimate for the duration of each department.

I have pre-populated metrics for each video that include Character Value (a subjective count of characters, so half-body characters would be 0.5), Difficulty (subjective difficulty from 0.5-2), and Frame Duration of the animatic. Would it be possible to have the model identify patterns that correlate to higher hour counts on it's own, or would they have to be pre-determined (like the list of factors I mentioned in the above paragraph).

So far, I've looked into pytorchvideo, which to my understanding, will assist in identifying pre-determined factors. It seems like the most promising route, but I'm having trouble getting started.

I'd dearly appreciate any guidance or tips!

Thanks,

\-Phil F",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191qeq2/r_seeking_advice_for_video_machine_learning/,2,2,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34801F90>
190to69,Ayakalam,2024-01-07 14:46:58+00:00,[D] Why are almost all probabilistic derivations so hard to follow in ML?,"I consider myself really good at math, having even taught it to university students, active in the field of ML, etc.

Yet, I find most - if not all - papers that deal with anything remotely probabilistic in ML to be atrociously explained.

Recently I decided to really get to understanding the OG \[DDPM\]([https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)) paper.

Here is part of the derivation where they ... somehow... insert the KLD. It's not clear to me at all how this jump was made. Yes, I have looked at the definition of KLD, yes, I have googled around but everyone seems to just take this on faith. ChatGPT says ""theres a hidden expectation that's not shown"".

https://preview.redd.it/glvvzcc351bc1.png?width=2014&format=png&auto=webp&s=d4c95a5716c0b8113e9a3346b8f99e3c5a3db919

Does anyone know?

&#x200B;

**Update:** Thanks everyone for the comments, my conclusion here is that DDPM paper has an error in it, namely, the above image. 

The error is because they show the outer expectation not being used up, where indeed it IS being used up. 

I found a correct write-up of the derivation here in Calvin's paper [here](https://arxiv.org/pdf/2208.11970.pdf). And here is the image: 

&#x200B;

https://preview.redd.it/54o6592vj2bc1.png?width=2370&format=png&auto=webp&s=78d089d3d5c183f286bac15d3e6d38ed5fa4e37e

The above is correct, while the DDPM paper is wrong. 

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190to69/d_why_are_almost_all_probabilistic_derivations_so/,58,208,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E34894FD0>
191tk1f,aliaslight,2024-01-08 19:27:55+00:00,[D] Any interesting work in non-research based ML?,"So I'm diving into machine learning (specifically reinforcement learning) and hope to eventually be able to improve current technological systems using it. But almost everything I see in this field looks like research, and hence companies also seem to be inclined towards PhDs, but I'm not entirely sure if I would like the research path. I have heard of the term applied machine learning, but I wanted to know if there is more to that than just learning syntax of some modules and tuning of hyperparameters through trial and error. Is there room for creative problem solving in applied machine learning?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191tk1f/d_any_interesting_work_in_nonresearch_based_ml/,4,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34879850>
191sjlg,tp_njmk,2024-01-08 18:47:32+00:00,[D] Help me build a budget(ish) deep learning rig,"I'm looking to build a deep learning rig for personal projects & learning:

&#x200B;

* 3090 24 GB FE (used) £600 ($800) 
* cpu: [Ryzen 9 5900X](https://www.amazon.co.uk/AMD-Ryzen-5900X-Processor-Cache/dp/B08164VTWH/) £280 ($350)
* motherboard: [MSI MPG B550](https://www.amazon.co.uk/MSI-GAMING-Motherboard-DisplayPort-Generation/dp/B08B4V6H3N/) £120 ($150)
* ram: 64gb c16 3200mhz £136 ($175)
* psu: MSI A1000G PCIE5 1000 W 80+ £140 ($180)

I'd be using it for vision DL, training toy stable diffusion models (fastai p2 ftw!) and generally tinkering with lora models.

The only thing I'm concrete on is the used 3090, feel free to swap out any of the other parts. Any personal experience is very much appreciated, I've been figuring it out from other reddit posts and [https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#CPU), although I guess this guide is quite old now.

Any help would be appreciated!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191sjlg/d_help_me_build_a_budgetish_deep_learning_rig/,4,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E3487A3D0>
191i9m7,APaperADay,2024-01-08 10:32:14+00:00,[R] Mindstorms in Natural Language-Based Societies of Mind,"**OpenReview** (*R0-FoMo Oral*): [https://openreview.net/forum?id=zd2qE6BBdU](https://openreview.net/forum?id=zd2qE6BBdU)

**arXiv**: [https://arxiv.org/abs/2305.17066](https://arxiv.org/abs/2305.17066)

**Code**: [https://github.com/mczhuge/NLSOM](https://github.com/mczhuge/NLSOM)

**Abstract**:

>Both Minsky's ""society of mind"" and Schmidhuber's ""learning to think""  inspire diverse societies of large multimodal neural networks (NNs) that  solve problems by interviewing each other in a ""mindstorm."" Recent  implementations of NN-based societies of minds consist of large language  models (LLMs) and other NN-based experts communicating through a  natural language interface. In doing so, they overcome the limitations  of single LLMs, improving multimodal zero-shot reasoning. In these  natural language-based societies of mind (**NLSOMs**), new agents -- all  communicating through the same universal symbolic language -- are easily  added in a modular fashion. To demonstrate the power of NLSOMs, we  assemble and experiment with several of them (having up to 129 members),  leveraging mindstorms in them to solve some practical AI tasks: visual  question answering, image captioning, text-to-image synthesis, 3D  generation, egocentric retrieval, embodied AI, and general  language-based task solving. We view this as a starting point towards  much larger NLSOMs with billions of agents-some of which may be humans.  And with this emergence of great societies of heterogeneous minds, many  new research questions have suddenly become paramount to the future of  artificial intelligence. What should be the social structure of an  NLSOM? What would be the (dis)advantages of having a monarchical rather  than a democratic structure? How can principles of NN economies be used  to maximize the total reward of a reinforcement learning NLSOM? In this  work, we identify, discuss, and try to answer some of these questions.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191i9m7/r_mindstorms_in_natural_languagebased_societies/,0,3,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E3485A790>
190q1vb,Instantinopaul,2024-01-07 11:19:08+00:00,"[D] So, Mamba vs. Transformers... is the hype real?","Heard all the buzz about Mamba, the new kid on the sequence modeling block. Supposedly it's faster, handles longer sequences better, and even outperforms Transformers on some tasks. But is it really a throne-stealer or just another flash in the pan?

My perception:

Strengths: Mamba boasts efficient memory usage, linear scaling with sequence length, and impressive performance in language and DNA modeling. Plus, it ditches the attention mechanism, potentially paving the way for faster inference. 

Weaknesses: Still early days, so Mamba's long-term stability and performance across diverse tasks remain to be seen. And while it doesn't need attention, its state space approach might be trickier to grasp for some folks. 

To the AI aficionados out there, is Mamba just the next shiny toy, or a genuine paradigm shift in sequence modeling? Will it dethrone the mighty Transformer, or coexist as a specialized tool? Let's hear your thoughts!

[https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190q1vb/d_so_mamba_vs_transformers_is_the_hype_real/,87,279,0.95,<praw.models.comment_forest.CommentForest object at 0x0000026E348D49D0>
191652a,Singularian2501,2024-01-07 23:30:52+00:00,[R] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs (SEAL) - New York University 2023 - 25% better than GPT-4V in search of visual details!,"Paper: [https://arxiv.org/abs/2312.14135v2](https://arxiv.org/abs/2312.14135v2) 

Github: [https://github.com/penghao-wu/vstar](https://github.com/penghao-wu/vstar) 

Abstract:

>When we look around and perform complex tasks, how we see and selectively process what we see is crucial. However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images. To address this, we introduce V\*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements. This integration results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL). We further create V\*Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details. **Our study highlights the necessity of incorporating visual search capabilities into multimodal systems.** 

https://preview.redd.it/0b78lih1r3bc1.jpg?width=1663&format=pjpg&auto=webp&s=78670288430588cfee2db280cb75e348254ec0eb

https://preview.redd.it/8kap1jh1r3bc1.jpg?width=1661&format=pjpg&auto=webp&s=d6e8a372cd91976e6e35710d32992a443981f06e

https://preview.redd.it/oakf3lh1r3bc1.jpg?width=1247&format=pjpg&auto=webp&s=612ab61b763254f5cabb3a93990cc5baa2a917e3

https://preview.redd.it/mta8emh1r3bc1.jpg?width=653&format=pjpg&auto=webp&s=209871901bf2ba26537b1587c4be388df055f30b",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191652a/r_v_guided_visual_search_as_a_core_mechanism_in/,0,24,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34773A50>
191o2e5,Dr_Superfluid,2024-01-08 15:43:51+00:00,[D] Portable AI machine - possibly M3 Max MacBook?,"Hey all,

In my workgroup we are currently in need of a machine that will need to move fairly regularly, for heavy duty AI usage. Our current setup is a 7950X, 32GB RAM, RTX 4090 (24GB). We are basically using PyTorch for most calculations (which are video-image analysis frame-by-frame). The workload maxes out the 4090 and uses all of the VRAM of the GPU (we are memory limited at the moment), and takes about 30 minutes. This is the timescale that we would more or less need to hit with the portable machine as well. We kind of want to avoid having a huge desktop+monitor+peripherals to set up everytime.

I know that the M chips are optimised to run AI and I have read that PyTorch does use the neural engine of the machine and also that their shared memory can be used by the gpu giving the maxed out version GPU access to 128 GB RAM.

Do you think that a completely maxed out 16"" M3 Max with 128GB RAM would be able to deliver similar or relatively similar performance to our current setup?

Or do you have any other suggestions of a machine that could be used?

The only Mac that we have access to right now is my personal 14"" M1 Pro binned model (16GB RAM) so I cannot really run any actual comparisons.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191o2e5/d_portable_ai_machine_possibly_m3_max_macbook/,0,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E3487AD10>
191o1s4,MustafaAlahmid,2024-01-08 15:43:05+00:00,[D] How to load distcp checkpoints ?,"I have fintuned full aparmeters of mistral 7-b model, and i have used FDSP in HF accelerate

I have a checkpoint which is place in a folder pytorch\_model\_0, which contains multiple distcp files.

how can i load them and merge them in the model ?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191o1s4/d_how_to_load_distcp_checkpoints/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34835FD0>
191t9m2,alphamemedream,2024-01-08 19:16:09+00:00,[D] Finding people with an art and ML background,"Hi everyone,

I'm working for a creative studio in Paris. The creative studio is run by a renown artist and there are 4 devs working for him. We're working with AI, blockchain and web3 in the context of culture.  We have many large cultural projects happening this year like experimental AI projects with film directors. We're looking for people who have an actual art and CS (ML ideally) background. They seem to be very hard to find. Would anyone know where I should look? Or if anyone knows people who would be interested, please message me. France seems to be very conservative in terms of these kind of profiles.

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191t9m2/d_finding_people_with_an_art_and_ml_background/,0,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E34751550>
191kn35,xrdts_99tx,2024-01-08 12:58:08+00:00,[D] PhD in Europe,"European countries are considered a good destination to do a PhD in computer science and engineering (e.g. machine learning, computer vision, computer graphics, quantum computing). But clearly, there are some with a higher demand (i.e. receive the most students) and any information about it can be useful when applying for publicly advertised PhD positions or funding options.

So, where did you do or are you doing your PhD? Also feel free to comment anything.

[View Poll](https://www.reddit.com/poll/191kn35)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191kn35/d_phd_in_europe/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E3486E050>
191erp3,Ok_Law_3126,2024-01-08 06:38:04+00:00,[D] ML models for feature prioritization,"Are there any ML models that can be used for feature prioritization? We use ProductBoard and it is a pain giving prioritization or linking every feedback to an impact metric and its often not accurate.   


Wondering if there is a model that can be used similar to other industrial engineering models being used in factories etc. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191erp3/d_ml_models_for_feature_prioritization/,2,3,0.72,<praw.models.comment_forest.CommentForest object at 0x0000026E34806850>
191jswe,sivav-r,2024-01-08 12:08:43+00:00,"[D] Which websites host AI Competitions, similar to Topcoder's Intellicase Bot (GPT) Challenge?",I'm looking for both R and D competitions.,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191jswe/d_which_websites_host_ai_competitions_similar_to/,0,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E32DE7B10>
191jlly,Euphoric-Chart1428,2024-01-08 11:56:22+00:00,"[R], [P] Regenerate Feature in Chatbot","[R], [P] Has anyone implemented the regenerate response feature on their chatbot using any API based LLM? While ensuring memory remains intact. Any help or reference article would be highly appreciated.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191jlly/r_p_regenerate_feature_in_chatbot/,0,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E34836D90>
191ccbb,Anirban_Hazra,2024-01-08 04:21:40+00:00,[News] Are Natural Language capable Personal Robot Assistants the Future of Google's Capabilities?,,MachineLearning,https://www.digitallynomad.in/post/google-deep-mind-personal-robot-assistants,0,5,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E3486FE90>
191uwym,KROOL4E,2024-01-08 20:21:33+00:00,[D] How can I get experience before I finish my degree in Machine Learning !!?,"I will soon finish on my Mechanical Engineering bachelors degree at a well respected UK uni and I will be studying a Machine Learning MSc next year (at the same uni). I have some programming experience, general engineering experience, and a month in industry where I programmed robots. I am under the impression that experience would be really useful in securing myself a job post-grad in a ML/DS type role.

**So, how can I get any experience in ML/DS, or anything related, before I finish my Masters?** 

I am open to anything, and if I don't manage any industrial experience, I planned on just doing online courses and working at Tesco's (or something). I would be open to a Phd after this MSc but it is not ideal, I would prefer a ML/DS job (or something that could set me on that kind of path)

Any advice would be greatly appreciated! and feel free to correct me if I am misled in any way. ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191uwym/d_how_can_i_get_experience_before_i_finish_my/,3,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E34807710>
190vo6n,Snoo_72181,2024-01-07 16:18:21+00:00,[D] Faster way to read ML papers?,"It may seem like I am trying to cut corners, but I want to know first if a paper I found indeed provides insights on how I can solve my ML problem at hand, and only after that I would to read the details.

Any tips would be well appreciated",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190vo6n/d_faster_way_to_read_ml_papers/,15,21,0.73,<praw.models.comment_forest.CommentForest object at 0x0000026E348DAC50>
1917xgg,Ermundo,2024-01-08 00:49:03+00:00,Transpose XGBoost Model to Excel [Project]," 

Hello,

I  am interested in exporting an XGBoost model to excel. I created an  XGBoost model in R with the caret package and XGBoost package. The final  model includes 50 decision trees. I was thinking that I could  essentially transpose all the decision trees into excel manually by  making if-then statements in excel (I would have a row of 50 values with  each value being an if-then statement representing a decision tree).  I  tried doing this manually but am having a tough time trouble shooting  why my predictions are off in excel. I have a few questions

1. How does XGBoost determine final score from all the decision trees. Is it average or summation?
2. Is there any easy way to transpose an XGBoost model into excel or am I just nuts to try this.

Thank you all for your help",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1917xgg/transpose_xgboost_model_to_excel_project/,11,5,0.78,<praw.models.comment_forest.CommentForest object at 0x0000026E34897ED0>
190c7y2,BlupHox,2024-01-06 22:33:40+00:00,[D] How does our brain prevent overfitting?,"This question opens up a tree of other questions to be honest It is fascinating, honestly, what are our mechanisms that prevent this from happening?

Are dreams just generative data augmentations so we prevent overfitting?

If we were to further antromorphize overfitting, do people with savant syndrome overfit? (as they excel incredibly at narrow tasks but have other disabilities when it comes to generalization. they still dream though) 

How come we don't memorize, but rather learn?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190c7y2/d_how_does_our_brain_prevent_overfitting/,250,366,0.86,<praw.models.comment_forest.CommentForest object at 0x0000026E349995D0>
191or1s,Snoo89157,2024-01-08 16:13:02+00:00,[D],Any idea if Mac Max Pro of 128 TB working memory and 8 TB is worth of difference of 2500 EUR with first one behind 64TB / 4TB if I wanna test machine learning and other demanding video making and editing stuff?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191or1s/d/,20,0,0.25,<praw.models.comment_forest.CommentForest object at 0x0000026E34930350>
190rw8w,Electronic-Letter592,2024-01-07 13:14:11+00:00,[Discussion] Can I use LORA/QLORA to fine-tune BERT?,"BERT, technically a LLM as well, is traditionally fine tuned/domain adapted with masking words on a domain specific dataset. But can I also use qlora with BERT based models for more efficient fine-tuning?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190rw8w/discussion_can_i_use_loraqlora_to_finetune_bert/,19,18,0.96,<praw.models.comment_forest.CommentForest object at 0x0000026E349A56D0>
191i2no,freaky_eater,2024-01-08 10:19:17+00:00,[D] Attend these events in Switzerland if you are interested in industrial Biomedical AI,"Dear brothers and sisters in and around Switzerland,

If you are PhD students or STEM graduates in Switzerland, and are interested in industrial biomedical AI, then you must attend these events to learn more about what goes in the industry.

Industrial Biomedical AI

\- Intelligent Health AI UK  
\- Intelligent Health AI Basel  
\- Web Summit Lisbon  
\- Festival of Biologics Basel  
\- Festival of Biologics USA  
\- EPFL Applied Machine Learning Days  
\- Swiss MedTech day  
\- SwissText  
\- BioTechX Basel  
\- BioTechX USA  
\- NLP Summit Healthcare  
\- BOOM summit Basel

If you want to have more information on these events read the [Medium post](https://anjani-kd.medium.com/dear-phds-in-switzerland-attend-these-events-if-you-are-passionate-about-industrial-biomedical-ai-325e3cd04ceb). If you know more such events that help graduates move to industry, let's discuss these events. 

&#x200B;

This year let's empower others. :) ",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191i2no/d_attend_these_events_in_switzerland_if_you_are/,0,0,0.17,<praw.models.comment_forest.CommentForest object at 0x0000026E348ED990>
191axx1,GoofyRoach,2024-01-08 03:09:48+00:00,[P]My go at explaining the transformer architecture.,"[Understanding Transformers architecture with Pytorch code](https://medium.com/@ashishbisht0307/understanding-transformers-architecture-with-pytorch-code-c422c5fb1cd2)

Code: [https://github.com/ashish-s-bisht/TransformerArchitecturePytorch](https://github.com/ashish-s-bisht/TransformerArchitecturePytorch)

&#x200B;",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191axx1/pmy_go_at_explaining_the_transformer_architecture/,1,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E348ECED0>
191jwjn,Accurate-Screen8774,2024-01-08 12:14:59+00:00,[Research] WebLLM - Is Decentralized AI Possible?,"im not an expert in AI or webGPU, but i have a trivial understanding of it.

my understanding of it is something like: ""AI is often used on GPU because it needs multiple CPUs"".

i am working on a project with decentralized P2P comminication (https://positive-intentions.com/). i am using webRTC for the P2P connection. this makes the communication fast between peers (especially on the same LAN network).

there is a project that enables LLMs to be run on the browser (https://webllm.mlc.ai/). i have tested it and it works in my app. it can also work in a way where a peer (mobile phone) can make requests to a peer (desktop computer that supports webGPU) and outsource the AI computation to get a response. (this can be intepreted to be something like a selfhosted AI between your phone and desktop-pc).

i am wondering, if a core requirement for AI is to have multiple GPUs and it is possible to connect to multiple peers. is it possible to split the AI computation between peers?

my app should allow for sending ""any"" payload between peers, but i am not sure how computation for AI can be split between peers.

if anyone can share any guidance on the matter it might be an option for me to ""figure it out"".",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191jwjn/research_webllm_is_decentralized_ai_possible/,12,0,0.35,<praw.models.comment_forest.CommentForest object at 0x0000026E349A6290>
190z1o4,gr_ferro,2024-01-07 18:42:21+00:00,[P] LiDAR and segmentation,"Good morning, everyone. Has anyone worked with LiDAR and has experience to help me? I need to calculate the volume of items using point clouds extracted with LiDAR. However, there will be multiple objects in the image. How can I select my object of interest? Should I segment the objects in the original image with a certain model and then locate this object in the point cloud, or should I only use the image with the point cloud?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190z1o4/p_lidar_and_segmentation/,0,3,0.8,<praw.models.comment_forest.CommentForest object at 0x0000026E3493A3D0>
1917oz8,StellaarMonkey,2024-01-08 00:38:31+00:00,[D] Does Keras EarlyStoppingCallback restore best weights when NaN loss is encountered?,"I know there is a callback called TerminateOnNaN callback and I know my NaNs due to exploding gradients. The reason I don't want to use this callback is because, if my intuition is right, exploded gradients can come back down. So my questions are:

1. Is it possible for a gradient to unexplode after exploding (meaning come back down under 2\^32)?
2. Does Keras EarlyStoppingCallback restore best weights if/when NaN loss is encountered?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1917oz8/d_does_keras_earlystoppingcallback_restore_best/,5,0,0.44,<praw.models.comment_forest.CommentForest object at 0x0000026E349A6C90>
191748v,janksm1,2024-01-08 00:12:53+00:00,[D] How to finetune a pretrained LLM to take and embedding and create a string of text,"I would like to use LoRa on a model like phi2 to train it to be used with an autoencoder.

So i would like to know if i can train a pretrained LLM to take text and produce an embedding then take the embedding as the encoder then train another model to take that embedding and create string of text. This will be trained like an autoencoder.

How can i train a LLM to produce the last token as an embedding and how can i train the model to understand the first token as an embedding?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191748v/d_how_to_finetune_a_pretrained_llm_to_take_and/,3,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E349AFAD0>
19171un,RevolutionaryTeach15,2024-01-08 00:10:03+00:00,[Discussion] seeking Advice,"Hello, I'm a second-year master's student about to start working on my end-of-study project, focusing on utilizing LLMs for sentiment analysis. I'm looking forward to making a meaningful contribution with my work. My goal is to try and publish my work and maybe get a scholarship with it. I am new to the research field, and it seems like I want everything to be on a golden plate. But I actually want to achieve something, perhaps looking for a Ph.D. in a really good university somewhere out of the country I am right now.
Could you provide advice on achieving the most with my project? Any tips on staying updated and relevant, as well as recommendations for essential frameworks and skills to learn (that can help me in my project and for my future goal) would be greatly appreciated!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19171un/discussion_seeking_advice/,0,0,0.44,<praw.models.comment_forest.CommentForest object at 0x0000026E34936B10>
190ggrg,Chromobacterium,2024-01-07 01:47:29+00:00,[R][P] Are denoising autoencoders out of style?,"Score matching models, particularly their denoising score matching realizations are very hot right now. However, almost all of them are in some form or another just large stochastic denoisers. I am wondering why denoising autoencoders haven't had as much research put into them, considering that both are theoretically and functionally similar (the denoising score matching paper derived in \[1\] explicitly makes the connection between the two).

Also, autoencoders are simply much more flexible than their U-Net counterparts, since they can be used for low-dimensional latent-variable modelling (e.g. VAEs). I am aware of several papers that combine denoising autoencoders with both variational autoencoders \[2\] and adversarial autoencoders \[3\], which is a decent start in my opinion. 

In my own research, I am finding major potential in them for probabilistic modelling in their own right.

&#x200B;

References

\[1\] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 2011.

\[2\] Antonia Creswell, Kai Arulkumaran, Anil Anthony Bharath. Improving Sampling from Generative Autoencoders with Markov Chains. arXiv, 2016.

\[3\] Antonia Creswell, Anil Anthony Bharath. Denoising Adversarial Autoencoders. arXiv, 2017.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190ggrg/rp_are_denoising_autoencoders_out_of_style/,10,32,0.97,<praw.models.comment_forest.CommentForest object at 0x0000026E348188D0>
190pedy,justnews_app,2024-01-07 10:33:39+00:00,[D] The paradox of AI to AI conversations,,MachineLearning,https://disruptonomics.com/ai_chat/,0,3,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E349B6AD0>
191itad,Kv-boii,2024-01-08 11:07:55+00:00,"[D] Coders of reddit, what's the best approach and algorithms for a prediction model?","I want to create a rank prediction model for the grade marks data of students, from the past rank allocation I want to predict what rank will it be for the future years",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/191itad/d_coders_of_reddit_whats_the_best_approach_and/,6,0,0.12,<praw.models.comment_forest.CommentForest object at 0x0000026E349B60D0>
190md55,APaperADay,2024-01-07 07:03:29+00:00,[R] Unsupervised Universal Image Segmentation,"**Paper**: [https://arxiv.org/abs/2312.17243](https://arxiv.org/abs/2312.17243)

**Code**: [https://github.com/u2seg/U2Seg](https://github.com/u2seg/U2Seg)

**Project page**: [https://u2seg.github.io/](https://u2seg.github.io/)

**Abstract**:

>Several unsupervised image segmentation approaches have been proposed   which eliminate the need for dense manually-annotated segmentation   masks; current models separately handle either semantic segmentation   (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER),   but not both (i.e., panoptic segmentation). We propose an Unsupervised   Universal Segmentation model (**U2Seg**) adept at  performing various image  segmentation tasks -- instance, semantic and  panoptic -- using a novel  unified framework. U2Seg generates pseudo  semantic labels for these  segmentation tasks via leveraging  self-supervised models followed by  clustering; each cluster represents  different semantic and/or instance  membership of pixels. We then  self-train the model on these pseudo  semantic labels, yielding  substantial performance gains over specialized  methods tailored to each  task: a +2.6 AP**^(box)**  boost vs. CutLER in  unsupervised instance segmentation on COCO and a  +7.0 PixelAcc increase  (vs. STEGO) in unsupervised semantic segmentation  on COCOStuff.  Moreover, our method sets up a new baseline for  unsupervised panoptic  segmentation, which has not been previously  explored. U2Seg is also a  strong pretrained model for few-shot  segmentation, surpassing CutLER by  +5.0 AP**^(mask)**  when trained on a low-data  regime, e.g., only 1% COCO labels. We hope  our simple yet effective  method can inspire more research on  unsupervised universal image  segmentation.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190md55/r_unsupervised_universal_image_segmentation/,0,6,0.81,<praw.models.comment_forest.CommentForest object at 0x0000026E3481A110>
1903k24,ArtZab,2024-01-06 16:23:04+00:00,[D] Incredible results with Long Agent Tree Search with open source models,"Hello,

I saw GPT-4 with Long Agent Tree Search topping the HumanEval with a 94.4% pass@1 for a few weeks now. [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval)

&#x200B;

The authors of the [original paper](https://arxiv.org/abs/2310.04406) posted their code in their [official github repo](https://github.com/andyz245/LanguageAgentTreeSearch) . I had to change some code to try it out with CodeLlama-7b and the human eval with pass@1 and only 2 max iterations increases HumanEval score from 37% to about 70%.

This is some incredible results in my opinion because this score is higher than GPT-3.5 with only a 7b model. I assume more testing has to be done, but nevertheless I am surprised people are not talking more about this.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/,9,114,0.98,<praw.models.comment_forest.CommentForest object at 0x0000026E349C3CD0>
190mlil,APaperADay,2024-01-07 07:18:17+00:00,[R] VCoder: Versatile Vision Encoders for Multimodal Large Language Models,"**Paper**: [https://arxiv.org/abs/2312.14233](https://arxiv.org/abs/2312.14233)

**Code**: [https://github.com/SHI-Labs/VCoder](https://github.com/SHI-Labs/VCoder)

**Dataset**: [https://huggingface.co/datasets/shi-labs/COST](https://huggingface.co/datasets/shi-labs/COST)

**Project page**: [https://praeclarumjj3.github.io/vcoder/](https://praeclarumjj3.github.io/vcoder/)

**Hugging Face Space**: [https://huggingface.co/spaces/shi-labs/VCoder](https://huggingface.co/spaces/shi-labs/VCoder)

**Video**: [https://www.youtube.com/watch?v=go493IGgVWo](https://www.youtube.com/watch?v=go493IGgVWo)

**Abstract**:

>Humans possess the remarkable skill of Visual Perception, the ability to  see and understand the seen, helping them make sense of the visual  world and, in turn, reason. Multimodal Large Language Models (MLLM) have  recently achieved impressive performance on vision-language tasks  ranging from visual question-answering and image captioning to visual  reasoning and image generation. However, when prompted to identify or  count (perceive) the entities in a given image, existing MLLM systems  fail. Working towards developing an accurate MLLM system for perception  and reasoning, we propose using Versatile vision enCoders (**VCoder**) as  perception eyes for Multimodal LLMs. We feed the VCoder with perception  modalities such as segmentation or depth maps, improving the MLLM's  perception abilities. Secondly, we leverage the images from COCO and  outputs from off-the-shelf vision perception models to create our COCO  Segmentation Text (**COST**) dataset for training and evaluating MLLMs on  the object perception task. Thirdly, we introduce metrics to assess the  object perception abilities in MLLMs on our COST dataset. Lastly, we  provide extensive experimental evidence proving the VCoder's improved  object-level perception skills over existing Multimodal LLMs, including  GPT-4V. We open-source our dataset, code, and models to promote  research. We open-source our code at [this https URL](https://github.com/SHI-Labs/VCoder)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190mlil/r_vcoder_versatile_vision_encoders_for_multimodal/,0,5,0.79,<praw.models.comment_forest.CommentForest object at 0x0000026E3481B1D0>
190l41n,meemaowie,2024-01-07 05:48:36+00:00,[D] How can I gain experience in AI/ML for research positions,"Hello, I am a freshman CS major really interested in doing AI/ML research, especially in reinforcement learning. I want to reach out to professors for research opportunities, but I don't have much experience to show. I've done some online courses, read textbooks, etc. but there's not much I can show other than the fact that I completed some coding assignments as part of them. Do you have any suggestions on what I can do to gain experience in reinforcement learning that I can show to a professor to prove that I am ready for research in their lab? I've been thinking of implementing some papers from scratch and/or doing some side projects that involve machine learning. Is this a good place to start?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190l41n/d_how_can_i_gain_experience_in_aiml_for_research/,3,11,0.76,<praw.models.comment_forest.CommentForest object at 0x0000026E349C8F50>
190gs4y,hkproj_,2024-01-07 02:03:24+00:00,"[P] Mamba and S4 Explained: Architecture, Parallel Scan, Kernel Fusion, Recurrent/Convolution formulation, Math derivations from first principles, HiPPO theory visually explained, Math visually explained",,MachineLearning,https://youtu.be/8Q_tqwpTpVU,0,15,0.89,<praw.models.comment_forest.CommentForest object at 0x0000026E34820490>
190s1ox,Primary-Track8298,2024-01-07 13:22:25+00:00,[D] Is there a better way to test chatgpt?,Trying to use chatgpt for document extraction but it’s unclear if the model actually performs as well or reliably as I need it to. Is there a way to quantitatively determine how accurate the model is or do I just have to manually test,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190s1ox/d_is_there_a_better_way_to_test_chatgpt/,0,0,0.43,<praw.models.comment_forest.CommentForest object at 0x0000026E3484D710>
190fyt6,Direct-Touch469,2024-01-07 01:23:35+00:00,Active Learning [D],"
Anyone know of some good literature/resources to start with active learning? I come from a statistics background and got interested in this area due to experimental design/design of experiments. Lots of ties between the area of optimal design and active learning, hence was wondering what any of you who are in this area recommend reading.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190fyt6/active_learning_d/,5,10,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E34821050>
190rbte,Slow_Low206,2024-01-07 12:42:30+00:00,[D] How to do Regression to predict the outcome of a full year ?," So the main problem is : We have data of a variable that varies over a single year in a given location, and then we have an indicator that we measure at the end of the year for this given location at this same year. That means the indicator for a single year depends solely on the data of that same year.

With a dataset with multiple years and multiple measures over the year but only one measure of the indicator per year, what is the logic that should be implemented to use a regression to be able to predict that indicator ?

Thank you in advance for your help",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190rbte/d_how_to_do_regression_to_predict_the_outcome_of/,0,0,0.4,<praw.models.comment_forest.CommentForest object at 0x0000026E32829710>
1905izt,SloppyDrunkCarrot,2024-01-06 17:49:04+00:00,[D] Trying to understand the argument that proprietary hardware manufacturers will re-org the industry and cause OpenAI enterprise value to drop,"One of the opinions of some Silicon Valley voices is that two primary things will cause proprietary/closed source model builders to leak value: (1) the latency amongst all of the current tools makes building production-quality code unfeasible — APIs should take 30-50ms rather than 30-50s. (2) The cost of 1m tokens on any of these platforms makes it economically impossible if you’re trying to build something. 

The argument is that cloud services will come out that give users millisecond latency and pricing on the order of 10-20 cents for 1m tokens, and they’ll need to build their own custom hardware to do it.

The people who discuss this aren’t ML engineers/researchers. What is the feasibility of something like this happening? Beyond actually making hardware that’s capable of reducing costs by orders of magnitude, what are the challenges with this viewpoint?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1905izt/d_trying_to_understand_the_argument_that/,25,23,0.85,<praw.models.comment_forest.CommentForest object at 0x0000026E349EA890>
19119z6,curiouscattttqq,2024-01-07 20:15:13+00:00,[D] will a mscs degree help if I have a comp bio PhD?,"I’m a PhD student currently studying computational biology with extensive ML applications in biology. For various reasons, instead of doing bio related jobs, I might consider an ML engineering job or data scientist job in tech after I graduate.

Now my question is:
 I have the opportunity to work toward a CS master degree in my PhD program.  But I’m not sure if it’s worth the time to do so?
Will the mscs degree actually be helpful for getting a ML related job if I already have a PhD in comp bio? (my undergrad and master are not in CS)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/19119z6/d_will_a_mscs_degree_help_if_i_have_a_comp_bio_phd/,15,0,0.33,<praw.models.comment_forest.CommentForest object at 0x0000026E349F0190>
190ot3g,FancyUsual7476,2024-01-07 09:51:28+00:00,[D] What will happen if I update the reference model in rlhf?,"They generally says that the reference model should not be updated, but what will happen if I update it while back propogation? Will it be training slower? Or maybe its reward will be flunctuating a lot? Will both model updates towards higher reward value or away from optimal?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190ot3g/d_what_will_happen_if_i_update_the_reference/,2,1,0.6,<praw.models.comment_forest.CommentForest object at 0x0000026E349EB5D0>
1913kn9,CapableBad,2024-01-07 21:47:02+00:00,Everything You Need To Know About Google Gemini [D],,MachineLearning,https://youtu.be/ejdKbzIXutA?si=yLUDJzUbcoRjZwVG,0,0,0.13,<praw.models.comment_forest.CommentForest object at 0x0000026E348282D0>
190qfdh,Unclegaybus,2024-01-07 11:44:39+00:00,[D] How do you get your training data? Is it as important as it used to be?,"I'm curious how everyone spins up & maintains their projects when the first step is to have a really nice / well-annotated dataset. 

Aware that ""Training Data"" can take many forms based on the approach and project. But I've found it complex for almost all models I want to build. Whether it's an Object Detection model or simply Fine-tuning GPT.

I've heard of the big companies paying for crowds to collect all sorts of survey responses/images etc. but if you don't have those kinds of resources (which a lot of us don't) how do you even get started?

Are you going out and taking photos yourselves? Creating transcripts & conversations? Can you completely rely on synthetic data?

Finally (this is where my understanding falls short)... in a world with these large multimodal models... will training data as we used to know it become less important?

Happy Sunday!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190qfdh/d_how_do_you_get_your_training_data_is_it_as/,2,0,0.44,<praw.models.comment_forest.CommentForest object at 0x0000026E349E8150>
190dzwu,RajHalifax,2024-01-06 23:52:09+00:00,[D] Relation Extraction,"I’m trying the REBEL model from Hugging Face for  relation extraction. It outputs relations triplets via triplet linearization. It’s trained on REBEL dataset which is essentially Wikipedia data. I have free form text, and I want to generate relation triplets out of it. So, how to create a dataset from that text so as to closely align with the REBEL dataset? I want to fine-tune the model on my free form text. 

REBEL model: https://huggingface.co/Babelscape/rebel-large

REBEL dataset: https://huggingface.co/datasets/Babelscape/rebel-dataset 

If there are any other ML models which you suppose are worth trying for relation extraction, the information will be very much appreciated. :)

Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190dzwu/d_relation_extraction/,7,4,0.83,<praw.models.comment_forest.CommentForest object at 0x0000026E349F2F10>
190lt6h,prinherbst,2024-01-07 06:29:21+00:00,[D] Appendix length and acceptance,"Do you believe the length of the appendix affects the likelihood of a paper being accepted? I sometimes come across papers with appendices exceeding 50 pages. In such cases, I can't help but think about the considerable effort the authors must have put into writing the paper. Do you think this creates a favorable bias in the current machine learning scene towards accepting papers?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190lt6h/d_appendix_length_and_acceptance/,2,0,0.5,<praw.models.comment_forest.CommentForest object at 0x0000026E349FC510>
190g5td,pedhoss,2024-01-07 01:32:46+00:00,[D] Best annotation platforms for labeling text data at scale,"We're labeling text data for evaluating/fine-tuning some large language models (LLMs). We have a team of annotators and collaborators in-house. We are looking for an annotation platform that ideally does not limit the number of annotators who can participate, has a good way of role management and monitoring labels and annotators, has flexibility in defining the labeling schema, and has a user-friendly UI.

We have already tried [Scale AI's Studio](https://scale.com/studio) and [Label Studio](https://github.com/HumanSignal/label-studio), but they each have their limitations, and based on their business model and the way they compute labeling units, the annotation could be costly at scale (Label Studio does have a free community version but there's no role management, so having annotators at scale could make labeling management hard.) We have also considered Amazon Mechanical Turk but one challenge could be that the annotators need to have a worker ID first to join the annotation task (correct me if I'm wrong).

Does anyone have any recs for a reliable annotation platform for text data at scale? Is there any obvious platform or tool we're missing? The tool does not have to be free, but reliability and support are important and at a reasonable cost when scaling the annotation.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190g5td/d_best_annotation_platforms_for_labeling_text/,1,2,0.76,<praw.models.comment_forest.CommentForest object at 0x0000026E349EF950>
190fwpn,Few-Letter312,2024-01-07 01:20:44+00:00,[D] Inference optimization with HPC,"Hi guys,

I have a task about optimization of inference on a model Llama. I have to create a framework to optimize the inference. 

i already have asked in huggingface forum, do you know any other forums where questions about machine learning/ai models are solved.

Also if you have knowledge in this topic please lmk, and I will post the question here too.

Im just not sure if its the right forum to do so 

Thanks for your help :p",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190fwpn/d_inference_optimization_with_hpc/,2,2,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E349FCAD0>
18zie7z,we_are_mammals,2024-01-05 21:39:40+00:00,Transformer-Based LLMs Are Not General Learners: A Universal Circuit Perspective [R],"https://openreview.net/forum?id=tGM7rOmJzV

> (LLMs') remarkable success triggers a notable shift in the research priorities of the artificial intelligence community. These impressive empirical achievements fuel an expectation that LLMs are “sparks of Artificial General Intelligence (AGI)"". However, some evaluation results have also presented confusing instances of LLM failures, including some in seemingly trivial tasks. For example, GPT-4 is able to solve some mathematical problems in IMO that could be challenging for graduate students, while it could make errors on arithmetic problems at an elementary school level in some cases.

> ...

> Our theoretical results indicate that T-LLMs fail to be general learners. However, the T-LLMs achieve great empirical success in various tasks. We provide a possible explanation for this inconsistency: while T-LLMs are not general learners, they can partially solve complex tasks by memorizing a number of instances, leading to an illusion that the T-LLMs have genuine problem-solving ability for these tasks.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/,56,263,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E349F13D0>
18zufv8,Wiskkey,2024-01-06 07:23:04+00:00,[R] The Expressive Power of Transformers with Chain of Thought,"[Paper](https://arxiv.org/abs/2310.07923). I am not affiliated with the authors.

Abstract:

>Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a ""chain of thought"" or ""scratchpad"", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context-sensitive languages, and polynomial steps make them recognize exactly the class of polynomial-time solvable problems -- the first exact characterization of a type of transformers in terms of standard complexity classes. Together, our results provide a nuanced framework for understanding how the length of a transformer's chain of thought or scratchpad impacts its reasoning power.",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/18zufv8/r_the_expressive_power_of_transformers_with_chain/,3,42,0.93,<praw.models.comment_forest.CommentForest object at 0x0000026E349FD050>
1904tl7,thk_ML,2024-01-06 17:19:04+00:00,Univariate anomaly detection [D]," Hi!

I'm facing a problem that seems 'easy,' but I've been struggling with it for a while now in the field of anomaly/outlier detection.

I have a dataset of around 60K data points. Each data point is part of a group (\~1500 groups; min group size is 15) and has a length parameter.

The task is to perform anomaly detection within the groups, i.e., if a data point has an irregular length compared to the other data points in the group, mark it as an anomaly.

I'm using a log2 transformation on the data, and after the transformation, the majority of groups (75%) are normally distributed based on Shapiro-Wilks test.

As a first solution, I tried the classical distance in std from the mean, where if the length is bigger than mean+3\*std, then this is an anomaly.

I had 2 problems with this solution:

In groups with a high number of data points, where the vast majority of data points had the same or very similar length, the std was very small, thus making the threshold very small, and it resulted in alerting on data points, which I do not consider as anomalies.

This method resulted in a relatively high detection (\~250 anomalies), and I aim to alert only a small number of the most extreme anomalies in my data across all the groups.

When I tried to increase the threshold, e.g., to 4std, I faced another problem, where I missed anomalies in groups where one data point had a very large length compared to the others, which resulted with a high std, and thus making the extreme data point to have a 'low' std from the mean distance.

I'd appreciate any help or thoughts on the subject. Thanks!",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1904tl7/univariate_anomaly_detection_d/,8,7,1.0,<praw.models.comment_forest.CommentForest object at 0x0000026E349F1390>
190luhp,Sensitive-Rule-8966,2024-01-07 06:31:24+00:00,"soccer prediction algorithm ""[Project]""","i want to make a neural network gradient decents based algorithm that will predict which team will win based on their history.  
my data will probably be the players of every game, if there are any injured players, and the winning team.  
can something like that work and have a decent accuracy? or is soccer to much cayotic?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190luhp/soccer_prediction_algorithm_project/,2,0,0.42,<praw.models.comment_forest.CommentForest object at 0x0000026E34A0DD10>
18zy0g4,Instantinopaul,2024-01-06 11:25:13+00:00,[D] JPMorgan drops DocLLM for multimodal documents!,"
JPMorgan drops DocLLM for multimodal documents for invoices, reports & contracts!

I have a few useful projects with pdf extraction in my mind. I am very excited to see an open source availability of equivalent model on the original paper.

Any thoughts on this??",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/18zy0g4/d_jpmorgan_drops_docllm_for_multimodal_documents/,7,12,0.84,<praw.models.comment_forest.CommentForest object at 0x0000026E34A0E010>
190lg6w,Entire-Fly-6957,2024-01-07 06:07:28+00:00,"[D] GPT4-V vs Gemini, Which model is better?😃","  The press conferences for the Google Gemini model are always impressive. I chose two tests from them to challenge GPT-4V, and it performed exceptionally well, impressively completing the tasks😃: 

[https://youtube.com/watch?v=RMkWBhqH0p4&si=QY4Opy2ToaKoWH-7](https://youtube.com/watch?v=RMkWBhqH0p4&si=QY4Opy2ToaKoWH-7)",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190lg6w/d_gpt4v_vs_gemini_which_model_is_better/,1,0,0.29,<praw.models.comment_forest.CommentForest object at 0x0000026E34A025D0>
1902xpv,cloneofsimo,2024-01-06 15:55:22+00:00,[P] Set EMA decay after training? Novel Karras Power EMA tutorial + implementation,"https://github.com/cloneofsimo/karras-power-ema-tutorial

Recently, Karras demonstrated post-hoc ema method, where he was able to ""simulate"" arbitrary ema decaying factor after the training by saving two copies of ema and clever math.

I took a deep breath to understand it, and wrote a tutorial on the readme + working example!

But you might say... why? It turns out Ema decay turns out to be quite radically sensitive hyperparameter

Because you can set EMA decay factor after training, you can ""sweep"" after training, to get  the best checkpoint.",MachineLearning,https://i.redd.it/3g4sm5soduac1.png,0,2,0.63,<praw.models.comment_forest.CommentForest object at 0x0000026E34A01F90>
190orz1,asmekal,2024-01-07 09:49:19+00:00,[D] Automated Paper Review: Thoughts?,"I have recently reviewed some CVPR papers and it seemed to me that the process of papers grading/ranking can be more or less automated, especially with the rising number of the submissions.

Questions to the community:

1. What are your thoughts on automated reviews in general? As an author? As a reviewer/conference organiser?
2. What qualities would you like to see in such a system? What would be helpful?

I've built a simple GPT Agent POC with some common sense reviewer guidance. Any feedback is welcome. Just attach pdf of the paper [in the chat](https://chat.openai.com/g/g-IKAbfLLtC-research-paper-reviewer) and it will give a review.

It does give some plausable-looking reviews. Most importantly it highlights the issues of the papers (at least most of them) right, which should be helpful for both the authors and for the conference comittee. The accept/reject actual accuracy, novelty check, comparisons with related work clearly can be improved. Let me know what features would you like to have first, if any.

*PS Be careful to what you submit to the bot, some companies have strict policies for unpublished research and reviewer's policies also might prohibit usage of LLMs. In particular ongoing CVPR review prohibits to show a paper to any LLM.*",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190orz1/d_automated_paper_review_thoughts/,14,0,0.19,<praw.models.comment_forest.CommentForest object at 0x0000026E3482EA10>
190pos6,andWan,2024-01-07 10:54:25+00:00,"[D] I observe a difference in the time it takes for GPT4 to answer, maybe depending on the questions complexity. Thoughts? References?",As the title says. What mechanisms are most likely to be used here? Any good paper about this?,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/190pos6/d_i_observe_a_difference_in_the_time_it_takes_for/,47,0,0.26,<praw.models.comment_forest.CommentForest object at 0x0000026E34A5A8D0>
1906vf0,juliusvi2,2024-01-06 18:45:47+00:00,[R] Mangio RVC - Threshold detection high when using rmvpe - converted audio gated?,"
I'm using Mangio RVC 23.7.0 to convert some voice audio using various models trained by myself and others. I've been using rmvpe for the pitch as it appears to have the most accurate results across the board but I have a massive issue in that it doesn't like any audio that isn't hitting high dB levels and I have resorted to compressing audio and limiting it to get decent results from rmvpe. Even so, volume fluctuates and I have to further compress a lot on the converted audio externally. The original uncompressed audio isn't even that quiet, averaging around - 12db. This is an extra step in my work flow that I really would like to do without. 

It sounds like there's a noise gate or a very high detection in the settings for rmvpe so it's having trouble with quieter parts, but for the life of me cannot figure out where it is.

Uncompressed voice audio I've not had an issue with when using so-vits-fork, as I've just set the detection threshold at around - 60db which catches every nuance in the voice, but from what I've tested rvc and rmvpe just give more accurate results in terms of the pronunciation and pitch detection. 

Is there anything I can do to make rmvpe or mangio rvc to detect lower levels of audio?",MachineLearning,https://www.reddit.com/r/MachineLearning/comments/1906vf0/r_mangio_rvc_threshold_detection_high_when_using/,0,1,0.67,<praw.models.comment_forest.CommentForest object at 0x0000026E3482F5D0>
190j0ox,unintended_purposes,2024-01-07 03:57:19+00:00,[D] The future of Humans: Operators of AI,,MachineLearning,https://medium.com/unintended-purposes/the-future-of-humans-operators-of-ai-244359017575,0,0,0.31,<praw.models.comment_forest.CommentForest object at 0x0000026E34A54990>
