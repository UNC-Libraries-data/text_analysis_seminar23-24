{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scattertext \n",
    "\n",
    "### Install\n",
    "\n",
    "It's very important to install scattertext using pip:\n",
    "\n",
    "`pip install scattertext`\n",
    "\n",
    "The conda and conda-forge versions are out of date and currently don't work due to some version conflicts!\n",
    "\n",
    "### Built-in example: 2012 US political convention speeches\n",
    "\n",
    "First, we'll work through a modified version of the built-in example from scattertext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1687281"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "df = st.SampleCorpora.ConventionData2012.get_data().assign(\n",
    "    parse=lambda df: df.text.apply(st.whitespace_nlp_with_sentences)\n",
    ")\n",
    "\n",
    "corpus = (\n",
    "    st.CorpusFromParsedDocuments(df, category_col='party', parsed_col='parse')\n",
    "    .build()\n",
    "    .remove_terms(eng_stopwords, ignore_absences=True)\n",
    "    .get_unigram_corpus()\n",
    "    .compact(st.AssociationCompactor(2000))\n",
    ")\n",
    "\n",
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='democrat',\n",
    "    category_name='Democratic',\n",
    "    not_category_name='Republican',\n",
    "    minimum_term_frequency=0, \n",
    "    pmi_threshold_coefficient=0,\n",
    "    width_in_pixels=1000, \n",
    "    metadata=corpus.get_df()['speaker'],\n",
    "    transform=st.Scalers.dense_rank,\n",
    "    include_gradient=True,\n",
    "    left_gradient_term='More Republican',\n",
    "    middle_gradient_term='Metric: Dense Rank Difference',\n",
    "    right_gradient_term='More Democratic',\n",
    ")\n",
    "open('./scattertext0.html', 'w').write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the result, we need to return to our folder and open the output .html file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scattertext with Reddit data\n",
    "\n",
    "Let's apply this to our Reddit data!  We can compare two different subreddits: GPT3 and MachineLearning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = pd.read_csv(\"gpt3_data.csv\")\n",
    "gpt3[\"subreddit\"] = \"gpt3\"\n",
    "\n",
    "ml = pd.read_csv(\"MachineLearning_data.csv\")\n",
    "ml[\"subreddit\"] = \"MachineLearning\"\n",
    "\n",
    "reddit = pd.concat([gpt3, ml], ignore_index=True)[[\"content\",\"subreddit\"]]\n",
    "reddit = reddit.loc[reddit.content.notnull(),:]\n",
    "reddit = reddit.assign(\n",
    "    parse=lambda df: df.content.apply(st.whitespace_nlp_with_sentences)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2075402"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = (\n",
    "    st.CorpusFromParsedDocuments(reddit, category_col='subreddit', parsed_col='parse')\n",
    "    .build()\n",
    "    .remove_terms(eng_stopwords, ignore_absences=True)\n",
    "    .get_unigram_corpus()\n",
    "    .compact(st.AssociationCompactor(2000))\n",
    ")\n",
    "\n",
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='gpt3',\n",
    "    category_name='gpt3',\n",
    "    not_category_name='MachineLearning',\n",
    "    minimum_term_frequency=0, \n",
    "    pmi_threshold_coefficient=0,\n",
    "    width_in_pixels=1000,\n",
    "    transform=st.Scalers.dense_rank,\n",
    "    include_gradient=True,\n",
    "    left_gradient_term='More MachineLearning',\n",
    "    middle_gradient_term='Metric: Dense Rank Difference',\n",
    "    right_gradient_term='More gpt3',\n",
    ")\n",
    "open('./scattertext_reddit.html', 'w').write(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
